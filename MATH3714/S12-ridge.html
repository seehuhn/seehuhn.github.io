<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 12 Ridge Regression | MATH3714 Linear Regression and Robustness</title>
  <meta name="description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2021/22" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 12 Ridge Regression | MATH3714 Linear Regression and Robustness" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://seehuhn.github.io/MATH3714/" />
  
  <meta property="og:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2021/22" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 12 Ridge Regression | MATH3714 Linear Regression and Robustness" />
  
  <meta name="twitter:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2021/22" />
  

<meta name="author" content="Jochen Voss" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="S11-improving.html"/>
<link rel="next" href="Sx1-matrices.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P96L0SF56N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-P96L0SF56N');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">MATH3714 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html"><i class="fa fa-check"></i>About MATH3714</a>
<ul>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#notes"><i class="fa fa-check"></i>Notes and videos</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#lectures"><i class="fa fa-check"></i>Lectures</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#workshops"><i class="fa fa-check"></i>Workshops and Problem Sheets</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#teams"><i class="fa fa-check"></i>Discussion Board</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#assessments"><i class="fa fa-check"></i>Assessments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="S01-simple.html"><a href="S01-simple.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="S01-simple.html"><a href="S01-simple.html#residual-sum-of-squares"><i class="fa fa-check"></i><b>1.1</b> Residual Sum of Squares</a></li>
<li class="chapter" data-level="1.2" data-path="S01-simple.html"><a href="S01-simple.html#linear-regression-as-a-parameter-estimation-problem"><i class="fa fa-check"></i><b>1.2</b> Linear Regression as a Parameter Estimation Problem</a></li>
<li class="chapter" data-level="1.3" data-path="S01-simple.html"><a href="S01-simple.html#sec:simple-mat"><i class="fa fa-check"></i><b>1.3</b> Matrix Notation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="S02-multiple.html"><a href="S02-multiple.html"><i class="fa fa-check"></i><b>2</b> Least Squares Estimates</a>
<ul>
<li class="chapter" data-level="2.1" data-path="S02-multiple.html"><a href="S02-multiple.html#data-and-models"><i class="fa fa-check"></i><b>2.1</b> Data and Models</a></li>
<li class="chapter" data-level="2.2" data-path="S02-multiple.html"><a href="S02-multiple.html#the-normal-equations"><i class="fa fa-check"></i><b>2.2</b> The Normal Equations</a></li>
<li class="chapter" data-level="2.3" data-path="S02-multiple.html"><a href="S02-multiple.html#fitted-values"><i class="fa fa-check"></i><b>2.3</b> Fitted Values</a></li>
<li class="chapter" data-level="2.4" data-path="S02-multiple.html"><a href="S02-multiple.html#example"><i class="fa fa-check"></i><b>2.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html"><i class="fa fa-check"></i>Interlude: Linear Regression in R</a>
<ul>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-fitting"><i class="fa fa-check"></i>Fitting a Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-model"><i class="fa fa-check"></i>Understanding the Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-predict"><i class="fa fa-check"></i>Making Predictions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P01.html"><a href="P01.html"><i class="fa fa-check"></i>Problem Sheet 1</a></li>
<li class="chapter" data-level="3" data-path="S03-cov.html"><a href="S03-cov.html"><i class="fa fa-check"></i><b>3</b> Random Vectors and Covariance</a>
<ul>
<li class="chapter" data-level="3.1" data-path="S03-cov.html"><a href="S03-cov.html#expectation"><i class="fa fa-check"></i><b>3.1</b> Expectation</a></li>
<li class="chapter" data-level="3.2" data-path="S03-cov.html"><a href="S03-cov.html#sec:covariance"><i class="fa fa-check"></i><b>3.2</b> Covariance Matrix</a></li>
<li class="chapter" data-level="3.3" data-path="S03-cov.html"><a href="S03-cov.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>3.3</b> The Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="S04-model.html"><a href="S04-model.html"><i class="fa fa-check"></i><b>4</b> Properties of the Least Squares Estimate</a>
<ul>
<li class="chapter" data-level="4.1" data-path="S04-model.html"><a href="S04-model.html#mean-and-covariance"><i class="fa fa-check"></i><b>4.1</b> Mean and Covariance</a></li>
<li class="chapter" data-level="4.2" data-path="S04-model.html"><a href="S04-model.html#hat-matrix"><i class="fa fa-check"></i><b>4.2</b> Properties of the Hat Matrix</a></li>
<li class="chapter" data-level="4.3" data-path="S04-model.html"><a href="S04-model.html#Cochran"><i class="fa fa-check"></i><b>4.3</b> Cochran’s theorem</a></li>
<li class="chapter" data-level="4.4" data-path="S04-model.html"><a href="S04-model.html#var-est-bias"><i class="fa fa-check"></i><b>4.4</b> Estimating the Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="S05-single.html"><a href="S05-single.html"><i class="fa fa-check"></i><b>5</b> Uncertainty for Individual Regression Coefficients</a>
<ul>
<li class="chapter" data-level="5.1" data-path="S05-single.html"><a href="S05-single.html#measuring-the-estimation-error"><i class="fa fa-check"></i><b>5.1</b> Measuring the Estimation Error</a></li>
<li class="chapter" data-level="5.2" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.3" data-path="S05-single.html"><a href="S05-single.html#hypthesis-tests"><i class="fa fa-check"></i><b>5.3</b> Hypthesis Tests</a></li>
<li class="chapter" data-level="5.4" data-path="S05-single.html"><a href="S05-single.html#r-experiments"><i class="fa fa-check"></i><b>5.4</b> R Experiments</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="S05-single.html"><a href="S05-single.html#fitting-the-model"><i class="fa fa-check"></i><b>5.4.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.4.2" data-path="S05-single.html"><a href="S05-single.html#estimating-the-variance-of-the-error"><i class="fa fa-check"></i><b>5.4.2</b> Estimating the Variance of the Error</a></li>
<li class="chapter" data-level="5.4.3" data-path="S05-single.html"><a href="S05-single.html#estimating-the-standard-errors"><i class="fa fa-check"></i><b>5.4.3</b> Estimating the Standard Errors</a></li>
<li class="chapter" data-level="5.4.4" data-path="S05-single.html"><a href="S05-single.html#hypothesis-tests"><i class="fa fa-check"></i><b>5.4.4</b> Hypothesis tests</a></li>
<li class="chapter" data-level="5.4.5" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals-1"><i class="fa fa-check"></i><b>5.4.5</b> Confidence Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html"><i class="fa fa-check"></i><b>6</b> Estimating Coefficients Simultaneously</a>
<ul>
<li class="chapter" data-level="6.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-dist"><i class="fa fa-check"></i><b>6.1</b> Linear Combinations of Coefficients</a></li>
<li class="chapter" data-level="6.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-CI"><i class="fa fa-check"></i><b>6.2</b> Confidence Regions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#result"><i class="fa fa-check"></i><b>6.2.1</b> Result</a></li>
<li class="chapter" data-level="6.2.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#numerical-experiments"><i class="fa fa-check"></i><b>6.2.2</b> Numerical Experiments</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-test"><i class="fa fa-check"></i><b>6.3</b> Hypothesis Tests</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html"><i class="fa fa-check"></i>Interlude: Loading Data into R</a>
<ul>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-csv-files"><i class="fa fa-check"></i>Importing CSV Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-microsoft-excel-files"><i class="fa fa-check"></i>Importing Microsoft Excel Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#checking-the-imported-data"><i class="fa fa-check"></i>Checking the Imported Data</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#common-problems"><i class="fa fa-check"></i>Common Problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="S07-examples.html"><a href="S07-examples.html"><i class="fa fa-check"></i><b>7</b> Examples</a>
<ul>
<li class="chapter" data-level="7.1" data-path="S07-examples.html"><a href="S07-examples.html#simple-confidence-interval"><i class="fa fa-check"></i><b>7.1</b> Simple Confidence Interval</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles"><i class="fa fa-check"></i><b>7.1.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.1.2" data-path="S07-examples.html"><a href="S07-examples.html#from-the-lm-output"><i class="fa fa-check"></i><b>7.1.2</b> From the <code>lm()</code> Output</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="S07-examples.html"><a href="S07-examples.html#confidence-intervals-for-the-mean"><i class="fa fa-check"></i><b>7.2</b> Confidence Intervals for the Mean</a></li>
<li class="chapter" data-level="7.3" data-path="S07-examples.html"><a href="S07-examples.html#testing-a-single-coefficient"><i class="fa fa-check"></i><b>7.3</b> Testing a Single Coefficient</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-1"><i class="fa fa-check"></i><b>7.3.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.3.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-i"><i class="fa fa-check"></i><b>7.3.2</b> Using the <code>lm()</code> Output, I</a></li>
<li class="chapter" data-level="7.3.3" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-ii"><i class="fa fa-check"></i><b>7.3.3</b> Using the <code>lm()</code> Output, II</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="S07-examples.html"><a href="S07-examples.html#testing-multiple-coefficents"><i class="fa fa-check"></i><b>7.4</b> Testing Multiple Coefficents</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-2"><i class="fa fa-check"></i><b>7.4.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.4.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output"><i class="fa fa-check"></i><b>7.4.2</b> Using the <code>lm()</code> output</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="P02.html"><a href="P02.html"><i class="fa fa-check"></i>Problem Sheet 2</a></li>
<li class="chapter" data-level="8" data-path="S08-diagnostics.html"><a href="S08-diagnostics.html"><i class="fa fa-check"></i><b>8</b> Regression Diagnostics</a>
<ul>
<li class="chapter" data-level="8.1" data-path="S08-diagnostics.html"><a href="S08-diagnostics.html#plots"><i class="fa fa-check"></i><b>8.1</b> Diagnostic Plots</a></li>
<li class="chapter" data-level="8.2" data-path="S08-diagnostics.html"><a href="S08-diagnostics.html#R-squared"><i class="fa fa-check"></i><b>8.2</b> The Coefficient of Multiple Determination</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="S09-influence.html"><a href="S09-influence.html"><i class="fa fa-check"></i><b>9</b> The Influence of Observations</a>
<ul>
<li class="chapter" data-level="9.1" data-path="S09-influence.html"><a href="S09-influence.html#deleting-observations"><i class="fa fa-check"></i><b>9.1</b> Deleting Observations</a></li>
<li class="chapter" data-level="9.2" data-path="S09-influence.html"><a href="S09-influence.html#cooks-distance"><i class="fa fa-check"></i><b>9.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="S10-multicoll.html"><a href="S10-multicoll.html"><i class="fa fa-check"></i><b>10</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="10.1" data-path="S10-multicoll.html"><a href="S10-multicoll.html#consequences-of-multicollinearity"><i class="fa fa-check"></i><b>10.1</b> Consequences of Multicollinearity</a></li>
<li class="chapter" data-level="10.2" data-path="S10-multicoll.html"><a href="S10-multicoll.html#detecting-multicollinearity"><i class="fa fa-check"></i><b>10.2</b> Detecting Multicollinearity</a></li>
<li class="chapter" data-level="10.3" data-path="S10-multicoll.html"><a href="S10-multicoll.html#mitigations"><i class="fa fa-check"></i><b>10.3</b> Mitigations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P03.html"><a href="P03.html"><i class="fa fa-check"></i>Problem Sheet 3</a></li>
<li class="chapter" data-level="11" data-path="S11-improving.html"><a href="S11-improving.html"><i class="fa fa-check"></i><b>11</b> Improving the Model Fit</a>
<ul>
<li class="chapter" data-level="11.1" data-path="S11-improving.html"><a href="S11-improving.html#linearising-the-mean"><i class="fa fa-check"></i><b>11.1</b> Linearising the Mean</a></li>
<li class="chapter" data-level="11.2" data-path="S11-improving.html"><a href="S11-improving.html#stabilising-the-variance"><i class="fa fa-check"></i><b>11.2</b> Stabilising the Variance</a></li>
<li class="chapter" data-level="11.3" data-path="S11-improving.html"><a href="S11-improving.html#the-power-transform"><i class="fa fa-check"></i><b>11.3</b> The Power Transform</a></li>
<li class="chapter" data-level="11.4" data-path="S11-improving.html"><a href="S11-improving.html#orthogonal-inputs"><i class="fa fa-check"></i><b>11.4</b> Orthogonal Inputs</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="S12-ridge.html"><a href="S12-ridge.html"><i class="fa fa-check"></i><b>12</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="S12-ridge.html"><a href="S12-ridge.html#definition-the-estimator"><i class="fa fa-check"></i><b>12.1</b> Definition the Estimator</a></li>
<li class="chapter" data-level="12.2" data-path="S12-ridge.html"><a href="S12-ridge.html#properties-of-the-estimate"><i class="fa fa-check"></i><b>12.2</b> Properties of the Estimate</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="S12-ridge.html"><a href="S12-ridge.html#bias"><i class="fa fa-check"></i><b>12.2.1</b> Bias</a></li>
<li class="chapter" data-level="12.2.2" data-path="S12-ridge.html"><a href="S12-ridge.html#variance"><i class="fa fa-check"></i><b>12.2.2</b> Variance</a></li>
<li class="chapter" data-level="12.2.3" data-path="S12-ridge.html"><a href="S12-ridge.html#mean-squared-error"><i class="fa fa-check"></i><b>12.2.3</b> Mean Squared Error</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="S12-ridge.html"><a href="S12-ridge.html#standardisation"><i class="fa fa-check"></i><b>12.3</b> Standardisation</a></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html"><i class="fa fa-check"></i><b>A</b> Linear Algebra Reminders</a>
<ul>
<li class="chapter" data-level="A.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#vectors"><i class="fa fa-check"></i><b>A.1</b> Vectors</a></li>
<li class="chapter" data-level="A.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-rules"><i class="fa fa-check"></i><b>A.2</b> Matrices</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#transpose"><i class="fa fa-check"></i><b>A.2.1</b> Transpose</a></li>
<li class="chapter" data-level="A.2.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-vector-product"><i class="fa fa-check"></i><b>A.2.2</b> Matrix-vector Product</a></li>
<li class="chapter" data-level="A.2.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-matrix-product"><i class="fa fa-check"></i><b>A.2.3</b> Matrix-matrix Product</a></li>
<li class="chapter" data-level="A.2.4" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#rank"><i class="fa fa-check"></i><b>A.2.4</b> Rank</a></li>
<li class="chapter" data-level="A.2.5" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.2.5</b> Matrix Inverse</a></li>
<li class="chapter" data-level="A.2.6" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.2.6</b> Orthogonal Matrices</a></li>
<li class="chapter" data-level="A.2.7" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#positive-definite"><i class="fa fa-check"></i><b>A.2.7</b> Positive Definite Matrices</a></li>
<li class="chapter" data-level="A.2.8" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#idempotent"><i class="fa fa-check"></i><b>A.2.8</b> Idempotent Matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#eigenvalues"><i class="fa fa-check"></i><b>A.3</b> Eigenvalues</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="Sx2-probability.html"><a href="Sx2-probability.html"><i class="fa fa-check"></i><b>B</b> Probability Reminders</a>
<ul>
<li class="chapter" data-level="B.1" data-path="Sx2-probability.html"><a href="Sx2-probability.html#independence"><i class="fa fa-check"></i><b>B.1</b> Independence</a></li>
<li class="chapter" data-level="B.2" data-path="Sx2-probability.html"><a href="Sx2-probability.html#chi-square"><i class="fa fa-check"></i><b>B.2</b> The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="B.3" data-path="Sx2-probability.html"><a href="Sx2-probability.html#t"><i class="fa fa-check"></i><b>B.3</b> The t-distribution</a></li>
</ul></li>
<li class="divider"></li>
<li class="chapter"><span><b>THE END</b></span></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3714 Linear Regression and Robustness</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="S12-ridge" class="section level1" number="12">
<h1><span class="header-section-number">Section 12</span> Ridge Regression</h1>
<p>In cases where <span class="math inline">\(X\)</span> shows approximate multicollinearity, the parameter estimates
<span class="math inline">\(\hat\beta\)</span> have coefficients with large variance. In case of exact
multicollinearity, <span class="math inline">\(\hat\beta\)</span> is no longer uniquely defined, and the minimum
of the residual sum of squares is attained on an (unbounded) subspace of the
parameter space. In both cases, a more stable estimate can be obtained by
changing the estimation procedure to include an additional penalty term which
discourages “large” values of <span class="math inline">\(\hat\beta\)</span>. This is the idea of ridge
regression. In this section we will see that the resulting estimates are
biased, but sometimes have smaller mean squared error than the least squares
estimator.</p>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/-X4kQpgYJLk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<div id="definition-the-estimator" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> Definition the Estimator</h2>
<div class="definition">
<p><span id="def:unlabeled-div-50" class="definition"><strong>Definition 12.1  </strong></span>In multiple linear regression, the <strong>ridge regression</strong> estimate for
the parameter vector <span class="math inline">\(\beta\)</span> is given by
<span class="math display">\[\begin{equation}
  \hat\beta^{(\lambda)}
  = \mathop{\mathrm{arg\,min}}\limits_\beta \Bigl(
      \sum_{i=1}^n \bigl( y_i - x_i^\top \beta \bigr)^2
      + \lambda \|\beta\|^2
    \Bigr).
\end{equation}\]</span>
The parameter <span class="math inline">\(\lambda \geq 0\)</span> is called the <strong>regularisation parameter</strong>.</p>
</div>
<p>Here we write <span class="math inline">\(\mathop{\mathrm{arg\,min}}\limits\nolimits_\beta (\cdots)\)</span> to denote the value of <span class="math inline">\(\beta\)</span>
which minimises the expression on the right-hand side, and <span class="math inline">\(\|\beta\|\)</span>
denotes the Euclidean norm of the vector <span class="math inline">\(\beta\)</span>, <em>i.e.</em>
<span class="math display">\[\begin{equation*}
  \|\beta\|^2
  = \sum_{j=0}^p \beta_j^2.
\end{equation*}\]</span></p>
<p>The term being minimised is
<span class="math display">\[\begin{equation*}
  r^{(\lambda)}
  = \sum_{i=1}^n \bigl( y_i - x_i^\top \beta \bigr)^2
      + \lambda \|\beta\|^2,
\end{equation*}\]</span>
consisting of the residual sum of squares plus a penalty term. The
regularisation parameter <span class="math inline">\(\lambda\)</span> controls the relative weight of these two
terms. For large <span class="math inline">\(\lambda\)</span>, the main objective to minimise the norm
<span class="math inline">\(\|\beta\|\)</span> and one can show that <span class="math inline">\(\lim_{\lambda \to\infty} \hat\beta^{(\lambda)} = 0\)</span>. For <span class="math inline">\(\lambda \downarrow 0\)</span> only the residual sum
of squares is left and if <span class="math inline">\(X^\top X\)</span> is invertible we get <span class="math inline">\(\hat\beta^{(0)} = \hat\beta\)</span>, <em>i.e.</em> here the ridge regression estimate coincides with the least
squares estimate.</p>
<p>As in lemma <a href="S02-multiple.html#lem:multiple-LSQ">2.1</a> we can find an explicit formula
for the ridge regression estimate. For this we write <span class="math inline">\(r^{(\lambda)}\)</span>
as
<span class="math display">\[\begin{equation*}
  r^{(\lambda)}
  = (y - X\top \beta)^\top (y - X\top \beta )
      + \lambda \beta^\top \beta
\end{equation*}\]</span>
and then take derivatives to find the minimum. The result is
given by
<span class="math display" id="eq:1523">\[\begin{equation}
  \hat\beta^{(\lambda)}
  = (X^\top X + \lambda I)^{-1} X^\top y.    \tag{12.1}
\end{equation}\]</span></p>
<div class="example">
<p><span id="exm:knorkel" class="example"><strong>Example 12.1  </strong></span>To illustrate the method, we compute the ridge regression estimate for
a badly conditioned toy dataset.</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="S12-ridge.html#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fix the true parameters for our simulated data:</span></span>
<span id="cb204-2"><a href="S12-ridge.html#cb204-2" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">5</span>, .<span class="dv">5</span>)</span>
<span id="cb204-3"><a href="S12-ridge.html#cb204-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb204-4"><a href="S12-ridge.html#cb204-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-5"><a href="S12-ridge.html#cb204-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20211112</span>)</span>
<span id="cb204-6"><a href="S12-ridge.html#cb204-6" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1.01</span>, <span class="fl">2.00</span>, <span class="fl">2.99</span>, <span class="fl">4.02</span>, <span class="fl">5.01</span>, <span class="fl">5.99</span>)</span>
<span id="cb204-7"><a href="S12-ridge.html#cb204-7" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.98</span>, <span class="fl">1.99</span>, <span class="fl">3.00</span>, <span class="fl">4.00</span>, <span class="fl">4.99</span>, <span class="fl">6.00</span>)</span>
<span id="cb204-8"><a href="S12-ridge.html#cb204-8" aria-hidden="true" tabindex="-1"></a><span class="co"># beta[1] in R corresponds to \beta_0, beta[2] = \beta_1, beta[3] = \beta_2</span></span>
<span id="cb204-9"><a href="S12-ridge.html#cb204-9" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> beta[<span class="dv">1</span>] <span class="sc">+</span> beta[<span class="dv">2</span>] <span class="sc">*</span> x1 <span class="sc">+</span> beta[<span class="dv">3</span>] <span class="sc">*</span> x2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">6</span>, <span class="dv">0</span>, sigma)</span>
<span id="cb204-10"><a href="S12-ridge.html#cb204-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-11"><a href="S12-ridge.html#cb204-11" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb204-12"><a href="S12-ridge.html#cb204-12" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2)</span>
<span id="cb204-13"><a href="S12-ridge.html#cb204-13" aria-hidden="true" tabindex="-1"></a>beta.ridge <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> lambda <span class="sc">*</span> <span class="fu">diag</span>(<span class="fu">ncol</span>(X)), <span class="fu">t</span>(X) <span class="sc">%*%</span> y)</span>
<span id="cb204-14"><a href="S12-ridge.html#cb204-14" aria-hidden="true" tabindex="-1"></a>beta.ridge[,<span class="dv">1</span>] <span class="co"># get first column instead of n × 1 matrix, for tidiness</span></span></code></pre></div>
<pre class="rOutput"><code>(Intercept)          x1          x2 
-0.01620501  0.52739367  0.47343531 </code></pre>
<p>For comparison, we also compute the least squares estimate:</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="S12-ridge.html#cb206-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2)</span>
<span id="cb206-2"><a href="S12-ridge.html#cb206-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(m)</span></code></pre></div>
<pre class="rOutput"><code>(Intercept)          x1          x2 
-0.02935851  1.03064453 -0.02749857 </code></pre>
<p>We see that despite the small value of <span class="math inline">\(\lambda\)</span> there is a considerable
difference between the estimated values <span class="math inline">\(\hat\beta\)</span> and
<span class="math inline">\(\hat\beta^{(\lambda)}\)</span> in this example. Experimenting with different
values of the seed also shows that the variance of <span class="math inline">\(\hat\beta^{(\lambda)}\)</span>
is much smaller than the variance of <span class="math inline">\(\hat\beta\)</span>. In the next section
we will see a theoretical explanation for this fact.</p>
</div>
<p>For <span class="math inline">\(\lambda &gt; 0\)</span> and <span class="math inline">\(v \neq 0\)</span> we have <span class="math inline">\(\|v\| &gt; 0\)</span> and thus
<span class="math display">\[\begin{align*}
  \| (X^\top X + \lambda I) v \|^2
  &amp;= v^\top (X^\top X + \lambda I)^\top (X^\top X + \lambda I) v \\
  &amp;= v^\top X^\top X X^\top X v + 2 \lambda v^\top X^\top X v + \lambda^2 v^\top v \\
  &amp;= \| X X^\top X v \|^2 + 2 \lambda \| X v \|^2 + \lambda^2 \| v \|^2 \\
  &amp;\geq \lambda^2 \| v \|^2 \\
  &amp;&gt; 0.
\end{align*}\]</span>
This shows, by theorem <a href="Sx1-matrices.html#thm:matrix-inverse">A.1</a>, that
the matrix <span class="math inline">\(X^\top X + \lambda I\)</span>
is invertible for every <span class="math inline">\(\lambda &gt; 0\)</span>. Thus the
ridge regression estimate for <span class="math inline">\(\lambda &gt; 0\)</span> is always uniquely defined,
even in cases where the least squares estimate is <em>not</em> uniquely defined.</p>
<p>There are variants of the method where the penalty term <span class="math inline">\(\|\beta\|^2\)</span> is is
replaced with a different penalty term. One example of this is Lasso (least
absolute shrinkage and selection operator) regression, which uses <span class="math inline">\(\|\beta\|_1 = \sum_{j=0}^p |\beta_j|\)</span> as the penalty term.</p>
<p>Outside statistics, ridge regression is known as <strong>Tikhonov regularisation</strong>.</p>
</div>
<div id="properties-of-the-estimate" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> Properties of the Estimate</h2>
<p>Using the statistical model
<span class="math display">\[\begin{equation*}
  Y = X \beta + \varepsilon
\end{equation*}\]</span>
as before, we have
<span class="math display">\[\begin{align*}
  \hat\beta^{(\lambda)}
  &amp;= (X^\top X + \lambda I)^{-1} X^\top Y \\
  &amp;= (X^\top X + \lambda I)^{-1} X^\top (X \beta + \varepsilon) \\
  &amp;= (X^\top X + \lambda I)^{-1} (X^\top X + \lambda I) \beta \\
    &amp;\hskip2cm - (X^\top X + \lambda I)^{-1} X^\top X \lambda I \beta \\
    &amp;\hskip2cm + (X^\top X + \lambda I)^{-1} X^\top \varepsilon
\end{align*}\]</span>
and simplifying the first term on the right-hand side we get
<span class="math display" id="eq:ridge-def">\[\begin{equation}
  \hat\beta^{(\lambda)}
  = \beta
    - \lambda (X^\top X + \lambda I)^{-1} X^\top X \beta
    + (X^\top X + \lambda I)^{-1} X^\top \varepsilon.   \tag{12.2}
\end{equation}\]</span></p>
<div id="bias" class="section level3" number="12.2.1">
<h3><span class="header-section-number">12.2.1</span> Bias</h3>
<p>Using the formula for <span class="math inline">\(\hat\beta^{(\lambda)}\)</span> we get
<span class="math display">\[\begin{equation*}
  \mathbb{E}\bigl( \hat\beta^{(\lambda)} \bigr)
  = \beta - \lambda (X^\top X + \lambda I)^{-1} X^\top X \beta + 0
\end{equation*}\]</span>
and thus
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{bias}}\bigl( \hat\beta^{(\lambda)} \bigr)
  = - \lambda (X^\top X + \lambda I)^{-1} X^\top X \beta.
\end{equation*}\]</span>
Since this term in general is non-zero, we see that <span class="math inline">\(\hat\beta^{(\lambda)}\)</span>
is a biased estimate. The amount of bias depends on the unknown, true
coefficient vector <span class="math inline">\(\beta\)</span>.</p>
<p>If <span class="math inline">\(X^\top X\)</span> is invertible, we have for small <span class="math inline">\(\lambda\)</span> that
<span class="math inline">\((X^\top X + \lambda I)^{-1} X^\top X \approx I\)</span> and thus
<span class="math display">\[\begin{equation*}
  \mathbb{E}\bigl( \hat\beta^{(\lambda)} \bigr)
  \approx \beta - \lambda \beta
  = (1-\lambda) \beta.
\end{equation*}\]</span>
In this sense we can see that the effect of <span class="math inline">\(\lambda\)</span> is to “shrink”
the estimate <span class="math inline">\(\hat\beta\)</span>.</p>
</div>
<div id="variance" class="section level3" number="12.2.2">
<h3><span class="header-section-number">12.2.2</span> Variance</h3>
<p>The covariance matrix of the vector <span class="math inline">\(\hat\beta^{(\lambda)}\)</span> is given
by
<span class="math display">\[\begin{align*}
  \mathop{\mathrm{Cov}}\bigl(\hat\beta^{(\lambda)}\bigr)
  &amp;= \mathop{\mathrm{Cov}}\Bigl(
      \beta
      - \lambda (X^\top X + \lambda I)^{-1} X^\top X \beta
      + (X^\top X + \lambda I)^{-1} X^\top \varepsilon
    \Bigr) \\
  &amp;= \mathop{\mathrm{Cov}}\Bigl(
      (X^\top X + \lambda I)^{-1} X^\top \varepsilon
    \Bigr) \\
  &amp;= (X^\top X + \lambda I)^{-1} X^\top \mathop{\mathrm{Cov}}(\varepsilon) X (X^\top X + \lambda I)^{-1} \\
  &amp;= \sigma^2 (X^\top X + \lambda I)^{-1} X^\top X (X^\top X + \lambda I)^{-1} \\
  &amp;= \sigma^2 (X^\top X + \lambda I)^{-1} - \lambda \sigma^2 (X^\top X + \lambda I)^{-2}.
\end{align*}\]</span>
While this is an explicit formula, the resulting covariance matrix
depends on the unknown error variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>If <span class="math inline">\(X^\top X\)</span> is invertible, this covariance will convert to the
covariance of the least squares estimator as <span class="math inline">\(\lambda \downarrow 0\)</span>.
From this we can find the variance of individual components
as <span class="math inline">\(\mathop{\mathrm{Var}}\bigl( \hat\beta^{(\lambda)}_j \bigr) = \mathop{\mathrm{Cov}}( \hat\beta^{(\lambda)} )_{jj}\)</span>.</p>
</div>
<div id="mean-squared-error" class="section level3" number="12.2.3">
<h3><span class="header-section-number">12.2.3</span> Mean Squared Error</h3>
<p>The Mean Squared Error (MSE) of an estimator <span class="math inline">\(\hat \beta_j\)</span> for <span class="math inline">\(\beta_j\)</span> is
given by
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{MSE}}\nolimits( \hat\beta_j )
  = \mathbb{E}\Bigl( \bigl( \hat\beta_j - \beta_j \bigr)^2 \Bigr).
\end{equation*}\]</span>
It is an easy exercise to show that this can equivalently be written as
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{MSE}}\nolimits\bigl( \hat\beta_j \bigr)
  = \mathop{\mathrm{Var}}\bigl( \hat\beta_j \bigr) + \mathop{\mathrm{bias}}\bigl( \hat\beta_j \bigr)^2.
\end{equation*}\]</span>
Using the formulas for the variance and bias from above, we
can find an formula for <span class="math inline">\(\mathop{\mathrm{MSE}}\nolimits(\hat\beta^{(\lambda)}_j)\)</span>,
but the result is hard to interpret. Instead of considering these analytical
expressions, we illustrate the mean squared error using a numerical
example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-51" class="example"><strong>Example 12.2  </strong></span>Continuing from example <a href="S12-ridge.html#exm:knorkel">12.1</a> above, we can determine
the mean squared error for every value of <span class="math inline">\(\lambda\)</span>. Here we
use the fact that this is simulated data where we know the true
values of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>We start with the bias:</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="S12-ridge.html#cb208-1" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb208-2"><a href="S12-ridge.html#cb208-2" aria-hidden="true" tabindex="-1"></a>Q <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> lambda <span class="sc">*</span> <span class="fu">diag</span>(<span class="fu">ncol</span>(X))</span>
<span id="cb208-3"><a href="S12-ridge.html#cb208-3" aria-hidden="true" tabindex="-1"></a>lambda <span class="sc">*</span> <span class="fu">solve</span>(Q, <span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">%*%</span> beta)</span></code></pre></div>
<pre class="rOutput"><code>                    [,1]
(Intercept) 0.0009178679
x1          0.0497789499
x2          0.0499545553</code></pre>
<p>For the covariance matrix we get the following:</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="S12-ridge.html#cb210-1" aria-hidden="true" tabindex="-1"></a>C1 <span class="ot">&lt;-</span> <span class="fu">solve</span>(Q) <span class="co"># Compute the inverse Q^{-1} ...</span></span>
<span id="cb210-2"><a href="S12-ridge.html#cb210-2" aria-hidden="true" tabindex="-1"></a>C2 <span class="ot">&lt;-</span> C1 <span class="sc">%*%</span> C1 <span class="co"># ... and also Q^{-2}, to get</span></span>
<span id="cb210-3"><a href="S12-ridge.html#cb210-3" aria-hidden="true" tabindex="-1"></a>sigma<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> (C1 <span class="sc">-</span> lambda<span class="sc">*</span>C2) <span class="co"># the covariance matrix.</span></span></code></pre></div>
<pre class="rOutput"><code>              (Intercept)            x1            x2
(Intercept)  7.291379e-05 -7.580602e-06 -9.226941e-06
x1          -7.580602e-06  3.831264e-06 -1.540098e-06
x2          -9.226941e-06 -1.540098e-06  4.221464e-06</code></pre>
<p>Now we repeat these calculations in a loop, to plot the mean squared
error as a function of <span class="math inline">\(\lambda\)</span>:</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="S12-ridge.html#cb212-1" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">0</span>, <span class="at">length.out =</span> <span class="dv">100</span>) <span class="co"># range of lambda to try</span></span>
<span id="cb212-2"><a href="S12-ridge.html#cb212-2" aria-hidden="true" tabindex="-1"></a>j.plus<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">1</span> <span class="co"># we plot the error of \beta_1</span></span>
<span id="cb212-3"><a href="S12-ridge.html#cb212-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-4"><a href="S12-ridge.html#cb212-4" aria-hidden="true" tabindex="-1"></a>variance <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(lambda))</span>
<span id="cb212-5"><a href="S12-ridge.html#cb212-5" aria-hidden="true" tabindex="-1"></a>bias <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(lambda))</span>
<span id="cb212-6"><a href="S12-ridge.html#cb212-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(lambda)) {</span>
<span id="cb212-7"><a href="S12-ridge.html#cb212-7" aria-hidden="true" tabindex="-1"></a>    lambda.i <span class="ot">&lt;-</span> lambda[i]</span>
<span id="cb212-8"><a href="S12-ridge.html#cb212-8" aria-hidden="true" tabindex="-1"></a>    Q <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> lambda.i <span class="sc">*</span> <span class="fu">diag</span>(<span class="fu">ncol</span>(X))</span>
<span id="cb212-9"><a href="S12-ridge.html#cb212-9" aria-hidden="true" tabindex="-1"></a>    C1 <span class="ot">&lt;-</span> <span class="fu">solve</span>(Q)</span>
<span id="cb212-10"><a href="S12-ridge.html#cb212-10" aria-hidden="true" tabindex="-1"></a>    bias[i] <span class="ot">&lt;-</span> (lambda.i <span class="sc">*</span> <span class="fu">solve</span>(Q, <span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">%*%</span> beta))[j.plus<span class="fl">.1</span>]</span>
<span id="cb212-11"><a href="S12-ridge.html#cb212-11" aria-hidden="true" tabindex="-1"></a>    C2 <span class="ot">&lt;-</span> C1 <span class="sc">%*%</span> C1</span>
<span id="cb212-12"><a href="S12-ridge.html#cb212-12" aria-hidden="true" tabindex="-1"></a>    C <span class="ot">&lt;-</span> C1 <span class="sc">-</span> lambda.i<span class="sc">*</span>C2</span>
<span id="cb212-13"><a href="S12-ridge.html#cb212-13" aria-hidden="true" tabindex="-1"></a>    variance[i] <span class="ot">&lt;-</span> sigma<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> C[j.plus<span class="fl">.1</span>, j.plus<span class="fl">.1</span>]</span>
<span id="cb212-14"><a href="S12-ridge.html#cb212-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb212-15"><a href="S12-ridge.html#cb212-15" aria-hidden="true" tabindex="-1"></a>MSE <span class="ot">&lt;-</span> variance <span class="sc">+</span> bias<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb212-16"><a href="S12-ridge.html#cb212-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-17"><a href="S12-ridge.html#cb212-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lambda, MSE, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">log =</span> <span class="st">&quot;x&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.1</span>))</span>
<span id="cb212-18"><a href="S12-ridge.html#cb212-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(lambda, bias<span class="sc">^</span><span class="dv">2</span>, <span class="at">col=</span><span class="st">&quot;#1b9e77&quot;</span>, <span class="at">lw =</span> <span class="dv">3</span>)</span>
<span id="cb212-19"><a href="S12-ridge.html#cb212-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(lambda, variance, <span class="at">col=</span><span class="st">&quot;#d95f02&quot;</span>, <span class="at">lw =</span> <span class="dv">3</span>)</span>
<span id="cb212-20"><a href="S12-ridge.html#cb212-20" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(lambda, MSE) <span class="co"># make sure MSE is not obscured by bias/variance</span></span>
<span id="cb212-21"><a href="S12-ridge.html#cb212-21" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> lambda[<span class="fu">which.min</span>(MSE)]) <span class="co"># mark the minimum</span></span>
<span id="cb212-22"><a href="S12-ridge.html#cb212-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-23"><a href="S12-ridge.html#cb212-23" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>,</span>
<span id="cb212-24"><a href="S12-ridge.html#cb212-24" aria-hidden="true" tabindex="-1"></a>       <span class="fu">c</span>(<span class="st">&quot;bias squared&quot;</span>, <span class="st">&quot;variance&quot;</span>, <span class="st">&quot;MSE&quot;</span>),</span>
<span id="cb212-25"><a href="S12-ridge.html#cb212-25" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;#1b9e77&quot;</span>, <span class="st">&quot;#d95f02&quot;</span>, <span class="st">&quot;black&quot;</span>),</span>
<span id="cb212-26"><a href="S12-ridge.html#cb212-26" aria-hidden="true" tabindex="-1"></a>       <span class="at">lw =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1</span>))</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/ridge-MSE-1.png" width="672" /></p>
<p>The vertical line at <span class="math inline">\(\lambda = 0.0183\)</span>
marks the optimal value of <span class="math inline">\(\lambda\)</span>. We can see that for small <span class="math inline">\(\lambda\)</span> the
MSE is dominated by the variance, whereas for large <span class="math inline">\(\lambda\)</span> the main
contribution is from the bias.</p>
</div>
</div>
</div>
<div id="standardisation" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> Standardisation</h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/5ueJiQi5EWQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>While ridge regression can be applied directly to the original data, the
penalty is only comparable between the components of <span class="math inline">\(\beta\)</span>, if the components
have similar magnitude. Also, we do not want a method which is depends on the
units in which the <span class="math inline">\(x\)</span> variables are measured. To address these issues, it is
common practice to first standardise each of the <span class="math inline">\(x\)</span>. In this way the
penalty will apply equally to all of the coefficients, and the results can be
transformed back to obtain <span class="math inline">\(\hat{y}\)</span> on the original scale.</p>
<p>We standardise every column of the input separately: For
the input variable <span class="math inline">\(x_j\)</span>, where <span class="math inline">\(j\in\{1, \ldots, p\}\)</span>, we get
<span class="math display">\[\begin{equation*}
  w_{ij}
  := \frac{x_{ij} - \overline{x_j}}{\mathrm{s}_{x_j}},
\end{equation*}\]</span>
for all <span class="math inline">\(i \in \{1, \ldots, n\}\)</span>.
where <span class="math inline">\(\overline{x_j} = \frac1n \sum_{i=1}^n x_{ij}\)</span> and
<span class="math display">\[\begin{equation*}
  \mathrm{s}_{x_j}^2
  = \frac{1}{n-1} \sum_{i=1}^n (x_{ij} - \overline{x_j})^2
\end{equation*}\]</span>
is the sample variance. We similarly standardise the outputs:
<span class="math display">\[\begin{equation*}
  z_i
  := \frac{y_i - \overline{y}}{\mathrm{s}_y},
\end{equation*}\]</span>
where <span class="math inline">\(\overline{y}\)</span> and <span class="math inline">\(\mathrm{s}_y\)</span> are the mean and standard
deviation of the <span class="math inline">\(y_i\)</span>. If we denote the regression coefficients
for the transformed data by <span class="math inline">\(\gamma\)</span>, then the residual sum of
squares for the transformed data is
<span class="math display">\[\begin{equation*}
  r_\mathrm{tfm}(\gamma)
  = \sum_{i=1}^n \bigl(y_i - \gamma_0 - \gamma_1 w_{i1} - \cdots - \gamma_p w_{ip} \bigr)^2.
\end{equation*}\]</span></p>
<p>The transformed inputs satisfy
<span class="math display">\[\begin{equation*}
  \sum_{i=1}^n w_{ij} = 0
\end{equation*}\]</span>
for all <span class="math inline">\(j\in\{1, \ldots, p\}\)</span> and a similar relation holds for the output.
Using these relations we find
<span class="math display">\[\begin{align*}
  r_\mathrm{tfm}(\gamma)
  &amp;= \sum_{i=1}^n \Bigl(
      \bigl( y_i - \gamma_1 w_{i1} - \cdots - \gamma_p w_{ip} \bigr)^2 \Bigr. \\
    &amp;\hskip2cm \Bigl.
      - 2 \gamma_0 \bigl( y_i - \gamma_1 w_{i1} - \cdots - \gamma_p w_{ip} \bigr)
      + \gamma_0^2
    \Bigr) \\
  &amp;= \sum_{i=1}^n
      \bigl( y_i - \gamma_1 w_{i1} - \cdots - \gamma_p w_{ip} \bigr)^2 \\
    &amp;\hskip2cm
      - 2 \gamma_0 \sum_{i=1}^n \bigl( y_i - \gamma_1 w_{i1} - \cdots - \gamma_p w_{ip} \bigr)
      + n \gamma_0^2 \\
  &amp;= \sum_{i=1}^n
      \bigl( y_i - \gamma_1 w_{i1} - \cdots - \gamma_p w_{ip} \bigr)^2 \\
    &amp;\hskip2cm
      - 2 \gamma_0 \Bigl( \sum_{i=1}^n y_i - \gamma_1 \sum_{i=1}^n w_{i1} - \cdots - \gamma_p \sum_{i=1}^n w_{ip} \Bigr)
      + n \gamma_0^2 \\
  &amp;= \sum_{i=1}^n
      \bigl( y_i - \gamma_1 w_{i1} - \cdots - \gamma_p w_{ip} \bigr)^2
      + n \gamma_0^2.
\end{align*}\]</span>
Thus, the coefficient for the intercept can be minimised separately and
the optimal value is always <span class="math inline">\(\gamma_0 = 0\)</span>. For this reason we do
not include the intercept in our model for the standardised data.
The design matrix is thus
<span class="math display">\[\begin{equation*}
  W
  = \begin{pmatrix}
    w_{1,1} &amp; \cdots &amp; w_{1,p} \\
    w_{2,1} &amp; \cdots &amp; w_{2,p} \\
    \vdots &amp; \ddots &amp; \vdots \\
    w_{n,1} &amp; \cdots &amp; w_{n,p}
  \end{pmatrix} \in \mathbb{R}^{n\times p},
\end{equation*}\]</span>
and the reduced coefficient vector is <span class="math inline">\(\gamma \in \mathbb{R}^p\)</span>.
The least squares estimator for the transformed data is then given by
<span class="math display">\[\begin{equation*}
  \hat\gamma
  = (W^\top W)^{-1} W^\top z
\end{equation*}\]</span>
and the ridge regression estimate is given by
<span class="math display">\[\begin{equation*}
  \hat\gamma^{(\lambda)}
  = (W^\top W + \lambda I)^{-1} W^\top z.
\end{equation*}\]</span></p>
<p>To transform back regression estimates obtained for the transformed data,
we need to revert the standardisation: we have
<span class="math display">\[\begin{equation*}
  x_{ij}
  = \mathrm{s}_{x_j} w_{ij} + \overline{x_j}
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
  y_i
  = \mathrm{s}_y z_i + \overline{y}.
\end{equation*}\]</span>
To obtain fitted values for an input <span class="math inline">\((\tilde x_1, \ldots, \tilde x_p)\)</span>
we have to first transform these inputs:
<span class="math inline">\(\tilde w_j = (\tilde x_j - \overline{x_j}) / \mathrm{s}_{x_j}\)</span>.
Note that the mean and standard deviation come from the data used to fit
the model, and are not computed from the <span class="math inline">\(\tilde x_i\)</span>.
We then find the model mean for the transformed data:
<span class="math display">\[\begin{align*}
  \hat z
  &amp;= \sum_{j=1}^p \hat\gamma_j \tilde w_j \\
  &amp;= \sum_{j=1}^p \hat\gamma_j \frac{\tilde x_j - \overline{x_j}}{\mathrm{s}_{x_j}} \\
  &amp;= - \sum_{k=1}^p \frac{1}{\mathrm{s}_{x_k}} \hat\gamma_k \overline{x_k}
    + \sum_{j=1}^p \frac{1}{\mathrm{s}_{x_j}} \hat\gamma_j \tilde x_j.
\end{align*}\]</span>
Finally, transforming back the response we get
<span class="math display">\[\begin{align*}
  \hat y
  &amp;= \bigl( \overline{y} - \sum_{k=1}^p \frac{\mathrm{s}_y}{\mathrm{s}_{x_k}} \hat\gamma_k \overline{x_k} \bigr)
    + \sum_{j=1}^p \frac{\mathrm{s}_y}{\mathrm{s}_{x_j}} \hat\gamma_j \tilde x_j \\
  &amp;= \hat\beta_0 + \sum_{j=1}^p \hat\beta_j \tilde x_j,
\end{align*}\]</span>
where
<span class="math display">\[\begin{equation*}
  \hat\beta_j
  = \frac{\mathrm{s}_y}{\mathrm{s}_{x_j}} \hat\gamma_j
\end{equation*}\]</span>
for all <span class="math inline">\(j \in \{1, \ldots, p\}\)</span> and
<span class="math display">\[\begin{equation*}
  \hat\beta_0
  = \overline{y} - \sum_{k=1}^p \frac{\mathrm{s}_y}{\mathrm{s}_{x_k}} \hat\gamma_k \overline{x_k}
  = \overline{y} - \sum_{k=1}^p \hat\beta_k \overline{x_k}.
\end{equation*}\]</span>
This transformation can be used both for least squares regression
and ridge regression estimates.</p>
<div class="mysummary">
<p><strong>Summary</strong></p>
<ul>
<li>Ridge regression is an alternative estimator for the regression coefficients.</li>
<li>The method is useful when multicollinearity is present.</li>
<li>Often it is advantageous to standardise the data before performing
ridge regression.</li>
</ul>
</div>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="S11-improving.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Sx1-matrices.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH3714.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
