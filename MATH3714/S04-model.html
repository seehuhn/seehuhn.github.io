<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 4 Properties of the Least Squares Estimate | MATH3714 Linear Regression and Robustness</title>
  <meta name="description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2025/26" />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 4 Properties of the Least Squares Estimate | MATH3714 Linear Regression and Robustness" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2025/26" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 4 Properties of the Least Squares Estimate | MATH3714 Linear Regression and Robustness" />
  
  <meta name="twitter:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2025/26" />
  

<meta name="author" content="Jochen Voss" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="P01.html"/>
<link rel="next" href="S05-single.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="jvstyle.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">MATH3714 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="S01-simple.html"><a href="S01-simple.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="S01-simple.html"><a href="S01-simple.html#residual-sum-of-squares"><i class="fa fa-check"></i><b>1.1</b> Residual Sum of Squares</a></li>
<li class="chapter" data-level="1.2" data-path="S01-simple.html"><a href="S01-simple.html#linear-regression-as-a-parameter-estimation-problem"><i class="fa fa-check"></i><b>1.2</b> Linear Regression as a Parameter Estimation Problem</a></li>
<li class="chapter" data-level="1.3" data-path="S01-simple.html"><a href="S01-simple.html#sec:simple-mat"><i class="fa fa-check"></i><b>1.3</b> Matrix Notation</a></li>
</ul></li>
<li class="part"><span><b>I Theory</b></span></li>
<li class="chapter" data-level="2" data-path="S02-multiple.html"><a href="S02-multiple.html"><i class="fa fa-check"></i><b>2</b> Least Squares Estimates</a>
<ul>
<li class="chapter" data-level="2.1" data-path="S02-multiple.html"><a href="S02-multiple.html#data-and-models"><i class="fa fa-check"></i><b>2.1</b> Data and Models</a></li>
<li class="chapter" data-level="2.2" data-path="S02-multiple.html"><a href="S02-multiple.html#the-normal-equations"><i class="fa fa-check"></i><b>2.2</b> The Normal Equations</a></li>
<li class="chapter" data-level="2.3" data-path="S02-multiple.html"><a href="S02-multiple.html#fitted-values"><i class="fa fa-check"></i><b>2.3</b> Fitted Values</a></li>
<li class="chapter" data-level="2.4" data-path="S02-multiple.html"><a href="S02-multiple.html#example"><i class="fa fa-check"></i><b>2.4</b> Example</a></li>
<li class="chapter" data-level="2.5" data-path="S02-multiple.html"><a href="S02-multiple.html#models-without-intercept"><i class="fa fa-check"></i><b>2.5</b> Models Without Intercept</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html"><i class="fa fa-check"></i>Interlude: Linear Regression in R</a>
<ul>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-fitting"><i class="fa fa-check"></i>Fitting a Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-model"><i class="fa fa-check"></i>Understanding the Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-predict"><i class="fa fa-check"></i>Making Predictions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="S03-cov.html"><a href="S03-cov.html"><i class="fa fa-check"></i><b>3</b> Random Vectors and Covariance</a>
<ul>
<li class="chapter" data-level="3.1" data-path="S03-cov.html"><a href="S03-cov.html#expectation"><i class="fa fa-check"></i><b>3.1</b> Expectation</a></li>
<li class="chapter" data-level="3.2" data-path="S03-cov.html"><a href="S03-cov.html#sec:covariance"><i class="fa fa-check"></i><b>3.2</b> Covariance Matrix</a></li>
<li class="chapter" data-level="3.3" data-path="S03-cov.html"><a href="S03-cov.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>3.3</b> The Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P01.html"><a href="P01.html"><i class="fa fa-check"></i>Problem Sheet 1</a></li>
<li class="chapter" data-level="4" data-path="S04-model.html"><a href="S04-model.html"><i class="fa fa-check"></i><b>4</b> Properties of the Least Squares Estimate</a>
<ul>
<li class="chapter" data-level="4.1" data-path="S04-model.html"><a href="S04-model.html#mean-and-covariance"><i class="fa fa-check"></i><b>4.1</b> Mean and Covariance</a></li>
<li class="chapter" data-level="4.2" data-path="S04-model.html"><a href="S04-model.html#hat-matrix"><i class="fa fa-check"></i><b>4.2</b> Properties of the Hat Matrix</a></li>
<li class="chapter" data-level="4.3" data-path="S04-model.html"><a href="S04-model.html#Cochran"><i class="fa fa-check"></i><b>4.3</b> Cochran’s theorem</a></li>
<li class="chapter" data-level="4.4" data-path="S04-model.html"><a href="S04-model.html#var-est-bias"><i class="fa fa-check"></i><b>4.4</b> Estimating the Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="S05-single.html"><a href="S05-single.html"><i class="fa fa-check"></i><b>5</b> Uncertainty for Individual Regression Coefficients</a>
<ul>
<li class="chapter" data-level="5.1" data-path="S05-single.html"><a href="S05-single.html#measuring-the-estimation-error"><i class="fa fa-check"></i><b>5.1</b> Measuring the Estimation Error</a></li>
<li class="chapter" data-level="5.2" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.3" data-path="S05-single.html"><a href="S05-single.html#hypothesis-tests"><i class="fa fa-check"></i><b>5.3</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="5.4" data-path="S05-single.html"><a href="S05-single.html#r-experiments"><i class="fa fa-check"></i><b>5.4</b> R Experiments</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="S05-single.html"><a href="S05-single.html#fitting-the-model"><i class="fa fa-check"></i><b>5.4.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.4.2" data-path="S05-single.html"><a href="S05-single.html#estimating-the-variance-of-the-error"><i class="fa fa-check"></i><b>5.4.2</b> Estimating the Variance of the Error</a></li>
<li class="chapter" data-level="5.4.3" data-path="S05-single.html"><a href="S05-single.html#estimating-the-standard-errors"><i class="fa fa-check"></i><b>5.4.3</b> Estimating the Standard Errors</a></li>
<li class="chapter" data-level="5.4.4" data-path="S05-single.html"><a href="S05-single.html#hypothesis-tests-1"><i class="fa fa-check"></i><b>5.4.4</b> Hypothesis tests</a></li>
<li class="chapter" data-level="5.4.5" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals-1"><i class="fa fa-check"></i><b>5.4.5</b> Confidence Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html"><i class="fa fa-check"></i><b>6</b> Estimating Coefficients Simultaneously</a>
<ul>
<li class="chapter" data-level="6.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-dist"><i class="fa fa-check"></i><b>6.1</b> Linear Combinations of Coefficients</a></li>
<li class="chapter" data-level="6.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-CI"><i class="fa fa-check"></i><b>6.2</b> Confidence Regions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#result"><i class="fa fa-check"></i><b>6.2.1</b> Result</a></li>
<li class="chapter" data-level="6.2.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#numerical-experiments"><i class="fa fa-check"></i><b>6.2.2</b> Numerical Experiments</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-test"><i class="fa fa-check"></i><b>6.3</b> Hypothesis Tests</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html"><i class="fa fa-check"></i>Interlude: Loading Data into R</a>
<ul>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-csv-files"><i class="fa fa-check"></i>Importing CSV Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-microsoft-excel-files"><i class="fa fa-check"></i>Importing Microsoft Excel Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#checking-the-imported-data"><i class="fa fa-check"></i>Checking the Imported Data</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#common-problems"><i class="fa fa-check"></i>Common Problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="S07-examples.html"><a href="S07-examples.html"><i class="fa fa-check"></i><b>7</b> Examples</a>
<ul>
<li class="chapter" data-level="7.1" data-path="S07-examples.html"><a href="S07-examples.html#simple-confidence-interval"><i class="fa fa-check"></i><b>7.1</b> Simple Confidence Interval</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles"><i class="fa fa-check"></i><b>7.1.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.1.2" data-path="S07-examples.html"><a href="S07-examples.html#from-the-lm-output"><i class="fa fa-check"></i><b>7.1.2</b> From the <code>lm()</code> Output</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="S07-examples.html"><a href="S07-examples.html#confidence-intervals-for-the-mean"><i class="fa fa-check"></i><b>7.2</b> Confidence Intervals for the Mean</a></li>
<li class="chapter" data-level="7.3" data-path="S07-examples.html"><a href="S07-examples.html#testing-a-single-coefficient"><i class="fa fa-check"></i><b>7.3</b> Testing a Single Coefficient</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-1"><i class="fa fa-check"></i><b>7.3.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.3.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-i"><i class="fa fa-check"></i><b>7.3.2</b> Using the <code>lm()</code> Output, I</a></li>
<li class="chapter" data-level="7.3.3" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-ii"><i class="fa fa-check"></i><b>7.3.3</b> Using the <code>lm()</code> Output, II</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="S07-examples.html"><a href="S07-examples.html#testing-multiple-coefficients"><i class="fa fa-check"></i><b>7.4</b> Testing Multiple Coefficients</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-2"><i class="fa fa-check"></i><b>7.4.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.4.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output"><i class="fa fa-check"></i><b>7.4.2</b> Using the <code>lm()</code> output</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="P02.html"><a href="P02.html"><i class="fa fa-check"></i>Problem Sheet 2</a></li>
<li class="chapter" data-level="8" data-path="S08-influence.html"><a href="S08-influence.html"><i class="fa fa-check"></i><b>8</b> The Influence of Observations</a>
<ul>
<li class="chapter" data-level="8.1" data-path="S08-influence.html"><a href="S08-influence.html#deleting"><i class="fa fa-check"></i><b>8.1</b> Deleting Observations</a></li>
<li class="chapter" data-level="8.2" data-path="S08-influence.html"><a href="S08-influence.html#influence"><i class="fa fa-check"></i><b>8.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="part"><span><b>II Linear Regression in Practice</b></span></li>
<li class="chapter" data-level="9" data-path="S09-plots.html"><a href="S09-plots.html"><i class="fa fa-check"></i><b>9</b> Diagnostic Plots</a>
<ul>
<li class="chapter" data-level="9.1" data-path="S09-plots.html"><a href="S09-plots.html#residual-plots"><i class="fa fa-check"></i><b>9.1</b> Residual Plots</a></li>
<li class="chapter" data-level="9.2" data-path="S09-plots.html"><a href="S09-plots.html#q-q-plots"><i class="fa fa-check"></i><b>9.2</b> Q-Q Plots</a></li>
<li class="chapter" data-level="9.3" data-path="S09-plots.html"><a href="S09-plots.html#other-plot-types"><i class="fa fa-check"></i><b>9.3</b> Other Plot Types</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="S10-diagnostics.html"><a href="S10-diagnostics.html"><i class="fa fa-check"></i><b>10</b> Measures for Model Fit</a>
<ul>
<li class="chapter" data-level="10.1" data-path="S10-diagnostics.html"><a href="S10-diagnostics.html#R-squared"><i class="fa fa-check"></i><b>10.1</b> The Coefficient of Multiple Determination</a></li>
<li class="chapter" data-level="10.2" data-path="S10-diagnostics.html"><a href="S10-diagnostics.html#adjusted-R-squared"><i class="fa fa-check"></i><b>10.2</b> Adjusted <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="10.3" data-path="S10-diagnostics.html"><a href="S10-diagnostics.html#error-var"><i class="fa fa-check"></i><b>10.3</b> Error Variance</a></li>
<li class="chapter" data-level="10.4" data-path="S10-diagnostics.html"><a href="S10-diagnostics.html#PRESS"><i class="fa fa-check"></i><b>10.4</b> Prediction Error Sum of Squares</a></li>
<li class="chapter" data-level="10.5" data-path="S10-diagnostics.html"><a href="S10-diagnostics.html#AIC"><i class="fa fa-check"></i><b>10.5</b> Akaike’s Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I03-lm-output.html"><a href="I03-lm-output.html"><i class="fa fa-check"></i>Interlude: Understanding the <code>lm()</code> Output</a></li>
<li class="chapter" data-level="11" data-path="S11-improving.html"><a href="S11-improving.html"><i class="fa fa-check"></i><b>11</b> Improving the Model Fit</a>
<ul>
<li class="chapter" data-level="11.1" data-path="S11-improving.html"><a href="S11-improving.html#linearising-the-mean"><i class="fa fa-check"></i><b>11.1</b> Linearising the Mean</a></li>
<li class="chapter" data-level="11.2" data-path="S11-improving.html"><a href="S11-improving.html#stabilising-the-variance"><i class="fa fa-check"></i><b>11.2</b> Stabilising the Variance</a></li>
<li class="chapter" data-level="11.3" data-path="S11-improving.html"><a href="S11-improving.html#power-transform"><i class="fa fa-check"></i><b>11.3</b> The Power Transform</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P03.html"><a href="P03.html"><i class="fa fa-check"></i>Problem Sheet 3</a></li>
<li class="chapter" data-level="12" data-path="S12-selection.html"><a href="S12-selection.html"><i class="fa fa-check"></i><b>12</b> Model Selection</a>
<ul>
<li class="chapter" data-level="12.1" data-path="S12-selection.html"><a href="S12-selection.html#candidate-models"><i class="fa fa-check"></i><b>12.1</b> Candidate Models</a></li>
<li class="chapter" data-level="12.2" data-path="S12-selection.html"><a href="S12-selection.html#misspecified-models"><i class="fa fa-check"></i><b>12.2</b> Misspecified Models</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="S12-selection.html"><a href="S12-selection.html#missing-variables"><i class="fa fa-check"></i><b>12.2.1</b> Missing Variables</a></li>
<li class="chapter" data-level="12.2.2" data-path="S12-selection.html"><a href="S12-selection.html#unnecessary-variables"><i class="fa fa-check"></i><b>12.2.2</b> Unnecessary Variables</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="S12-selection.html"><a href="S12-selection.html#sec:nested-models"><i class="fa fa-check"></i><b>12.3</b> Testing Nested Models</a></li>
<li class="chapter" data-level="12.4" data-path="S12-selection.html"><a href="S12-selection.html#choosing-between-models"><i class="fa fa-check"></i><b>12.4</b> Choosing Between Models</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="S12-selection.html"><a href="S12-selection.html#criteria-for-model-selection"><i class="fa fa-check"></i><b>12.4.1</b> Criteria for Model Selection</a></li>
<li class="chapter" data-level="12.4.2" data-path="S12-selection.html"><a href="S12-selection.html#exhaustive-search"><i class="fa fa-check"></i><b>12.4.2</b> Exhaustive Search</a></li>
<li class="chapter" data-level="12.4.3" data-path="S12-selection.html"><a href="S12-selection.html#stepwise-forward-selection"><i class="fa fa-check"></i><b>12.4.3</b> Stepwise Forward Selection</a></li>
<li class="chapter" data-level="12.4.4" data-path="S12-selection.html"><a href="S12-selection.html#stepwise-backward-selection"><i class="fa fa-check"></i><b>12.4.4</b> Stepwise Backward Selection</a></li>
<li class="chapter" data-level="12.4.5" data-path="S12-selection.html"><a href="S12-selection.html#bidirectional-selection"><i class="fa fa-check"></i><b>12.4.5</b> Bidirectional Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="S13-exhaustive.html"><a href="S13-exhaustive.html"><i class="fa fa-check"></i><b>13</b> Exhaustive Model Search</a>
<ul>
<li class="chapter" data-level="13.1" data-path="S13-exhaustive.html"><a href="S13-exhaustive.html#exhaustive-search-1"><i class="fa fa-check"></i><b>13.1</b> Exhaustive Search</a></li>
<li class="chapter" data-level="13.2" data-path="S13-exhaustive.html"><a href="S13-exhaustive.html#search-algorithm"><i class="fa fa-check"></i><b>13.2</b> Search Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="S14-factors.html"><a href="S14-factors.html"><i class="fa fa-check"></i><b>14</b> Factors</a>
<ul>
<li class="chapter" data-level="14.1" data-path="S14-factors.html"><a href="S14-factors.html#indicator-variables"><i class="fa fa-check"></i><b>14.1</b> Indicator Variables</a></li>
<li class="chapter" data-level="14.2" data-path="S14-factors.html"><a href="S14-factors.html#interactions"><i class="fa fa-check"></i><b>14.2</b> Interactions</a></li>
<li class="chapter" data-level="14.3" data-path="S14-factors.html"><a href="S14-factors.html#interactions-example"><i class="fa fa-check"></i><b>14.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="S15-multicoll.html"><a href="S15-multicoll.html"><i class="fa fa-check"></i><b>15</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="15.1" data-path="S15-multicoll.html"><a href="S15-multicoll.html#consequences-of-multicollinearity"><i class="fa fa-check"></i><b>15.1</b> Consequences of Multicollinearity</a></li>
<li class="chapter" data-level="15.2" data-path="S15-multicoll.html"><a href="S15-multicoll.html#detecting-multicollinearity"><i class="fa fa-check"></i><b>15.2</b> Detecting Multicollinearity</a></li>
<li class="chapter" data-level="15.3" data-path="S15-multicoll.html"><a href="S15-multicoll.html#mitigations"><i class="fa fa-check"></i><b>15.3</b> Mitigations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P04.html"><a href="P04.html"><i class="fa fa-check"></i>Problem Sheet 4</a></li>
<li class="chapter" data-level="16" data-path="S16-ridge.html"><a href="S16-ridge.html"><i class="fa fa-check"></i><b>16</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="16.1" data-path="S16-ridge.html"><a href="S16-ridge.html#definition-the-estimator"><i class="fa fa-check"></i><b>16.1</b> Definition the Estimator</a></li>
<li class="chapter" data-level="16.2" data-path="S16-ridge.html"><a href="S16-ridge.html#properties-of-the-estimate"><i class="fa fa-check"></i><b>16.2</b> Properties of the Estimate</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="S16-ridge.html"><a href="S16-ridge.html#bias"><i class="fa fa-check"></i><b>16.2.1</b> Bias</a></li>
<li class="chapter" data-level="16.2.2" data-path="S16-ridge.html"><a href="S16-ridge.html#variance"><i class="fa fa-check"></i><b>16.2.2</b> Variance</a></li>
<li class="chapter" data-level="16.2.3" data-path="S16-ridge.html"><a href="S16-ridge.html#mean-squared-error"><i class="fa fa-check"></i><b>16.2.3</b> Mean Squared Error</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="S16-ridge.html"><a href="S16-ridge.html#standardisation"><i class="fa fa-check"></i><b>16.3</b> Standardisation</a></li>
</ul></li>
<li class="part"><span><b>III Robust Regression</b></span></li>
<li class="chapter" data-level="17" data-path="S17-robust.html"><a href="S17-robust.html"><i class="fa fa-check"></i><b>17</b> Robust Regression</a>
<ul>
<li class="chapter" data-level="17.1" data-path="S17-robust.html"><a href="S17-robust.html#outliers"><i class="fa fa-check"></i><b>17.1</b> Outliers</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="S17-robust.html"><a href="S17-robust.html#leverage"><i class="fa fa-check"></i><b>17.1.1</b> Leverage</a></li>
<li class="chapter" data-level="17.1.2" data-path="S17-robust.html"><a href="S17-robust.html#studentised-residuals"><i class="fa fa-check"></i><b>17.1.2</b> Studentised Residuals</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="S17-robust.html"><a href="S17-robust.html#breakdown-points"><i class="fa fa-check"></i><b>17.2</b> Breakdown Points</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="S18-m-est.html"><a href="S18-m-est.html"><i class="fa fa-check"></i><b>18</b> M-Estimators</a>
<ul>
<li class="chapter" data-level="18.1" data-path="S18-m-est.html"><a href="S18-m-est.html#definition"><i class="fa fa-check"></i><b>18.1</b> Definition</a></li>
<li class="chapter" data-level="18.2" data-path="S18-m-est.html"><a href="S18-m-est.html#iterative-methods"><i class="fa fa-check"></i><b>18.2</b> Iterative Methods</a></li>
<li class="chapter" data-level="18.3" data-path="S18-m-est.html"><a href="S18-m-est.html#objective-functions"><i class="fa fa-check"></i><b>18.3</b> Objective Functions</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="S18-m-est.html"><a href="S18-m-est.html#least-squares-method"><i class="fa fa-check"></i><b>18.3.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="18.3.2" data-path="S18-m-est.html"><a href="S18-m-est.html#least-absolute-values"><i class="fa fa-check"></i><b>18.3.2</b> Least Absolute Values</a></li>
<li class="chapter" data-level="18.3.3" data-path="S18-m-est.html"><a href="S18-m-est.html#Huber"><i class="fa fa-check"></i><b>18.3.3</b> Huber’s <span class="math inline">\(t\)</span>-function</a></li>
<li class="chapter" data-level="18.3.4" data-path="S18-m-est.html"><a href="S18-m-est.html#hampel"><i class="fa fa-check"></i><b>18.3.4</b> Hampel’s Method</a></li>
<li class="chapter" data-level="18.3.5" data-path="S18-m-est.html"><a href="S18-m-est.html#bisquare"><i class="fa fa-check"></i><b>18.3.5</b> Tukey’s Bisquare Method</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="S18-m-est.html"><a href="S18-m-est.html#breakdown-points-of-m-estimators"><i class="fa fa-check"></i><b>18.4</b> Breakdown Points of M-Estimators</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="S19-efficiency.html"><a href="S19-efficiency.html"><i class="fa fa-check"></i><b>19</b> Efficiency of Robust Estimators</a>
<ul>
<li class="chapter" data-level="19.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#efficiency"><i class="fa fa-check"></i><b>19.1</b> Efficiency</a></li>
<li class="chapter" data-level="19.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#robust-estimators"><i class="fa fa-check"></i><b>19.2</b> Robust estimators</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#LMS"><i class="fa fa-check"></i><b>19.2.1</b> Least Median of Squares</a></li>
<li class="chapter" data-level="19.2.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#LTS"><i class="fa fa-check"></i><b>19.2.2</b> Least Trimmed Squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="I04-robust.html"><a href="I04-robust.html"><i class="fa fa-check"></i>Interlude: Robust Regression in R</a>
<ul>
<li class="chapter" data-level="" data-path="I04-robust.html"><a href="I04-robust.html#m-estimators"><i class="fa fa-check"></i>M-Estimators</a></li>
<li class="chapter" data-level="" data-path="I04-robust.html"><a href="I04-robust.html#lav-estimation"><i class="fa fa-check"></i>LAV Estimation</a></li>
<li class="chapter" data-level="" data-path="I04-robust.html"><a href="I04-robust.html#high-breakdown-point-methods"><i class="fa fa-check"></i>High Breakdown Point Methods</a></li>
<li class="chapter" data-level="" data-path="I04-robust.html"><a href="I04-robust.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P05.html"><a href="P05.html"><i class="fa fa-check"></i>Problem Sheet 5</a></li>
<li class="chapter" data-level="20" data-path="S20-examples.html"><a href="S20-examples.html"><i class="fa fa-check"></i><b>20</b> Examples</a>
<ul>
<li class="chapter" data-level="20.1" data-path="S20-examples.html"><a href="S20-examples.html#comparing-robust-methods"><i class="fa fa-check"></i><b>20.1</b> Comparing Robust Methods</a></li>
<li class="chapter" data-level="20.2" data-path="S20-examples.html"><a href="S20-examples.html#residual-analysis"><i class="fa fa-check"></i><b>20.2</b> Residual Analysis</a></li>
<li class="chapter" data-level="20.3" data-path="S20-examples.html"><a href="S20-examples.html#weight-comparisons"><i class="fa fa-check"></i><b>20.3</b> Weight Comparisons</a></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html"><i class="fa fa-check"></i><b>A</b> Linear Algebra Reminders</a>
<ul>
<li class="chapter" data-level="A.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#vectors"><i class="fa fa-check"></i><b>A.1</b> Vectors</a></li>
<li class="chapter" data-level="A.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-rules"><i class="fa fa-check"></i><b>A.2</b> Matrices</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#transpose"><i class="fa fa-check"></i><b>A.2.1</b> Transpose</a></li>
<li class="chapter" data-level="A.2.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-vector-product"><i class="fa fa-check"></i><b>A.2.2</b> Matrix-vector Product</a></li>
<li class="chapter" data-level="A.2.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-matrix-product"><i class="fa fa-check"></i><b>A.2.3</b> Matrix-matrix Product</a></li>
<li class="chapter" data-level="A.2.4" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#rank"><i class="fa fa-check"></i><b>A.2.4</b> Rank</a></li>
<li class="chapter" data-level="A.2.5" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#trace"><i class="fa fa-check"></i><b>A.2.5</b> Trace</a></li>
<li class="chapter" data-level="A.2.6" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.2.6</b> Matrix Inverse</a></li>
<li class="chapter" data-level="A.2.7" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.2.7</b> Orthogonal Matrices</a></li>
<li class="chapter" data-level="A.2.8" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#positive-definite"><i class="fa fa-check"></i><b>A.2.8</b> Positive Definite Matrices</a></li>
<li class="chapter" data-level="A.2.9" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#idempotent"><i class="fa fa-check"></i><b>A.2.9</b> Idempotent Matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#eigenvalues"><i class="fa fa-check"></i><b>A.3</b> Eigenvalues</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="Sx2-probability.html"><a href="Sx2-probability.html"><i class="fa fa-check"></i><b>B</b> Probability Reminders</a>
<ul>
<li class="chapter" data-level="B.1" data-path="Sx2-probability.html"><a href="Sx2-probability.html#independence"><i class="fa fa-check"></i><b>B.1</b> Independence</a></li>
<li class="chapter" data-level="B.2" data-path="Sx2-probability.html"><a href="Sx2-probability.html#expectation-1"><i class="fa fa-check"></i><b>B.2</b> Expectation</a></li>
<li class="chapter" data-level="B.3" data-path="Sx2-probability.html"><a href="Sx2-probability.html#variance-1"><i class="fa fa-check"></i><b>B.3</b> Variance</a></li>
<li class="chapter" data-level="B.4" data-path="Sx2-probability.html"><a href="Sx2-probability.html#covariance"><i class="fa fa-check"></i><b>B.4</b> Covariance</a></li>
<li class="chapter" data-level="B.5" data-path="Sx2-probability.html"><a href="Sx2-probability.html#well-known-distributions"><i class="fa fa-check"></i><b>B.5</b> Well-Known Distributions</a>
<ul>
<li class="chapter" data-level="B.5.1" data-path="Sx2-probability.html"><a href="Sx2-probability.html#chi-square"><i class="fa fa-check"></i><b>B.5.1</b> The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="B.5.2" data-path="Sx2-probability.html"><a href="Sx2-probability.html#t"><i class="fa fa-check"></i><b>B.5.2</b> The t-distribution</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li class="chapter"><span><b>THE END</b></span></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3714 Linear Regression and Robustness</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="S04-model" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Section 4</span> Properties of the Least Squares Estimate<a href="S04-model.html#S04-model" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!--
TODO:
- This section is too long!
- make Cochran's theorem general enought so that the use in S06 is
  explicitly covered here.
- explain h_{ii} \in [1/n, 1] for S17.  Maybe even (1/n, 1)?
- Maybe show var(\hat\beta_j) ~ 1/sqrt(n)
- identify and cite the source which introduced $\mathrm{SS}_\mathrm{T}$ etc.
-->
<p>In the previous section we introduced random vectors and the multivariate
normal distribution. We now use these tools to build a statistical model
for multiple linear regression. As in the one-dimensional case, we assume
that the errors are random. More precisely we have
<span class="math display" id="eq:lmstats">\[\begin{equation}
  Y = X \beta + \varepsilon.  \tag{4.1}
\end{equation}\]</span>
where <span class="math inline">\(\varepsilon= (\varepsilon_1, \ldots, \varepsilon_n)\)</span> is a random error.
The individual errors <span class="math inline">\(\varepsilon_1, \ldots, \varepsilon_n\)</span>
are assumed to be i.i.d. random variables with <span class="math inline">\(\varepsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span>.</p>
<p>Remarks:</p>
<ul>
<li><p>The parameters in this model are the regression coefficients
<span class="math inline">\(\beta = (\beta_0, \beta_1, \cdots, \beta_p) \in \mathbb{R}^{p+1}\)</span> and
the error variance <span class="math inline">\(\sigma^2\)</span>.</p></li>
<li><p>Using the notation from the previous section, we have
<span class="math inline">\(\varepsilon\sim \mathcal{N}(0, \sigma^2 I)\)</span> and <span class="math inline">\(Y \sim \mathcal{N}(X\beta, \sigma^2 I)\)</span>.</p></li>
<li><p>The model assumes that the <span class="math inline">\(x\)</span>-values are fixed and known. The only
random quantities in the model are <span class="math inline">\(\varepsilon_i\)</span> and <span class="math inline">\(Y_i\)</span>. This is a modelling
assumption, there are more general models where the <span class="math inline">\(x\)</span>-values are also
random.</p></li>
</ul>
<p>Having defined the model, our next task is to understand the properties of
the least squares estimator <span class="math inline">\(\hat\beta\)</span> when applied to data from this model.
We study these properties by treating the estimates as random variables.</p>
<div id="mean-and-covariance" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Mean and Covariance<a href="S04-model.html#mean-and-covariance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/s52WJNkRM00" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>We begin by examining the bias of the estimator. To find <span class="math inline">\(\mathop{\mathrm{bias}}(\hat\beta) =
\mathbb{E}(\hat\beta) - \beta\)</span> we need to determine the expectation of <span class="math inline">\(\hat\beta =
(X^\top X)^{-1} X^\top Y\)</span>, where <span class="math inline">\(Y\)</span> is the random vector from
the model <a href="S04-model.html#eq:lmstats">(4.1)</a>.</p>
<div class="lemma">
<p><span id="lem:hat-beta-dist" class="lemma"><strong>Lemma 4.1  </strong></span>We have</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\hat\beta = \beta + (X^\top X)^{-1} X^\top \varepsilon\)</span> and</p></li>
<li><p><span class="math inline">\(\hat\beta \sim \mathcal{N}\bigl( \beta, \sigma^2 (X^\top X)^{-1} \bigr)\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-13" class="proof"><em>Proof</em>. </span>From lemma <a href="S02-multiple.html#lem:multiple-LSQ">2.1</a> we know
<span class="math display">\[\begin{equation*}
  \hat\beta
  = (X^\top X)^{-1} X^\top Y,
\end{equation*}\]</span>
Using the definition of <span class="math inline">\(Y\)</span> we can write this as
<span class="math display">\[\begin{align*}
  \hat\beta
  &amp;= (X^\top X)^{-1} X^\top Y \\
  &amp;= (X^\top X)^{-1} X^\top (X\beta + \varepsilon) \\
  &amp;= (X^\top X)^{-1} X^\top X \beta + (X^\top X)^{-1} X^\top \varepsilon\\
  &amp;= \beta + (X^\top X)^{-1} X^\top \varepsilon.
\end{align*}\]</span>
This proves the first claim.</p>
<p>Since <span class="math inline">\(\varepsilon\)</span> follows a multivariate normal distribution,
<span class="math inline">\(\beta + (X^\top X)^{-1} X^\top \varepsilon\)</span> is also normally distributed.
Taking expectations we get
<span class="math display">\[\begin{align*}
  \mathbb{E}(\hat\beta)
  &amp;= \mathbb{E}\bigl( \beta + (X^\top X)^{-1} X^\top \varepsilon\bigr) \\
  &amp;= \beta + (X^\top X)^{-1} X^\top \mathbb{E}(\varepsilon) \\
  &amp;= \beta,
\end{align*}\]</span>
since <span class="math inline">\(\mathbb{E}(\varepsilon) = 0\)</span>.</p>
<p>For the covariance we find
<span class="math display">\[\begin{align*}
  \mathop{\mathrm{Cov}}(\hat\beta)
  &amp;= \mathop{\mathrm{Cov}}\bigl( \beta + (X^\top X)^{-1} X^\top \varepsilon\bigr) \\
  &amp;= \mathop{\mathrm{Cov}}\bigl( (X^\top X)^{-1} X^\top \varepsilon\bigr) \\
  &amp;= (X^\top X)^{-1} X^\top \mathop{\mathrm{Cov}}(\varepsilon) \bigl( (X^\top X)^{-1} X^\top \bigr)^\top \\
  &amp;= (X^\top X)^{-1} X^\top \mathop{\mathrm{Cov}}(\varepsilon) X (X^\top X)^{-1}.
\end{align*}\]</span>
Since <span class="math inline">\(\mathop{\mathrm{Cov}}(\varepsilon) = \sigma^2 I\)</span>, this simplifies to
<span class="math display">\[\begin{align*}
  \mathop{\mathrm{Cov}}(\hat\beta)
  &amp;= (X^\top X)^{-1} X^\top \sigma^2 I X (X^\top X)^{-1} \\
  &amp;= \sigma^2 (X^\top X)^{-1} X^\top X (X^\top X)^{-1} \\
  &amp;= \sigma^2 (X^\top X)^{-1}.
\end{align*}\]</span>
This completes the proof.</p>
</div>
<p>The lemma implies that <span class="math inline">\(\mathbb{E}(\hat\beta) = \beta\)</span>, <em>i.e.</em> the estimator
<span class="math inline">\(\hat\beta\)</span> is unbiased. Note that for this statement we only used <span class="math inline">\(\mathbb{E}(\varepsilon) =
0\)</span> to compute the expectation of <span class="math inline">\(\hat\beta\)</span>. Thus, the estimator will still
be unbiased for correlated noise or for noise which is not normally distributed.</p>
<p>We have seen earlier in equation <a href="S03-cov.html#eq:Cov-diag-elem">(3.2)</a> that the diagonal elements
of a covariance matrix give the variances of the elements of the random vector.
Setting <span class="math inline">\(C := (X^\top X)^{-1}\)</span> as a shorthand here, we find that the
individual estimated coefficients <span class="math inline">\(\hat\beta_j\)</span> satisfy
<span class="math display" id="eq:beta-hat-j">\[\begin{equation}
  \hat\beta_j
  \sim \mathcal{N}\bigl( \beta_j, \sigma^2 C_{jj} \bigr)  \tag{4.2}
\end{equation}\]</span>
for <span class="math inline">\(j \in \{0, \ldots, p\}\)</span>.</p>
<p>Before we can use these results in practice, we need to address a problem:
the error variance <span class="math inline">\(\sigma^2\)</span> is usually unknown. We will return to
estimating <span class="math inline">\(\sigma^2\)</span> in section <a href="S04-model.html#var-est-bias">4.4</a> below.</p>
</div>
<div id="hat-matrix" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Properties of the Hat Matrix<a href="S04-model.html#hat-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/ObG3Q5xgrWM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>Before we can estimate the error variance <span class="math inline">\(\sigma^2\)</span>, we need to establish some
important properties of the hat matrix <span class="math inline">\(H = X (X^\top X)^{-1} X^\top\)</span>. These
properties will be essential tools for deriving the distribution of our
variance estimator.</p>
<div class="lemma">
<p><span id="lem:hat-matrix-props" class="lemma"><strong>Lemma 4.2  </strong></span>The hat matrix <span class="math inline">\(H\)</span> has the following properties:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(H^\top = H\)</span></li>
<li><span class="math inline">\((I-H)^\top = I-H\)</span></li>
<li><span class="math inline">\(H^2 = H\)</span></li>
<li><span class="math inline">\((I-H)^2 = I-H\)</span></li>
<li><span class="math inline">\(HX = X\)</span></li>
<li><span class="math inline">\((I-H)X = 0\)</span></li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-14" class="proof"><em>Proof</em>. </span>For property 1 we have
<span class="math display">\[\begin{align*}
  H^\top
  &amp;= \bigl( X (X^\top X)^{-1} X^\top \bigr)^\top \\
  &amp;= (X^\top)^\top \bigl((X^\top X)^{-1}\bigr)^\top X^\top \\
  &amp;= X (X^\top X)^{-1} X^\top \\
  &amp;= H,
\end{align*}\]</span>
where we used that the inverse of a symmetric matrix is symmetric.</p>
<p>Property 2 follows from property 1, since
<span class="math display">\[\begin{equation*}
  (I - H)^\top = I^\top - H^\top = I - H.
\end{equation*}\]</span></p>
<p>For property 3 we have
<span class="math display">\[\begin{align*}
  H^2
  &amp;= \bigl( X (X^\top X)^{-1} X^\top \bigr) \bigl( X (X^\top X)^{-1} X^\top \bigr) \\
  &amp;= X \bigl( (X^\top X)^{-1} X^\top X \bigr) (X^\top X)^{-1} X^\top \\
  &amp;= X (X^\top X)^{-1} X^\top \\
  &amp;= H.
\end{align*}\]</span></p>
<p>Property 4 follows from property 3, since
<span class="math display">\[\begin{equation*}
  (I - H)^2 = I - 2H + H^2 = I - 2H + H = I - H.
\end{equation*}\]</span></p>
<p>For property 5 we have
<span class="math display">\[\begin{equation*}
  H X = X (X^\top X)^{-1} X^\top X = X I = X.
\end{equation*}\]</span></p>
<p>Property 6 follows from property 5, since
<span class="math display">\[\begin{equation*}
  (I - H) X = IX - HX = X - X = 0.
\end{equation*}\]</span>
This completes the proof.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-15" class="remark"><em>Remark</em>. </span>Properties 1 and 2 state that <span class="math inline">\(H\)</span> and <span class="math inline">\(I - H\)</span> are <strong>symmetric</strong>.
Properties 3 and 4 state that <span class="math inline">\(H\)</span> and <span class="math inline">\(I - H\)</span> are <strong>idempotent</strong>.</p>
</div>
<p>Before proceeding to further properties of <span class="math inline">\(H\)</span>, we note without proof that
geometrically, <span class="math inline">\(H\)</span> can be interpreted as the orthogonal projection onto
the subspace of <span class="math inline">\(\mathbb{R}^n\)</span> which is spanned by the columns of <span class="math inline">\(X\)</span>. This subspace
contains the possible output vectors
of the linear system and the least squares procedure finds the point <span class="math inline">\(\hat y\)</span>
in this subspace which is closest to the observed data <span class="math inline">\(y\in\mathbb{R}^n\)</span>.</p>
<p>We can now use the transformation properties of the multivariate normal
distribution to find the distributions of the fitted values and residuals.</p>
<div class="lemma">
<p><span id="lem:hat-y-eps-dist" class="lemma"><strong>Lemma 4.3  </strong></span>Under the model <a href="S04-model.html#eq:lmstats">(4.1)</a>, the fitted values and residuals satisfy
<span class="math display" id="eq:hat-y-dist">\[\begin{equation}
  \hat Y \sim \mathcal{N}\bigl( X\beta, \sigma^2 H \bigr)  \tag{4.3}
\end{equation}\]</span>
and
<span class="math display" id="eq:hat-eps-dist">\[\begin{equation}
  \hat\varepsilon\sim \mathcal{N}\bigl( 0, \sigma^2 (I - H) \bigr).  \tag{4.4}
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>Since <span class="math inline">\(Y \sim \mathcal{N}(X\beta, \sigma^2 I)\)</span>, we have <span class="math inline">\(\hat Y = HY \sim \mathcal{N}(HX\beta, H\sigma^2 I H^\top)\)</span>
by lemma <a href="S03-cov.html#lem:Cov-is-quadratic">3.1</a>. Using properties 3 and 5 of
lemma <a href="S04-model.html#lem:hat-matrix-props">4.2</a>, we find <span class="math inline">\(\hat Y \sim \mathcal{N}(X\beta, \sigma^2 H)\)</span>.
Similarly, <span class="math inline">\(\hat\varepsilon= (I - H)Y \sim \mathcal{N}\bigl((I - H)X\beta, \sigma^2(I - H)\bigr)\)</span>,
and using lemma <a href="S04-model.html#lem:hat-matrix-props">4.2</a> again, we obtain
<span class="math inline">\(\hat\varepsilon\sim \mathcal{N}\bigl(0, \sigma^2(I - H)\bigr)\)</span>.</p>
</div>
<p>The distributions of <span class="math inline">\(\hat Y\)</span> and <span class="math inline">\(\hat\varepsilon\)</span> reveal a deeper geometric
structure, described in the next lemma.</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-17" class="lemma"><strong>Lemma 4.4  </strong></span>For every vector <span class="math inline">\(v \in \mathbb{R}^n\)</span> we have
<span class="math display" id="eq:eps-y-orth">\[\begin{equation}
  \|v\|^2
  = \|Hv\|^2 + \|(I-H)v\|^2.  \tag{4.5}
\end{equation}\]</span>
In particular, for the data vector <span class="math inline">\(y\)</span> we have <span class="math inline">\(\|y\|^2 = \|\hat y\|^2 + \|\hat\varepsilon\|^2\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-18" class="proof"><em>Proof</em>. </span>We can write <span class="math inline">\(v\)</span> as
<span class="math display">\[\begin{align*}
  v
  &amp;= (H + I - H)v \\
  &amp;= Hv + (I-H)v.
\end{align*}\]</span>
The inner product between these two components is
<span class="math display">\[\begin{align*}
  (Hv)^\top (I-H)v
  &amp;= v^\top H^\top (I-H) v \\
  &amp;= v^\top H (I-H) v \\
  &amp;= v^\top (H-H^2) v \\
  &amp;= v^\top (H-H) v \\
  &amp;= 0,
\end{align*}\]</span>
so the two vectors are orthogonal.
As a result we get
<span class="math display">\[\begin{align*}
  \|v\|^2
  &amp;= v^\top v \\
  &amp;= \bigl( Hv + (I-H)v \bigr)^\top \bigl( Hv + (I-H)v \bigr) \\
  &amp;= (Hv)^\top Hv + 2 (Hv)^\top (I-H)v + \bigl((I-H)v\bigr)^\top (I-H)v \\
  &amp;= \| Hv \|^2 + \|(I-H)v \|^2
\end{align*}\]</span>
for any vector <span class="math inline">\(v \in \mathbb{R}^n\)</span>.
(This is <a href="https://en.wikipedia.org/wiki/Pythagorean_theorem">Pythagoras’ theorem</a> in <span class="math inline">\(\mathbb{R}^n\)</span>.)
Since <span class="math inline">\(\hat y = Hy\)</span> and <span class="math inline">\(\hat\varepsilon= (I - H)y\)</span>, we can apply this idea to the
vector <span class="math inline">\(v = y\)</span> to get <span class="math inline">\(\|y\|^2 = \|\hat y\|^2 + \|\hat\varepsilon\|^2\)</span>.
This completes the proof.</p>
</div>
<p>Some authors define:</p>
<ul>
<li><span class="math inline">\(\mathrm{SS}_\mathrm{T} = \sum_{i=1}^n y_i^2\)</span> (where “T” stands for “total”)</li>
<li><span class="math inline">\(\mathrm{SS}_\mathrm{R} = \sum_{i=1}^n \hat y_i^2\)</span> (where “R” stands for “regression”)</li>
<li><span class="math inline">\(\mathrm{SS}_\mathrm{E} = \sum_{i=1}^n \hat\varepsilon_i^2 = \sum_{i=1}^n (y_i-\hat y_i)^2\)</span> (where “E” stands for “error”)</li>
</ul>
<p>Using this notation, our equation
<span class="math inline">\(\|y\|^2 = \|\hat y\|^2 + \|\hat\varepsilon\|^2\)</span>
turns into
<span class="math display">\[\begin{equation*}
  \mathrm{SS}_\mathrm{T}
  = \mathrm{SS}_\mathrm{R} + \mathrm{SS}_\mathrm{E}.
\end{equation*}\]</span></p>
<p>A final algebraic property of the hat matrix concerns its trace.</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-19" class="lemma"><strong>Lemma 4.5  </strong></span>We have
<span class="math display" id="eq:tr-H">\[\begin{equation}
  \mathop{\mathrm{tr}}(H) = p+1.  \tag{4.6}
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-20" class="proof"><em>Proof</em>. </span>Using the property of the trace that <span class="math inline">\(\mathop{\mathrm{tr}}(ABC) = \mathop{\mathrm{tr}}(BCA)\)</span> (see section <a href="Sx1-matrices.html#trace">A.2.5</a>),
we find
<span class="math display">\[\begin{align*}
  \mathop{\mathrm{tr}}(H)
  &amp;= \mathop{\mathrm{tr}}\bigl( X (X^\top X)^{-1} X^\top \bigr) \\
  &amp;= \mathop{\mathrm{tr}}\bigl( (X^\top X)^{-1} X^\top X \bigr) \\
  &amp;= \mathop{\mathrm{tr}}(I) \\
  &amp;= p+1.
\end{align*}\]</span>
This completes the proof.</p>
</div>
</div>
<div id="Cochran" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Cochran’s theorem<a href="S04-model.html#Cochran" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/ZFru_oHHq98" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>To prove that our variance estimator is unbiased, we need to understand the
distribution of certain expressions involving the error vector. These
expressions are of the form <span class="math inline">\(\varepsilon^\top A \varepsilon\)</span> for suitable matrices <span class="math inline">\(A\)</span>.
Such expressions are called <strong>quadratic forms</strong>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-21" class="definition"><strong>Definition 4.1  </strong></span>For <span class="math inline">\(x\in\mathbb{R}^n\)</span> and <span class="math inline">\(A\in\mathbb{R}^{n\times n}\)</span>, the expression
<span class="math inline">\(x^\top A x\)</span> is called a <strong>quadratic form</strong>.</p>
</div>
<p>We use a simplified version of <a href="https://en.wikipedia.org/wiki/Cochran%27s_theorem">Cochran’s theorem</a>
as our main tool in this and the following section:</p>
<div class="theorem">
<p><span id="thm:Cochran" class="theorem"><strong>Theorem 4.1  </strong></span>The following statements are true:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\frac{1}{\sigma^2} \varepsilon^\top H \varepsilon\sim \chi^2(p+1)\)</span></p></li>
<li><p><span class="math inline">\(\frac{1}{\sigma^2} \varepsilon^\top (I - H) \varepsilon\sim \chi^2(n - p - 1)\)</span></p></li>
<li><p><span class="math inline">\(H \varepsilon\)</span> and <span class="math inline">\((I-H)\varepsilon\)</span> are independent.</p></li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-22" class="proof"><em>Proof</em>. </span>We transform the error vector <span class="math inline">\(\varepsilon\)</span> to a coordinate system where <span class="math inline">\(H\)</span>
becomes diagonal. In this new coordinate system, the quadratic forms
<span class="math inline">\(\varepsilon^\top H \varepsilon\)</span> and <span class="math inline">\(\varepsilon^\top (I-H) \varepsilon\)</span> become sums of squares of
independent standard normals, making their distributions easy to determine.</p>
<p>We start by diagonalising <span class="math inline">\(H\)</span>.
Since <span class="math inline">\(H\)</span> is symmetric, we can diagonalise <span class="math inline">\(H\)</span> (see <a href="Sx1-matrices.html#thm:spectral">A.2</a>
in the appendix): there is an orthogonal matrix <span class="math inline">\(U\)</span> such that
<span class="math inline">\(D := U H U^\top\)</span> is diagonal, and the diagonal elements of <span class="math inline">\(D\)</span> are
the eigenvalues of <span class="math inline">\(H\)</span>.</p>
<p>Next we determine the eigenvalues of <span class="math inline">\(H\)</span>.
Since <span class="math inline">\(H\)</span> is idempotent, we have <span class="math inline">\(H^2 = H\)</span>. This implies that every
eigenvalue <span class="math inline">\(\lambda\)</span> of <span class="math inline">\(H\)</span> satisfies <span class="math inline">\(\lambda^2 = \lambda\)</span>, and thus
<span class="math inline">\(\lambda \in \{0, 1\}\)</span>. Therefore, the diagonal elements of <span class="math inline">\(D\)</span>
can only be <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>.</p>
<p>Since <span class="math inline">\(U\)</span> is orthogonal, we have <span class="math inline">\(U^\top U = I\)</span> and thus
<span class="math display">\[\begin{equation*}
  U^\top D U
  = U^\top U H U^\top U
  = H.
\end{equation*}\]</span>
The same matrix <span class="math inline">\(U\)</span> also diagonalises <span class="math inline">\(I-H\)</span>, since
<span class="math display">\[\begin{equation*}
  U (I -H) U^\top = U U^\top - U H U^\top = I - D.
\end{equation*}\]</span>
Note that exactly one of the diagonal elements <span class="math inline">\(D_{ii}\)</span> and <span class="math inline">\((I - D)_{ii}\)</span>
is <span class="math inline">\(1\)</span> and the other is <span class="math inline">\(0\)</span> for every <span class="math inline">\(i\)</span>. This partitions the index set
<span class="math inline">\(\{1, \ldots, n\}\)</span> into two disjoint subsets: the set <span class="math inline">\(\mathcal{I}_1 = \{ i : D_{ii} = 1 \}\)</span>
contains exactly <span class="math inline">\(p+1\)</span> elements (corresponding to <span class="math inline">\(\mathop{\mathrm{rank}}(H) = p+1\)</span>), and the
set <span class="math inline">\(\mathcal{I}_0 = \{ i : D_{ii} = 0 \}\)</span> contains the remaining <span class="math inline">\(n - p - 1\)</span>
elements. This partition is the key to understanding both the degrees of
freedom in the <span class="math inline">\(\chi^2\)</span> distributions and the independence of the two quadratic forms.</p>
<p>Since <span class="math inline">\(\varepsilon\sim \mathcal{N}(0, \sigma^2 I)\)</span> we find that <span class="math inline">\(\eta := U \varepsilon\)</span> is normally
distributed with mean <span class="math inline">\(U 0 = 0\)</span> and covariance matrix <span class="math inline">\(\sigma^2 U I U^\top =
\sigma^2 U U^\top = \sigma^2 I\)</span>. Thus <span class="math inline">\(\eta\)</span> has the same distribution as
<span class="math inline">\(\varepsilon\)</span> does: <span class="math inline">\(\eta \sim \mathcal{N}(0, \sigma^2I)\)</span> and the components <span class="math inline">\(\eta_i\)</span>
are independent of each other. We have
<span class="math display">\[\begin{equation*}
  H \varepsilon
  = U^\top D U \varepsilon
  = U^\top D \eta.
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
  (I - H) \varepsilon
  = U^\top (I - D) U \varepsilon
  = U^\top (I - D) \eta.
\end{equation*}\]</span>
To prove the independence statement, we examine how the components of <span class="math inline">\(\eta\)</span>
contribute to <span class="math inline">\(D\eta\)</span> and <span class="math inline">\((I-D)\eta\)</span>. Since <span class="math inline">\(D\)</span> is diagonal, we have
<span class="math display">\[\begin{equation*}
  (D\eta)_i
  = D_{ii} \eta_i
  = \begin{cases}
      \eta_i &amp; \text{if } i \in \mathcal{I}_1 \\
      0      &amp; \text{if } i \in \mathcal{I}_0.
    \end{cases}
\end{equation*}\]</span>
Similarly,
<span class="math display">\[\begin{equation*}
  \bigl((I - D)\eta\bigr)_i
  = (1 - D_{ii}) \eta_i
  = \begin{cases}
      0      &amp; \text{if } i \in \mathcal{I}_1 \\
      \eta_i &amp; \text{if } i \in \mathcal{I}_0.
    \end{cases}
\end{equation*}\]</span>
This shows that each component <span class="math inline">\(\eta_i\)</span> contributes to exactly one of the
two vectors <span class="math inline">\(D\eta\)</span> and <span class="math inline">\((I-D)\eta\)</span>: it contributes to <span class="math inline">\(D\eta\)</span> if <span class="math inline">\(i \in \mathcal{I}_1\)</span>,
and to <span class="math inline">\((I-D)\eta\)</span> if <span class="math inline">\(i \in \mathcal{I}_0\)</span>.
Since the sets <span class="math inline">\(\mathcal{I}_1\)</span> and <span class="math inline">\(\mathcal{I}_0\)</span> are disjoint, the vectors
<span class="math inline">\(D\eta\)</span> and <span class="math inline">\((I-D)\eta\)</span> depend on disjoint subsets of the independent random
variables <span class="math inline">\(\eta_1, \ldots, \eta_n\)</span>. Therefore, <span class="math inline">\(D\eta\)</span> and <span class="math inline">\((I-D)\eta\)</span>
are independent.
Finally, since <span class="math inline">\(H\varepsilon= U^\top D\eta\)</span> and <span class="math inline">\((I-H)\varepsilon= U^\top(I-D)\eta\)</span>
are deterministic linear transformations of <span class="math inline">\(D\eta\)</span> and <span class="math inline">\((I-D)\eta\)</span>, it
follows that <span class="math inline">\(H\varepsilon\)</span> and <span class="math inline">\((I - H)\varepsilon\)</span> are also independent.
This proves the third statement of the theorem.</p>
<p>For the first statement, we note that
<span class="math display">\[\begin{align*}
  \varepsilon^\top H \varepsilon
  &amp;= \varepsilon^\top U^\top D U \varepsilon\\
  &amp;= \eta^\top D \eta \\
  &amp;= \sum_{i=1 \atop D_{ii}=1}^n \eta_i^2.
\end{align*}\]</span>
Since <span class="math inline">\((X^\top X) \in\mathbb{R}^{(p+1)\times (p+1)}\)</span> is invertible, one can show
that <span class="math inline">\(\mathop{\mathrm{rank}}(H) = p+1\)</span> and thus that there are <span class="math inline">\(p+1\)</span> terms contributing
to the sum (we skip the proof of this statement here). Thus,
<span class="math display">\[\begin{equation*}
  \frac{1}{\sigma^2} \varepsilon^\top H \varepsilon
  = \sum_{i=1 \atop D_{ii}=1}^n \bigl(\eta_i/\sigma)^2
\end{equation*}\]</span>
is the sum of the squares of <span class="math inline">\(p+1\)</span> independent standard normals,
and thus is <span class="math inline">\(\chi^2(p+1)\)</span>-distributed. This completes the proof of the
first statement.</p>
<p>Finally, the second statement follows in much of the same way as the
first one, except that <span class="math inline">\(H\)</span> is replaced with <span class="math inline">\(I-H\)</span> and the sum is
over the <span class="math inline">\(n - p - 1\)</span> indices <span class="math inline">\(i\)</span> where <span class="math inline">\(D_{ii} = 0\)</span>.
This completes the proof.</p>
</div>
<p>Since <span class="math inline">\(\varepsilon^\top H \varepsilon= (H\varepsilon)^\top (H\varepsilon)\)</span> is a function of <span class="math inline">\(H\varepsilon\)</span>,
and similarly for <span class="math inline">\(I-H\)</span>, the independence in part 3 of the theorem extends to
the quadratic forms in parts 1 and 2.</p>
</div>
<div id="var-est-bias" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Estimating the Error Variance<a href="S04-model.html#var-est-bias" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/TgN7Ah4ycf4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>Recall from equation <a href="S04-model.html#eq:beta-hat-j">(4.2)</a> that the variance of each
coefficient estimate is <span class="math inline">\(\sigma^2 C_{jj}\)</span>. To use this result in practice—for
constructing confidence intervals or performing hypothesis tests—we need
to estimate the unknown parameter <span class="math inline">\(\sigma^2\)</span>. A natural approach is to examine
the residuals <span class="math inline">\(y_i - \hat y_i\)</span>, which estimate the unobserved errors <span class="math inline">\(\varepsilon_i\)</span>.</p>
<p>Having established the necessary properties of the hat matrix and Cochran’s
theorem, we can now show that an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>
is given by
<span class="math display" id="eq:hat-sigma-squared">\[\begin{equation}
  \hat\sigma^2
  := \frac{1}{n-p-1} \sum_{i=1}^n (y_i - \hat y_i)^2
  = \frac{1}{n-p-1} \sum_{i=1}^n \hat\varepsilon_i^2, \tag{4.7}.
\end{equation}\]</span>
As for the case of simple linear regression, in <a href="S01-simple.html#eq:reg-sigma-est">(1.4)</a>, the
estimate does not have the prefactor <span class="math inline">\(1/n\)</span>, which one might expect, but
the denominator is decreased by one for each component of the vector <span class="math inline">\(\beta\)</span>.
Using Cochran’s theorem, we can now show that the estimator <span class="math inline">\(\hat\sigma^2\)</span> is
unbiased.</p>
<div class="lemma">
<p><span id="lem:sigma-hat-unbiased" class="lemma"><strong>Lemma 4.6  </strong></span>We have
<span class="math display">\[\begin{equation*}
  \mathbb{E}(\hat\sigma^2) = \sigma^2.
\end{equation*}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-23" class="proof"><em>Proof</em>. </span>We first note that for data from the model we have
<span class="math display">\[\begin{equation*}
  \hat\varepsilon
  = (I-H) y
  = (I-H) (X\beta + \varepsilon)
  = (I-H) \varepsilon,
\end{equation*}\]</span>
where we used property 6 of lemma <a href="S04-model.html#lem:hat-matrix-props">4.2</a> to see that
<span class="math inline">\((I-H)X\beta = 0\)</span>. Using this relation, we get
<span class="math display">\[\begin{align*}
  (n - p - 1) \hat\sigma^2
  &amp;= \hat\varepsilon^\top \hat\varepsilon\\
  &amp;= \varepsilon^\top (I - H)^\top (I - H) \varepsilon\\
  &amp;= \varepsilon^\top (I - H) \varepsilon.
\end{align*}\]</span></p>
<p>Now we can apply Cochran’s theorem. This shows that
<span class="math display" id="eq:sigma-hat-chi-squared">\[\begin{equation}
  \frac{1}{\sigma^2} (n - p - 1) \hat\sigma^2
  = \frac{1}{\sigma^2} \varepsilon^\top (I-H) \varepsilon
  \sim \chi^2(n - p - 1).  \tag{4.8}
\end{equation}\]</span>
Since the expectation of a <span class="math inline">\(\chi^2(\nu)\)</span> distribution equals <span class="math inline">\(\nu\)</span>
(see appendix <a href="Sx2-probability.html#chi-square">B.5.1</a>), we find
<span class="math display">\[\begin{equation*}
  \frac{1}{\sigma^2} (n - p - 1) \mathbb{E}(\hat\sigma^2)
  = n - p - 1
\end{equation*}\]</span>
and thus
<span class="math display">\[\begin{equation*}
  \mathbb{E}(\hat\sigma^2)
  = \sigma^2.
\end{equation*}\]</span>
This completes the proof.</p>
</div>
<p>The lemma shows that <span class="math inline">\(\hat\sigma^2\)</span> is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>.</p>
<p>This concludes our study of the statistical properties of the least squares
estimator. In the next section we will use these properties to construct
confidence intervals and hypothesis tests for the regression coefficients.</p>
<div class="mysummary">
<p><strong>Summary</strong></p>
<ul>
<li>The least squares estimator for the regression coefficients is unbiased.</li>
<li>The hat matrix is idempotent and symmetric.</li>
<li>Cochran’s theorem allows us to understand the distribution of some
quadratic forms involving the hat matrix.</li>
<li><span class="math inline">\(\hat\sigma^2\)</span> is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="P01.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="S05-single.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": null,
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["MATH3714.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
