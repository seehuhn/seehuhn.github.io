<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 4 Properties of the Least Squares Estimate | MATH3714 Linear Regression and Robustness</title>
  <meta name="description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2022/23" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 4 Properties of the Least Squares Estimate | MATH3714 Linear Regression and Robustness" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2022/23" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 4 Properties of the Least Squares Estimate | MATH3714 Linear Regression and Robustness" />
  
  <meta name="twitter:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2022/23" />
  

<meta name="author" content="Jochen Voss" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="S03-cov.html"/>
<link rel="next" href="S05-single.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P96L0SF56N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-P96L0SF56N');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="jvstyle.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">MATH3714 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html"><i class="fa fa-check"></i>About MATH3714</a>
<ul>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#notes"><i class="fa fa-check"></i>Notes and videos</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#lectures"><i class="fa fa-check"></i>Lectures</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#workshops"><i class="fa fa-check"></i>Workshops and Problem Sheets</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#assessments"><i class="fa fa-check"></i>Assessments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="S01-simple.html"><a href="S01-simple.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="S01-simple.html"><a href="S01-simple.html#residual-sum-of-squares"><i class="fa fa-check"></i><b>1.1</b> Residual Sum of Squares</a></li>
<li class="chapter" data-level="1.2" data-path="S01-simple.html"><a href="S01-simple.html#linear-regression-as-a-parameter-estimation-problem"><i class="fa fa-check"></i><b>1.2</b> Linear Regression as a Parameter Estimation Problem</a></li>
<li class="chapter" data-level="1.3" data-path="S01-simple.html"><a href="S01-simple.html#sec:simple-mat"><i class="fa fa-check"></i><b>1.3</b> Matrix Notation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="S02-multiple.html"><a href="S02-multiple.html"><i class="fa fa-check"></i><b>2</b> Least Squares Estimates</a>
<ul>
<li class="chapter" data-level="2.1" data-path="S02-multiple.html"><a href="S02-multiple.html#data-and-models"><i class="fa fa-check"></i><b>2.1</b> Data and Models</a></li>
<li class="chapter" data-level="2.2" data-path="S02-multiple.html"><a href="S02-multiple.html#the-normal-equations"><i class="fa fa-check"></i><b>2.2</b> The Normal Equations</a></li>
<li class="chapter" data-level="2.3" data-path="S02-multiple.html"><a href="S02-multiple.html#fitted-values"><i class="fa fa-check"></i><b>2.3</b> Fitted Values</a></li>
<li class="chapter" data-level="2.4" data-path="S02-multiple.html"><a href="S02-multiple.html#example"><i class="fa fa-check"></i><b>2.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html"><i class="fa fa-check"></i>Interlude: Linear Regression in R</a>
<ul>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-fitting"><i class="fa fa-check"></i>Fitting a Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-model"><i class="fa fa-check"></i>Understanding the Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-predict"><i class="fa fa-check"></i>Making Predictions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P01.html"><a href="P01.html"><i class="fa fa-check"></i>Problem Sheet 1</a></li>
<li class="chapter" data-level="3" data-path="S03-cov.html"><a href="S03-cov.html"><i class="fa fa-check"></i><b>3</b> Random Vectors and Covariance</a>
<ul>
<li class="chapter" data-level="3.1" data-path="S03-cov.html"><a href="S03-cov.html#expectation"><i class="fa fa-check"></i><b>3.1</b> Expectation</a></li>
<li class="chapter" data-level="3.2" data-path="S03-cov.html"><a href="S03-cov.html#sec:covariance"><i class="fa fa-check"></i><b>3.2</b> Covariance Matrix</a></li>
<li class="chapter" data-level="3.3" data-path="S03-cov.html"><a href="S03-cov.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>3.3</b> The Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="S04-model.html"><a href="S04-model.html"><i class="fa fa-check"></i><b>4</b> Properties of the Least Squares Estimate</a>
<ul>
<li class="chapter" data-level="4.1" data-path="S04-model.html"><a href="S04-model.html#mean-and-covariance"><i class="fa fa-check"></i><b>4.1</b> Mean and Covariance</a></li>
<li class="chapter" data-level="4.2" data-path="S04-model.html"><a href="S04-model.html#hat-matrix"><i class="fa fa-check"></i><b>4.2</b> Properties of the Hat Matrix</a></li>
<li class="chapter" data-level="4.3" data-path="S04-model.html"><a href="S04-model.html#Cochran"><i class="fa fa-check"></i><b>4.3</b> Cochran’s theorem</a></li>
<li class="chapter" data-level="4.4" data-path="S04-model.html"><a href="S04-model.html#var-est-bias"><i class="fa fa-check"></i><b>4.4</b> Estimating the Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="S05-single.html"><a href="S05-single.html"><i class="fa fa-check"></i><b>5</b> Uncertainty for Individual Regression Coefficients</a>
<ul>
<li class="chapter" data-level="5.1" data-path="S05-single.html"><a href="S05-single.html#measuring-the-estimation-error"><i class="fa fa-check"></i><b>5.1</b> Measuring the Estimation Error</a></li>
<li class="chapter" data-level="5.2" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.3" data-path="S05-single.html"><a href="S05-single.html#hypthesis-tests"><i class="fa fa-check"></i><b>5.3</b> Hypthesis Tests</a></li>
<li class="chapter" data-level="5.4" data-path="S05-single.html"><a href="S05-single.html#r-experiments"><i class="fa fa-check"></i><b>5.4</b> R Experiments</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="S05-single.html"><a href="S05-single.html#fitting-the-model"><i class="fa fa-check"></i><b>5.4.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.4.2" data-path="S05-single.html"><a href="S05-single.html#estimating-the-variance-of-the-error"><i class="fa fa-check"></i><b>5.4.2</b> Estimating the Variance of the Error</a></li>
<li class="chapter" data-level="5.4.3" data-path="S05-single.html"><a href="S05-single.html#estimating-the-standard-errors"><i class="fa fa-check"></i><b>5.4.3</b> Estimating the Standard Errors</a></li>
<li class="chapter" data-level="5.4.4" data-path="S05-single.html"><a href="S05-single.html#hypothesis-tests"><i class="fa fa-check"></i><b>5.4.4</b> Hypothesis tests</a></li>
<li class="chapter" data-level="5.4.5" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals-1"><i class="fa fa-check"></i><b>5.4.5</b> Confidence Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html"><i class="fa fa-check"></i><b>6</b> Estimating Coefficients Simultaneously</a>
<ul>
<li class="chapter" data-level="6.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-dist"><i class="fa fa-check"></i><b>6.1</b> Linear Combinations of Coefficients</a></li>
<li class="chapter" data-level="6.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-CI"><i class="fa fa-check"></i><b>6.2</b> Confidence Regions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#result"><i class="fa fa-check"></i><b>6.2.1</b> Result</a></li>
<li class="chapter" data-level="6.2.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#numerical-experiments"><i class="fa fa-check"></i><b>6.2.2</b> Numerical Experiments</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-test"><i class="fa fa-check"></i><b>6.3</b> Hypothesis Tests</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html"><i class="fa fa-check"></i>Interlude: Loading Data into R</a>
<ul>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-csv-files"><i class="fa fa-check"></i>Importing CSV Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-microsoft-excel-files"><i class="fa fa-check"></i>Importing Microsoft Excel Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#checking-the-imported-data"><i class="fa fa-check"></i>Checking the Imported Data</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#common-problems"><i class="fa fa-check"></i>Common Problems</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P02.html"><a href="P02.html"><i class="fa fa-check"></i>Problem Sheet 2</a></li>
<li class="chapter" data-level="7" data-path="S07-examples.html"><a href="S07-examples.html"><i class="fa fa-check"></i><b>7</b> Examples</a>
<ul>
<li class="chapter" data-level="7.1" data-path="S07-examples.html"><a href="S07-examples.html#simple-confidence-interval"><i class="fa fa-check"></i><b>7.1</b> Simple Confidence Interval</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles"><i class="fa fa-check"></i><b>7.1.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.1.2" data-path="S07-examples.html"><a href="S07-examples.html#from-the-lm-output"><i class="fa fa-check"></i><b>7.1.2</b> From the <code>lm()</code> Output</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="S07-examples.html"><a href="S07-examples.html#confidence-intervals-for-the-mean"><i class="fa fa-check"></i><b>7.2</b> Confidence Intervals for the Mean</a></li>
<li class="chapter" data-level="7.3" data-path="S07-examples.html"><a href="S07-examples.html#testing-a-single-coefficient"><i class="fa fa-check"></i><b>7.3</b> Testing a Single Coefficient</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-1"><i class="fa fa-check"></i><b>7.3.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.3.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-i"><i class="fa fa-check"></i><b>7.3.2</b> Using the <code>lm()</code> Output, I</a></li>
<li class="chapter" data-level="7.3.3" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-ii"><i class="fa fa-check"></i><b>7.3.3</b> Using the <code>lm()</code> Output, II</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="S07-examples.html"><a href="S07-examples.html#testing-multiple-coefficents"><i class="fa fa-check"></i><b>7.4</b> Testing Multiple Coefficents</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-2"><i class="fa fa-check"></i><b>7.4.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.4.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output"><i class="fa fa-check"></i><b>7.4.2</b> Using the <code>lm()</code> output</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="S08-diagnostics.html"><a href="S08-diagnostics.html"><i class="fa fa-check"></i><b>8</b> Regression Diagnostics</a>
<ul>
<li class="chapter" data-level="8.1" data-path="S08-diagnostics.html"><a href="S08-diagnostics.html#plots"><i class="fa fa-check"></i><b>8.1</b> Diagnostic Plots</a></li>
<li class="chapter" data-level="8.2" data-path="S08-diagnostics.html"><a href="S08-diagnostics.html#R-squared"><i class="fa fa-check"></i><b>8.2</b> The Coefficient of Multiple Determination</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I03-lm-output.html"><a href="I03-lm-output.html"><i class="fa fa-check"></i>Interlude: Understanding the <code>lm()</code> Output</a></li>
<li class="chapter" data-level="9" data-path="S09-influence.html"><a href="S09-influence.html"><i class="fa fa-check"></i><b>9</b> The Influence of Observations</a>
<ul>
<li class="chapter" data-level="9.1" data-path="S09-influence.html"><a href="S09-influence.html#deleting"><i class="fa fa-check"></i><b>9.1</b> Deleting Observations</a></li>
<li class="chapter" data-level="9.2" data-path="S09-influence.html"><a href="S09-influence.html#influence"><i class="fa fa-check"></i><b>9.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="S10-multicoll.html"><a href="S10-multicoll.html"><i class="fa fa-check"></i><b>10</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="10.1" data-path="S10-multicoll.html"><a href="S10-multicoll.html#consequences-of-multicollinearity"><i class="fa fa-check"></i><b>10.1</b> Consequences of Multicollinearity</a></li>
<li class="chapter" data-level="10.2" data-path="S10-multicoll.html"><a href="S10-multicoll.html#detecting-multicollinearity"><i class="fa fa-check"></i><b>10.2</b> Detecting Multicollinearity</a></li>
<li class="chapter" data-level="10.3" data-path="S10-multicoll.html"><a href="S10-multicoll.html#mitigations"><i class="fa fa-check"></i><b>10.3</b> Mitigations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P03.html"><a href="P03.html"><i class="fa fa-check"></i>Problem Sheet 3</a></li>
<li class="chapter" data-level="11" data-path="S11-improving.html"><a href="S11-improving.html"><i class="fa fa-check"></i><b>11</b> Improving the Model Fit</a>
<ul>
<li class="chapter" data-level="11.1" data-path="S11-improving.html"><a href="S11-improving.html#linearising-the-mean"><i class="fa fa-check"></i><b>11.1</b> Linearising the Mean</a></li>
<li class="chapter" data-level="11.2" data-path="S11-improving.html"><a href="S11-improving.html#stabilising-the-variance"><i class="fa fa-check"></i><b>11.2</b> Stabilising the Variance</a></li>
<li class="chapter" data-level="11.3" data-path="S11-improving.html"><a href="S11-improving.html#power-transform"><i class="fa fa-check"></i><b>11.3</b> The Power Transform</a></li>
<li class="chapter" data-level="11.4" data-path="S11-improving.html"><a href="S11-improving.html#orthogonal-inputs"><i class="fa fa-check"></i><b>11.4</b> Orthogonal Inputs</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="S12-ridge.html"><a href="S12-ridge.html"><i class="fa fa-check"></i><b>12</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="S12-ridge.html"><a href="S12-ridge.html#definition-the-estimator"><i class="fa fa-check"></i><b>12.1</b> Definition the Estimator</a></li>
<li class="chapter" data-level="12.2" data-path="S12-ridge.html"><a href="S12-ridge.html#properties-of-the-estimate"><i class="fa fa-check"></i><b>12.2</b> Properties of the Estimate</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="S12-ridge.html"><a href="S12-ridge.html#bias"><i class="fa fa-check"></i><b>12.2.1</b> Bias</a></li>
<li class="chapter" data-level="12.2.2" data-path="S12-ridge.html"><a href="S12-ridge.html#variance"><i class="fa fa-check"></i><b>12.2.2</b> Variance</a></li>
<li class="chapter" data-level="12.2.3" data-path="S12-ridge.html"><a href="S12-ridge.html#mean-squared-error"><i class="fa fa-check"></i><b>12.2.3</b> Mean Squared Error</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="S12-ridge.html"><a href="S12-ridge.html#standardisation"><i class="fa fa-check"></i><b>12.3</b> Standardisation</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="S13-models.html"><a href="S13-models.html"><i class="fa fa-check"></i><b>13</b> Model selection</a>
<ul>
<li class="chapter" data-level="13.1" data-path="S13-models.html"><a href="S13-models.html#candidates-models"><i class="fa fa-check"></i><b>13.1</b> Candidates Models</a></li>
<li class="chapter" data-level="13.2" data-path="S13-models.html"><a href="S13-models.html#misspecified-models"><i class="fa fa-check"></i><b>13.2</b> Misspecified Models</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="S13-models.html"><a href="S13-models.html#missing-variables"><i class="fa fa-check"></i><b>13.2.1</b> Missing Variables</a></li>
<li class="chapter" data-level="13.2.2" data-path="S13-models.html"><a href="S13-models.html#unnecessary-variables"><i class="fa fa-check"></i><b>13.2.2</b> Unnecessary Variables</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="S13-models.html"><a href="S13-models.html#criteria"><i class="fa fa-check"></i><b>13.3</b> Assessing Models</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P04.html"><a href="P04.html"><i class="fa fa-check"></i>Problem Sheet 4</a></li>
<li class="chapter" data-level="14" data-path="S14-methods.html"><a href="S14-methods.html"><i class="fa fa-check"></i><b>14</b> Automatic Model Selection</a>
<ul>
<li class="chapter" data-level="14.1" data-path="S14-methods.html"><a href="S14-methods.html#exhaustive-search"><i class="fa fa-check"></i><b>14.1</b> Exhaustive Search</a></li>
<li class="chapter" data-level="14.2" data-path="S14-methods.html"><a href="S14-methods.html#search-algorithm"><i class="fa fa-check"></i><b>14.2</b> Search Algorithm</a></li>
<li class="chapter" data-level="14.3" data-path="S14-methods.html"><a href="S14-methods.html#other-methods"><i class="fa fa-check"></i><b>14.3</b> Other Methods</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="S14-methods.html"><a href="S14-methods.html#stepwise-forward-selection"><i class="fa fa-check"></i><b>14.3.1</b> Stepwise Forward Selection</a></li>
<li class="chapter" data-level="14.3.2" data-path="S14-methods.html"><a href="S14-methods.html#stepwise-backward-selection"><i class="fa fa-check"></i><b>14.3.2</b> Stepwise Backward Selection</a></li>
<li class="chapter" data-level="14.3.3" data-path="S14-methods.html"><a href="S14-methods.html#hybrid-methods"><i class="fa fa-check"></i><b>14.3.3</b> Hybrid Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="S15-factors.html"><a href="S15-factors.html"><i class="fa fa-check"></i><b>15</b> Factors</a>
<ul>
<li class="chapter" data-level="15.1" data-path="S15-factors.html"><a href="S15-factors.html#indicator-variables"><i class="fa fa-check"></i><b>15.1</b> Indicator Variables</a></li>
<li class="chapter" data-level="15.2" data-path="S15-factors.html"><a href="S15-factors.html#interactions"><i class="fa fa-check"></i><b>15.2</b> Interactions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html"><i class="fa fa-check"></i>Practical</a>
<ul>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#dataset"><i class="fa fa-check"></i>Dataset</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#tasks"><i class="fa fa-check"></i>Tasks</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="S16-examples.html"><a href="S16-examples.html"><i class="fa fa-check"></i><b>16</b> Examples</a>
<ul>
<li class="chapter" data-level="16.1" data-path="S16-examples.html"><a href="S16-examples.html#interactions-example"><i class="fa fa-check"></i><b>16.1</b> Use of Interaction Terms in Modelling</a></li>
<li class="chapter" data-level="16.2" data-path="S16-examples.html"><a href="S16-examples.html#codings"><i class="fa fa-check"></i><b>16.2</b> Alternative Factor Codings</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="S17-robust.html"><a href="S17-robust.html"><i class="fa fa-check"></i><b>17</b> Robust Regression</a>
<ul>
<li class="chapter" data-level="17.1" data-path="S17-robust.html"><a href="S17-robust.html#outliers"><i class="fa fa-check"></i><b>17.1</b> Outliers</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="S17-robust.html"><a href="S17-robust.html#leverage"><i class="fa fa-check"></i><b>17.1.1</b> Leverage</a></li>
<li class="chapter" data-level="17.1.2" data-path="S17-robust.html"><a href="S17-robust.html#studentised-residuals"><i class="fa fa-check"></i><b>17.1.2</b> Studentised Residuals</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="S17-robust.html"><a href="S17-robust.html#breakdown-points"><i class="fa fa-check"></i><b>17.2</b> Breakdown Points</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P05.html"><a href="P05.html"><i class="fa fa-check"></i>Problem Sheet 5</a></li>
<li class="chapter" data-level="18" data-path="S18-m-est.html"><a href="S18-m-est.html"><i class="fa fa-check"></i><b>18</b> M-Estimators</a>
<ul>
<li class="chapter" data-level="18.1" data-path="S18-m-est.html"><a href="S18-m-est.html#definition"><i class="fa fa-check"></i><b>18.1</b> Definition</a></li>
<li class="chapter" data-level="18.2" data-path="S18-m-est.html"><a href="S18-m-est.html#iterative-methods"><i class="fa fa-check"></i><b>18.2</b> Iterative Methods</a></li>
<li class="chapter" data-level="18.3" data-path="S18-m-est.html"><a href="S18-m-est.html#objective-functions"><i class="fa fa-check"></i><b>18.3</b> Objective Functions</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="S18-m-est.html"><a href="S18-m-est.html#least-squares-method"><i class="fa fa-check"></i><b>18.3.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="18.3.2" data-path="S18-m-est.html"><a href="S18-m-est.html#least-absolute-values"><i class="fa fa-check"></i><b>18.3.2</b> Least Absolute Values</a></li>
<li class="chapter" data-level="18.3.3" data-path="S18-m-est.html"><a href="S18-m-est.html#Huber"><i class="fa fa-check"></i><b>18.3.3</b> Huber’s <span class="math inline">\(t\)</span>-function</a></li>
<li class="chapter" data-level="18.3.4" data-path="S18-m-est.html"><a href="S18-m-est.html#hampels-method"><i class="fa fa-check"></i><b>18.3.4</b> Hampel’s Method</a></li>
<li class="chapter" data-level="18.3.5" data-path="S18-m-est.html"><a href="S18-m-est.html#tukeys-bisquare-method"><i class="fa fa-check"></i><b>18.3.5</b> Tukey’s Bisquare Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="S19-efficiency.html"><a href="S19-efficiency.html"><i class="fa fa-check"></i><b>19</b> Efficiency of Robust Estimators</a>
<ul>
<li class="chapter" data-level="19.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#efficiency"><i class="fa fa-check"></i><b>19.1</b> Efficiency</a></li>
<li class="chapter" data-level="19.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#robust-estimators"><i class="fa fa-check"></i><b>19.2</b> Robust estimators</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#least-median-of-squares"><i class="fa fa-check"></i><b>19.2.1</b> Least Median of Squares</a></li>
<li class="chapter" data-level="19.2.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#least-trimmed-squares"><i class="fa fa-check"></i><b>19.2.2</b> Least Trimmed Squares</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html"><i class="fa fa-check"></i><b>A</b> Linear Algebra Reminders</a>
<ul>
<li class="chapter" data-level="A.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#vectors"><i class="fa fa-check"></i><b>A.1</b> Vectors</a></li>
<li class="chapter" data-level="A.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-rules"><i class="fa fa-check"></i><b>A.2</b> Matrices</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#transpose"><i class="fa fa-check"></i><b>A.2.1</b> Transpose</a></li>
<li class="chapter" data-level="A.2.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-vector-product"><i class="fa fa-check"></i><b>A.2.2</b> Matrix-vector Product</a></li>
<li class="chapter" data-level="A.2.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-matrix-product"><i class="fa fa-check"></i><b>A.2.3</b> Matrix-matrix Product</a></li>
<li class="chapter" data-level="A.2.4" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#rank"><i class="fa fa-check"></i><b>A.2.4</b> Rank</a></li>
<li class="chapter" data-level="A.2.5" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#trace"><i class="fa fa-check"></i><b>A.2.5</b> Trace</a></li>
<li class="chapter" data-level="A.2.6" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.2.6</b> Matrix Inverse</a></li>
<li class="chapter" data-level="A.2.7" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.2.7</b> Orthogonal Matrices</a></li>
<li class="chapter" data-level="A.2.8" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#positive-definite"><i class="fa fa-check"></i><b>A.2.8</b> Positive Definite Matrices</a></li>
<li class="chapter" data-level="A.2.9" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#idempotent"><i class="fa fa-check"></i><b>A.2.9</b> Idempotent Matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#eigenvalues"><i class="fa fa-check"></i><b>A.3</b> Eigenvalues</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="Sx2-probability.html"><a href="Sx2-probability.html"><i class="fa fa-check"></i><b>B</b> Probability Reminders</a>
<ul>
<li class="chapter" data-level="B.1" data-path="Sx2-probability.html"><a href="Sx2-probability.html#independence"><i class="fa fa-check"></i><b>B.1</b> Independence</a></li>
<li class="chapter" data-level="B.2" data-path="Sx2-probability.html"><a href="Sx2-probability.html#chi-square"><i class="fa fa-check"></i><b>B.2</b> The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="B.3" data-path="Sx2-probability.html"><a href="Sx2-probability.html#t"><i class="fa fa-check"></i><b>B.3</b> The t-distribution</a></li>
</ul></li>
<li class="divider"></li>
<li class="chapter"><span><b>THE END</b></span></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3714 Linear Regression and Robustness</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="S04-model" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Section 4</span> Properties of the Least Squares Estimate<a href="S04-model.html#S04-model" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!--
TODO:
- explain \sum h_{ii} = p here, for use in S13 and S17
- explain h_{ii} \in [1/n, 1] for S17.  Maybe even (1/n, 1)?
- explicitly state the distributions of $\hat\eps$ and $\hat y$
  (e.g. for use in S17)
- Maybe show var(\hat\beta_j) ~ 1/sqrt(n)
- identify and cite the source which introducted $\mathrm{SS}_\mathrm{T}$ etc.
-->
<p>Like in the one-dimensional case, we can build a <strong>statistical model</strong>
for the data. Here we assume that the residuals are random. More
precisely we have
<span class="math display" id="eq:lmstats">\[\begin{equation}
  Y = X \beta + \varepsilon.  \tag{4.1}
\end{equation}\]</span>
where <span class="math inline">\(\varepsilon= (\varepsilon_1, \ldots, \varepsilon_n)\)</span> is a random error.
The individual errors <span class="math inline">\(\varepsilon_1, \ldots, \varepsilon_n\)</span>
are now assumed to be i.i.d. random variables with <span class="math inline">\(\mathbb{E}(\varepsilon_i) = 0\)</span> and
<span class="math inline">\(\mathop{\mathrm{Var}}(\varepsilon_i) = \sigma^2\)</span>.</p>
<ul>
<li><p>Again, we assume that the <span class="math inline">\(x\)</span>-values are fixed and known. The only
random quantities in the model are <span class="math inline">\(\varepsilon_i\)</span> and <span class="math inline">\(Y_i\)</span>.</p></li>
<li><p>The parameters in this model are now the regression coefficients
<span class="math inline">\(\beta = (\beta_0, \beta_1, \cdots, \beta_p) \in \mathbb{R}^{p+1}\)</span> and
the error variance <span class="math inline">\(\sigma^2\)</span>.</p></li>
</ul>
<p>The usual approach in statistics to quantify how well an estimator works
is to apply it to random samples from the statistical model, where we
can assume that we know the parameters, and then to study how well the
parameters are reconstructed by the estimators. Since this approach
uses random samples as input the the estimator, we obtain random estimates
and we need to use statistical methods to quantify how close the estimate
is to the truth.</p>
<div id="mean-and-covariance" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Mean and Covariance<a href="S04-model.html#mean-and-covariance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/s52WJNkRM00" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>The bias of an estimator is the difference between the expected value
of the estimate and the truth. For the least squares estimator
we have
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{bias}}(\hat\beta)
  = \mathbb{E}(\hat\beta) - \beta,
\end{equation*}\]</span>
where
<span class="math display">\[\begin{equation*}
  \hat\beta
  = (X^\top X)^{-1} X^\top Y
\end{equation*}\]</span>
and <span class="math inline">\(Y\)</span> is the random vector from <a href="S04-model.html#eq:lmstats">(4.1)</a>.</p>
<div class="lemma">
<p><span id="lem:hat-beta-dist" class="lemma"><strong>Lemma 4.1  </strong></span>We have</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\hat\beta = \beta + (X^\top X)^{-1} X^\top \varepsilon\)</span> and</p></li>
<li><p><span class="math inline">\(\hat\beta \sim \mathcal{N}\bigl( \beta, \sigma^2 (X^\top X)^{-1} \bigr)\)</span>.</p></li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-13" class="proof"><em>Proof</em>. </span>From lemma <a href="S02-multiple.html#lem:multiple-LSQ">2.1</a> we know
<span class="math display">\[\begin{equation*}
  \hat\beta
  = (X^\top X)^{-1} X^\top Y,
\end{equation*}\]</span>
Using the definition of <span class="math inline">\(Y\)</span> we can write this as
<span class="math display">\[\begin{align*}
  \hat\beta
  &amp;= (X^\top X)^{-1} X^\top Y \\
  &amp;= (X^\top X)^{-1} X^\top (X\beta + \varepsilon) \\
  &amp;= (X^\top X)^{-1} X^\top X \beta + (X^\top X)^{-1} X^\top \varepsilon\\
  &amp;= \beta + (X^\top X)^{-1} X^\top \varepsilon.
\end{align*}\]</span>
This proves the first claim.</p>
<p>Since <span class="math inline">\(\varepsilon\)</span> follows a multi-variate normal distribution,
<span class="math inline">\(\beta + (X^\top X)^{-1} X^\top \varepsilon\)</span> is also normally distributed.
Taking expectations we get
<span class="math display">\[\begin{align*}
  \mathbb{E}(\hat\beta)
  &amp;= \mathbb{E}\bigl( \beta + (X^\top X)^{-1} X^\top \varepsilon\bigr) \\
  &amp;= \beta + (X^\top X)^{-1} X^\top \mathbb{E}(\varepsilon) \\
  &amp;= \beta,
\end{align*}\]</span>
since <span class="math inline">\(\mathbb{E}(\varepsilon) = 0\)</span>.</p>
<p>For the covariance we find
<span class="math display">\[\begin{align*}
  \mathop{\mathrm{Cov}}(\hat\beta)
  &amp;= \mathop{\mathrm{Cov}}\bigl( \beta + (X^\top X)^{-1} X^\top \varepsilon\bigr) \\
  &amp;= \mathop{\mathrm{Cov}}\bigl( (X^\top X)^{-1} X^\top \varepsilon\bigr) \\
  &amp;= (X^\top X)^{-1} X^\top \mathop{\mathrm{Cov}}(\varepsilon) \bigl( (X^\top X)^{-1} X^\top \bigr)^\top \\
  &amp;= (X^\top X)^{-1} X^\top \mathop{\mathrm{Cov}}(\varepsilon) X (X^\top X)^{-1}.
\end{align*}\]</span>
Since <span class="math inline">\(\mathop{\mathrm{Cov}}(\varepsilon) = \sigma^2 I\)</span>, this simplifies to
<span class="math display">\[\begin{align*}
  \mathop{\mathrm{Cov}}(\hat\beta)
  &amp;= (X^\top X)^{-1} X^\top \sigma^2 I X (X^\top X)^{-1} \\
  &amp;= \sigma^2 (X^\top X)^{-1} X^\top X (X^\top X)^{-1} \\
  &amp;= \sigma^2 (X^\top X)^{-1}.
\end{align*}\]</span>
This completes the proof.</p>
</div>
<p>The lemma implies that <span class="math inline">\(\mathbb{E}(\hat\beta) = \beta\)</span>, <em>i.e.</em> the estimator
<span class="math inline">\(\hat\beta\)</span> is unbiased. Note that for this statement we only used <span class="math inline">\(\mathbb{E}(\varepsilon) = 0\)</span> to compute the expectation of <span class="math inline">\(\hat\beta\)</span>. Thus, the estimator will still
be unbiased for correlated noise or for noise which is not normally distributed.</p>
<p>We have seen <a href="S03-cov.html#eq:Cov-diag-elem">earlier</a> that the diagonal elements
of a covariance matrix give the variances of the elements of the random vector.
Setting <span class="math inline">\(C := (X^\top X)^{-1}\)</span> as a shorthand here, we find that the
individual estimated coefficients <span class="math inline">\(\hat\beta_i\)</span> satisfy
<span class="math display" id="eq:beta-hat-i">\[\begin{equation}
  \hat\beta_i
  \sim \mathcal{N}\bigl( \beta, \sigma^2 C_{ii} \bigr)  \tag{4.2}
\end{equation}\]</span>
for <span class="math inline">\(i \in \{1, \ldots, n\}\)</span>.</p>
<p>These results about the (co-)variances of the estimator are not very
useful in practice, because the error variance <span class="math inline">\(\sigma^2\)</span> is usually unknown.
To derive more useful results, we will consider how to estimate this
variance.</p>
</div>
<div id="hat-matrix" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Properties of the Hat Matrix<a href="S04-model.html#hat-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/ObG3Q5xgrWM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>In this and the following sections we will use various
properties of the hat matrix <span class="math inline">\(H = X (X^\top X)^{-1} X^\top\)</span>.</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-14" class="lemma"><strong>Lemma 4.2  </strong></span>The hat matrix <span class="math inline">\(H\)</span> has the following properties:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(H\)</span> is symmetric, <em>i.e.</em> <span class="math inline">\(H^\top = H\)</span>.</li>
<li><span class="math inline">\(H\)</span> is idempotent, <em>i.e.</em> <span class="math inline">\(H^2 = H\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em>. </span>For the first statement we have
<span class="math display">\[\begin{align*}
  H^\top
  &amp;= \bigl( X (X^\top X)^{-1} X^\top \bigr)^\top \\
  &amp;= (X^\top)^\top \bigl((X^\top X)^{-1}\bigr)^\top X^\top \\
  &amp;= X (X^\top X)^{-1} X^\top \\
  &amp;= H,
\end{align*}\]</span>
where we used that the inverse of a symmetric matrix is symmetric.
The second statement follow from
<span class="math display">\[\begin{align*}
  H^2
  &amp;= \bigl( X (X^\top X)^{-1} X^\top \bigr) \bigl( X (X^\top X)^{-1} X^\top \bigr) \\
  &amp;= X \bigl( (X^\top X)^{-1} X^\top X \bigr) (X^\top X)^{-1} X^\top \\
  &amp;= X (X^\top X)^{-1} X^\top.
\end{align*}\]</span>
This completes the proof.</p>
</div>
<p>Both properties from the lemma carry over from <span class="math inline">\(H\)</span> to <span class="math inline">\(I - H\)</span>: we
have <span class="math inline">\((I - H)^\top = I^\top - H^\top = I - H\)</span> and
<span class="math display">\[\begin{align*}
  (I - H)^2
  &amp;= (I - H)(I - H) \\
  &amp;= I^2 - HI - IH + H^2 \\
  &amp;= I - H - H + H \\
  &amp;= I - H.
\end{align*}\]</span></p>
<p>For future reference we also state two simpler results: we have
<span class="math display" id="eq:XH">\[\begin{equation}
  H X = X (X^\top X)^{-1} X^\top X = X  \tag{4.3}
\end{equation}\]</span>
and
<span class="math display" id="eq:XIH">\[\begin{equation}
  (I - H) X = IX - HX = X - X = 0.  \tag{4.4}
\end{equation}\]</span></p>
<p>Finally, if we have a vector <span class="math inline">\(v \in \mathbb{R}^n\)</span> we can write <span class="math inline">\(v\)</span> as
<span class="math display">\[\begin{align*}
  v
  &amp;= (H + I - H)v \\
  &amp;= Hv + (I-H)v.
\end{align*}\]</span>
The inner product between these two components is
<span class="math display">\[\begin{align*}
  (Hv)^\top (I-H)v
  &amp;= v^\top H^\top (I-H) v \\
  &amp;= v^\top H (I-H) v \\
  &amp;= v^\top (H-H^2) v \\
  &amp;= v^\top (H-H) v \\
  &amp;= 0,
\end{align*}\]</span>
so the two vectors are orthogonal.
As a result we get
<span class="math display">\[\begin{align*}
  \|v\|^2
  &amp;= v^\top v \\
  &amp;= \bigl( Hv + (I-H)v \bigr)^\top \bigl( Hv + (I-H)v \bigr) \\
  &amp;= (Hv)^\top Hv + 2 (Hv)^\top (I-H)v + \bigl((I-H)v\bigr)^\top (I-H)v \\
  &amp;= \| Hv \|^2 + \|(I-H)v \|^2
\end{align*}\]</span>
for any vector <span class="math inline">\(v \in \mathbb{R}^n\)</span>.
(This is <a href="https://en.wikipedia.org/wiki/Pythagorean_theorem">Pythagoras’ theorem</a> in <span class="math inline">\(\mathbb{R}^n\)</span>.)
Since <span class="math inline">\(\hat y = Hy\)</span> and <span class="math inline">\(\hat\varepsilon= (I - H)y\)</span>, we can apply this idea to the
vector <span class="math inline">\(v = y\)</span> to get
<span class="math display" id="eq:eps-y-orth">\[\begin{equation}
  \|y\|^2
  = \|\hat y\|^2 + \|\hat\varepsilon\|^2.  \tag{4.5}
\end{equation}\]</span></p>
<p>We note without proof that geometrically, <span class="math inline">\(H\)</span> can be interpreted as the
orthogonal projection onto the subspace of <span class="math inline">\(\mathbb{R}^n\)</span> which is spanned by
the columns of <span class="math inline">\(X\)</span>. This subspace contains the possible output vectors
of the linear system and the least squares procedure finds the point <span class="math inline">\(\hat y\)</span>
in this subspace which is closest to the observed data <span class="math inline">\(y\in\mathbb{R}^n\)</span>.</p>
<p>Some authors define:</p>
<ul>
<li><span class="math inline">\(\mathrm{SS}_\mathrm{T} = \sum_{i=1}^n y_i^2\)</span> (where “T” stands for “total”)</li>
<li><span class="math inline">\(\mathrm{SS}_\mathrm{R} = \sum_{i=1}^n \hat y_i^2\)</span> (where “R” stands for “regression”)</li>
<li><span class="math inline">\(\mathrm{SS}_\mathrm{E} = \sum_{i=1}^n \hat\varepsilon_i^2 = \sum_{i=1}^n (y_i-\hat y_i)^2\)</span> (where “E” stands for “error”)</li>
</ul>
<p>Using this notation, our equation
<span class="math inline">\(\|y\|^2 = \|\hat y\|^2 + \|\hat\varepsilon\|^2\)</span>
turns into
<span class="math display">\[\begin{equation*}
  \mathrm{SS}_\mathrm{T}
  = \mathrm{SS}_\mathrm{R} + \mathrm{SS}_\mathrm{E}.
\end{equation*}\]</span></p>
</div>
<div id="Cochran" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Cochran’s theorem<a href="S04-model.html#Cochran" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/ZFru_oHHq98" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>Our main tool in this and the following section will be a simplified
version of <a href="https://en.wikipedia.org/wiki/Cochran%27s_theorem">Cochran’s theorem</a>:</p>
<div class="theorem">
<p><span id="thm:Cochran" class="theorem"><strong>Theorem 4.1  </strong></span>The following statements are true:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\frac{1}{\sigma^2} \varepsilon^\top H \varepsilon\sim \chi^2(p+1)\)</span></p></li>
<li><p><span class="math inline">\(\frac{1}{\sigma^2} \varepsilon^\top (I - H) \varepsilon\sim \chi^2(n - p - 1)\)</span></p></li>
<li><p><span class="math inline">\(H \varepsilon\)</span> and <span class="math inline">\((I-H)\varepsilon\)</span> are independent.</p></li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>Since <span class="math inline">\(H\)</span> is symmetric, we can diagonalise <span class="math inline">\(H\)</span> (see <a href="Sx1-matrices.html#thm:spectral">A.2</a>
in the appendix): there is an orthogonal matrix <span class="math inline">\(U\)</span> such that
<span class="math inline">\(D := U H U^\top\)</span> is diagonal, and the diagonal elements of <span class="math inline">\(D\)</span> are
the eigenvalues of <span class="math inline">\(H\)</span>. Since <span class="math inline">\(H\)</span> is idempotent, these diagonal elements
can only be <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. Also, since <span class="math inline">\(U\)</span> is orthogonal, we have <span class="math inline">\(U^\top U = I\)</span>
and thus
<span class="math display">\[\begin{equation*}
  U^\top D U
  = U^\top U H U^\top U
  = H.
\end{equation*}\]</span>
The same matrix <span class="math inline">\(U\)</span> also diagonalises <span class="math inline">\(I-H\)</span>, since
<span class="math inline">\(U (I -H) U^\top = U U^\top - U H U^\top = I - D\)</span>. Exactly one
of the diagonal elements <span class="math inline">\(D_{ii}\)</span> and <span class="math inline">\((I - D)_{ii}\)</span> is <span class="math inline">\(1\)</span> and the other
one is <span class="math inline">\(0\)</span> for every <span class="math inline">\(i\)</span>.</p>
<p>Since <span class="math inline">\(\varepsilon\sim \mathcal{N}(0, \sigma^2 I)\)</span> we find that <span class="math inline">\(\eta := U \varepsilon\)</span> is normally
distributed with mean <span class="math inline">\(U 0 = 0\)</span> and covariance matrix <span class="math inline">\(\sigma^2 U I U^\top = \sigma^2 U U^\top = \sigma^2 I\)</span>. Thus <span class="math inline">\(\eta\)</span> has the same distribution as
<span class="math inline">\(\varepsilon\)</span> does: <span class="math inline">\(\eta \sim \mathcal{N}(0, \sigma^2I)\)</span> and the components <span class="math inline">\(\eta_i\)</span>
are independent of each other. We have
<span class="math display">\[\begin{equation*}
  H \varepsilon
  = U^\top D U \varepsilon
  = U^\top D \eta.
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
  (I - H) \varepsilon
  = U^\top (I - D) U \varepsilon
  = U^\top (I - D) \eta.
\end{equation*}\]</span>
Since <span class="math inline">\((D\eta)_i = 0\)</span> if <span class="math inline">\(D_{ii}=0\)</span> and <span class="math inline">\(\bigl((I - D) \eta)_i = 0\)</span>
otherwise, each component of <span class="math inline">\(\eta\)</span> contributes to exactly one of the
two vectors <span class="math inline">\(D\eta\)</span> and <span class="math inline">\((I-D)\eta\)</span>. Thus, <span class="math inline">\(D\eta\)</span> and <span class="math inline">\((I-D)\eta\)</span>
are independent, and thus <span class="math inline">\(H\varepsilon\)</span> and <span class="math inline">\((I - H)\varepsilon\)</span> are also independent.
This proves the third statement of the theorem.</p>
<p>For the first statement, we note that
<span class="math display">\[\begin{align*}
  \varepsilon^\top H \varepsilon
  &amp;= \varepsilon^\top U^\top D U \varepsilon\\
  &amp;= \eta^\top D \eta \\
  &amp;= \sum_{i=1 \atop D_{ii}=1}^n \eta_i^2.
\end{align*}\]</span>
Since <span class="math inline">\((X^\top X) \in\mathbb{R}^{(p+1)\times (p+1)}\)</span> is invertible, one can show
that <span class="math inline">\(\mathop{\mathrm{rank}}(H) = p+1\)</span> and thus that there are <span class="math inline">\(p+1\)</span> terms contributing
to the sum (we skip the proof of this statement here). Thus,
<span class="math display">\[\begin{equation*}
  \frac{1}{\sigma^2} \varepsilon^\top H \varepsilon
  = \sum_{i=1 \atop D_{ii}=1}^n \bigl(\eta_i/\sigma)^2
\end{equation*}\]</span>
is the sum of the squares of <span class="math inline">\(p+1\)</span> independent standard normals,
and thus is <span class="math inline">\(\chi^2(p+1)\)</span> distributed. This completes the proof of the
first statement.</p>
<p>Finally, the second statement follows in much of the same way as the
first one, except that <span class="math inline">\(H\)</span> is replaced with <span class="math inline">\(I-H\)</span> and the sum is
over the <span class="math inline">\(n - p - 1\)</span> indices <span class="math inline">\(i\)</span> where <span class="math inline">\(D_{ii} = 0\)</span>.
This completes the proof.</p>
</div>
<p>Expressions of the form <span class="math inline">\(x^\top A x\)</span> for <span class="math inline">\(x\in\mathbb{R}^n\)</span> and <span class="math inline">\(A\in\mathbb{R}^{n\times n}\)</span>
are called <strong>quadratic forms</strong>.</p>
<p>While the theorem as written only states that <span class="math inline">\(H \varepsilon\)</span> and <span class="math inline">\((I - H)\varepsilon\)</span>
are independent of each other, we can replace one or both of these terms
the corresponding quadratic forms as still keep the independence. Since
<span class="math inline">\((H \varepsilon)^\top (H \varepsilon) = \varepsilon^\top H^\top H \varepsilon= \varepsilon^\top H \varepsilon\)</span>,
the quadratic form <span class="math inline">\(\varepsilon^\top H \varepsilon\)</span> is a function of <span class="math inline">\(H \varepsilon\)</span> and a similar
statement holds with <span class="math inline">\(H-I\)</span> instead of <span class="math inline">\(H\)</span>.</p>
</div>
<div id="var-est-bias" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Estimating the Error Variance<a href="S04-model.html#var-est-bias" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/TgN7Ah4ycf4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>So far we have only considered how to estimate the parameter
vector <span class="math inline">\(\beta\)</span> and we have ignored the parameter <span class="math inline">\(\sigma^2\)</span>.
We will see that an unbiased estimator for <span class="math inline">\(\sigma^2\)</span> is
given by
<span class="math display" id="eq:hat-sigma-squared">\[\begin{equation}
  \hat\sigma^2
  := \frac{1}{n-p-1} \sum_{i=1}^n (y_i - \hat y_i)^2, \tag{4.6}
\end{equation}\]</span>
where <span class="math inline">\(\hat y_i\)</span> are the fitted values from equation <a href="S02-multiple.html#eq:fitted-values">(2.5)</a>.
As for the one-dimensional case in <a href="S01-simple.html#eq:reg-sigma-est">(1.4)</a>, the estimate
does not have the prefactor <span class="math inline">\(1/n\)</span>, which one might naively expect,
but the denominator is decreased by one for each component of the
vector <span class="math inline">\(\beta\)</span>. Using Cochran’s theorem, we can now show that the estimator
<span class="math inline">\(\hat\sigma^2\)</span> is unbiased.</p>
<p>We first note that
<span class="math display">\[\begin{align*}
  (n - p - 1) \hat\sigma^2
  &amp;= (y - \hat y)^\top (y - \hat y) \\
  &amp;= (y - H y)^\top (y - H y) \\
  &amp;= y^\top (I - H)^\top (I - H) y \\
  &amp;= y^\top (I - H) y.
\end{align*}\]</span>
To determine the bias, we need to use <span class="math inline">\(Y = X\beta + \varepsilon\)</span> in place of the
data. This gives
<span class="math display">\[\begin{align*}
  (n - p - 1) \hat\sigma^2
  &amp;= Y^\top (I-H) Y \\
  &amp;= (X\beta + \varepsilon)^\top (I-H) (X\beta + \varepsilon) \\
  &amp;= \beta^\top X^\top (I-H) X \beta
      + 2 \varepsilon^\top (I-H) X \beta
      + \varepsilon^\top (I-H) \varepsilon\\
  &amp;= \varepsilon^\top (I-H) \varepsilon,
\end{align*}\]</span>
where we used equation <a href="S04-model.html#eq:XIH">(4.4)</a> to see that the first two terms in
the sum equal zero.</p>
<p>Now we can apply Cochran’s theorem. This shows that
<span class="math display" id="eq:sigma-hat-chi-squared">\[\begin{equation}
  \frac{1}{\sigma^2} (n - p - 1) \hat\sigma^2
  = \frac{1}{\sigma^2} \varepsilon^\top (I-H) \varepsilon
  \sim \chi^2(n - p - 1).  \tag{4.7}
\end{equation}\]</span>
Since the expectation of a <span class="math inline">\(\chi^2(\nu)\)</span> distribution equals <span class="math inline">\(\nu\)</span>
(see appendix <a href="Sx2-probability.html#chi-square">B.2</a>), we find
<span class="math display">\[\begin{equation*}
  \frac{1}{\sigma^2} (n - p - 1) \mathbb{E}(\hat\sigma^2)
  = n - p - 1
\end{equation*}\]</span>
and thus
<span class="math display">\[\begin{equation*}
  \mathbb{E}(\hat\sigma^2)
  = \sigma^2.
\end{equation*}\]</span>
This proves that <span class="math inline">\(\hat\sigma^2\)</span> is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>.</p>
<div class="mysummary">
<p><strong>Summary</strong></p>
<ul>
<li>The least squares estimator for the regression coefficients is unbiased.</li>
<li>The hat matrix is idempotent and symmetric.</li>
<li>Cochran’s theorem allows to understand the distribution of some
quadratic forms involving the hat matrix.</li>
<li><span class="math inline">\(\hat\sigma^2\)</span> is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="S03-cov.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="S05-single.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH3714.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
