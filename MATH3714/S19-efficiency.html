<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 19 Efficiency of Robust Estimators | MATH3714 Linear Regression and Robustness</title>
  <meta name="description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2024/25" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 19 Efficiency of Robust Estimators | MATH3714 Linear Regression and Robustness" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2024/25" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 19 Efficiency of Robust Estimators | MATH3714 Linear Regression and Robustness" />
  
  <meta name="twitter:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2024/25" />
  

<meta name="author" content="Jochen Voss" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="P05.html"/>
<link rel="next" href="Sx1-matrices.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="jvstyle.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">MATH3714 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="S01-simple.html"><a href="S01-simple.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="S01-simple.html"><a href="S01-simple.html#residual-sum-of-squares"><i class="fa fa-check"></i><b>1.1</b> Residual Sum of Squares</a></li>
<li class="chapter" data-level="1.2" data-path="S01-simple.html"><a href="S01-simple.html#linear-regression-as-a-parameter-estimation-problem"><i class="fa fa-check"></i><b>1.2</b> Linear Regression as a Parameter Estimation Problem</a></li>
<li class="chapter" data-level="1.3" data-path="S01-simple.html"><a href="S01-simple.html#sec:simple-mat"><i class="fa fa-check"></i><b>1.3</b> Matrix Notation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="S02-multiple.html"><a href="S02-multiple.html"><i class="fa fa-check"></i><b>2</b> Least Squares Estimates</a>
<ul>
<li class="chapter" data-level="2.1" data-path="S02-multiple.html"><a href="S02-multiple.html#data-and-models"><i class="fa fa-check"></i><b>2.1</b> Data and Models</a></li>
<li class="chapter" data-level="2.2" data-path="S02-multiple.html"><a href="S02-multiple.html#the-normal-equations"><i class="fa fa-check"></i><b>2.2</b> The Normal Equations</a></li>
<li class="chapter" data-level="2.3" data-path="S02-multiple.html"><a href="S02-multiple.html#fitted-values"><i class="fa fa-check"></i><b>2.3</b> Fitted Values</a></li>
<li class="chapter" data-level="2.4" data-path="S02-multiple.html"><a href="S02-multiple.html#models-without-intercept"><i class="fa fa-check"></i><b>2.4</b> Models Without Intercept</a></li>
<li class="chapter" data-level="2.5" data-path="S02-multiple.html"><a href="S02-multiple.html#example"><i class="fa fa-check"></i><b>2.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html"><i class="fa fa-check"></i>Interlude: Linear Regression in R</a>
<ul>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-fitting"><i class="fa fa-check"></i>Fitting a Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-model"><i class="fa fa-check"></i>Understanding the Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-predict"><i class="fa fa-check"></i>Making Predictions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="S03-cov.html"><a href="S03-cov.html"><i class="fa fa-check"></i><b>3</b> Random Vectors and Covariance</a>
<ul>
<li class="chapter" data-level="3.1" data-path="S03-cov.html"><a href="S03-cov.html#expectation"><i class="fa fa-check"></i><b>3.1</b> Expectation</a></li>
<li class="chapter" data-level="3.2" data-path="S03-cov.html"><a href="S03-cov.html#sec:covariance"><i class="fa fa-check"></i><b>3.2</b> Covariance Matrix</a></li>
<li class="chapter" data-level="3.3" data-path="S03-cov.html"><a href="S03-cov.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>3.3</b> The Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P01.html"><a href="P01.html"><i class="fa fa-check"></i>Problem Sheet 1</a></li>
<li class="chapter" data-level="4" data-path="S04-model.html"><a href="S04-model.html"><i class="fa fa-check"></i><b>4</b> Properties of the Least Squares Estimate</a>
<ul>
<li class="chapter" data-level="4.1" data-path="S04-model.html"><a href="S04-model.html#mean-and-covariance"><i class="fa fa-check"></i><b>4.1</b> Mean and Covariance</a></li>
<li class="chapter" data-level="4.2" data-path="S04-model.html"><a href="S04-model.html#hat-matrix"><i class="fa fa-check"></i><b>4.2</b> Properties of the Hat Matrix</a></li>
<li class="chapter" data-level="4.3" data-path="S04-model.html"><a href="S04-model.html#Cochran"><i class="fa fa-check"></i><b>4.3</b> Cochran’s theorem</a></li>
<li class="chapter" data-level="4.4" data-path="S04-model.html"><a href="S04-model.html#var-est-bias"><i class="fa fa-check"></i><b>4.4</b> Estimating the Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="S05-single.html"><a href="S05-single.html"><i class="fa fa-check"></i><b>5</b> Uncertainty for Individual Regression Coefficients</a>
<ul>
<li class="chapter" data-level="5.1" data-path="S05-single.html"><a href="S05-single.html#measuring-the-estimation-error"><i class="fa fa-check"></i><b>5.1</b> Measuring the Estimation Error</a></li>
<li class="chapter" data-level="5.2" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.3" data-path="S05-single.html"><a href="S05-single.html#hypthesis-tests"><i class="fa fa-check"></i><b>5.3</b> Hypthesis Tests</a></li>
<li class="chapter" data-level="5.4" data-path="S05-single.html"><a href="S05-single.html#r-experiments"><i class="fa fa-check"></i><b>5.4</b> R Experiments</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="S05-single.html"><a href="S05-single.html#fitting-the-model"><i class="fa fa-check"></i><b>5.4.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.4.2" data-path="S05-single.html"><a href="S05-single.html#estimating-the-variance-of-the-error"><i class="fa fa-check"></i><b>5.4.2</b> Estimating the Variance of the Error</a></li>
<li class="chapter" data-level="5.4.3" data-path="S05-single.html"><a href="S05-single.html#estimating-the-standard-errors"><i class="fa fa-check"></i><b>5.4.3</b> Estimating the Standard Errors</a></li>
<li class="chapter" data-level="5.4.4" data-path="S05-single.html"><a href="S05-single.html#hypothesis-tests"><i class="fa fa-check"></i><b>5.4.4</b> Hypothesis tests</a></li>
<li class="chapter" data-level="5.4.5" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals-1"><i class="fa fa-check"></i><b>5.4.5</b> Confidence Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html"><i class="fa fa-check"></i><b>6</b> Estimating Coefficients Simultaneously</a>
<ul>
<li class="chapter" data-level="6.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-dist"><i class="fa fa-check"></i><b>6.1</b> Linear Combinations of Coefficients</a></li>
<li class="chapter" data-level="6.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-CI"><i class="fa fa-check"></i><b>6.2</b> Confidence Regions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#result"><i class="fa fa-check"></i><b>6.2.1</b> Result</a></li>
<li class="chapter" data-level="6.2.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#numerical-experiments"><i class="fa fa-check"></i><b>6.2.2</b> Numerical Experiments</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-test"><i class="fa fa-check"></i><b>6.3</b> Hypothesis Tests</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html"><i class="fa fa-check"></i>Interlude: Loading Data into R</a>
<ul>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-csv-files"><i class="fa fa-check"></i>Importing CSV Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-microsoft-excel-files"><i class="fa fa-check"></i>Importing Microsoft Excel Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#checking-the-imported-data"><i class="fa fa-check"></i>Checking the Imported Data</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#common-problems"><i class="fa fa-check"></i>Common Problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="S07-examples.html"><a href="S07-examples.html"><i class="fa fa-check"></i><b>7</b> Examples</a>
<ul>
<li class="chapter" data-level="7.1" data-path="S07-examples.html"><a href="S07-examples.html#simple-confidence-interval"><i class="fa fa-check"></i><b>7.1</b> Simple Confidence Interval</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles"><i class="fa fa-check"></i><b>7.1.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.1.2" data-path="S07-examples.html"><a href="S07-examples.html#from-the-lm-output"><i class="fa fa-check"></i><b>7.1.2</b> From the <code>lm()</code> Output</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="S07-examples.html"><a href="S07-examples.html#confidence-intervals-for-the-mean"><i class="fa fa-check"></i><b>7.2</b> Confidence Intervals for the Mean</a></li>
<li class="chapter" data-level="7.3" data-path="S07-examples.html"><a href="S07-examples.html#testing-a-single-coefficient"><i class="fa fa-check"></i><b>7.3</b> Testing a Single Coefficient</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-1"><i class="fa fa-check"></i><b>7.3.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.3.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-i"><i class="fa fa-check"></i><b>7.3.2</b> Using the <code>lm()</code> Output, I</a></li>
<li class="chapter" data-level="7.3.3" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-ii"><i class="fa fa-check"></i><b>7.3.3</b> Using the <code>lm()</code> Output, II</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="S07-examples.html"><a href="S07-examples.html#testing-multiple-coefficents"><i class="fa fa-check"></i><b>7.4</b> Testing Multiple Coefficents</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-2"><i class="fa fa-check"></i><b>7.4.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.4.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output"><i class="fa fa-check"></i><b>7.4.2</b> Using the <code>lm()</code> output</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="P02.html"><a href="P02.html"><i class="fa fa-check"></i>Problem Sheet 2</a></li>
<li class="chapter" data-level="8" data-path="S08-influence.html"><a href="S08-influence.html"><i class="fa fa-check"></i><b>8</b> The Influence of Observations</a>
<ul>
<li class="chapter" data-level="8.1" data-path="S08-influence.html"><a href="S08-influence.html#deleting"><i class="fa fa-check"></i><b>8.1</b> Deleting Observations</a></li>
<li class="chapter" data-level="8.2" data-path="S08-influence.html"><a href="S08-influence.html#influence"><i class="fa fa-check"></i><b>8.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="S09-plots.html"><a href="S09-plots.html"><i class="fa fa-check"></i><b>9</b> Diagnostic Plots</a>
<ul>
<li class="chapter" data-level="9.1" data-path="S09-plots.html"><a href="S09-plots.html#residual-plots"><i class="fa fa-check"></i><b>9.1</b> Residual Plots</a></li>
<li class="chapter" data-level="9.2" data-path="S09-plots.html"><a href="S09-plots.html#q-q-plots"><i class="fa fa-check"></i><b>9.2</b> Q-Q Plots</a></li>
<li class="chapter" data-level="9.3" data-path="S09-plots.html"><a href="S09-plots.html#other-plot-types"><i class="fa fa-check"></i><b>9.3</b> Other Plot Types</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="S10-improving.html"><a href="S10-improving.html"><i class="fa fa-check"></i><b>10</b> Improving the Model Fit</a>
<ul>
<li class="chapter" data-level="10.1" data-path="S10-improving.html"><a href="S10-improving.html#linearising-the-mean"><i class="fa fa-check"></i><b>10.1</b> Linearising the Mean</a></li>
<li class="chapter" data-level="10.2" data-path="S10-improving.html"><a href="S10-improving.html#stabilising-the-variance"><i class="fa fa-check"></i><b>10.2</b> Stabilising the Variance</a></li>
<li class="chapter" data-level="10.3" data-path="S10-improving.html"><a href="S10-improving.html#power-transform"><i class="fa fa-check"></i><b>10.3</b> The Power Transform</a></li>
<li class="chapter" data-level="10.4" data-path="S10-improving.html"><a href="S10-improving.html#candidate-models"><i class="fa fa-check"></i><b>10.4</b> Candidate Models</a></li>
<li class="chapter" data-level="10.5" data-path="S10-improving.html"><a href="S10-improving.html#misspecified-models"><i class="fa fa-check"></i><b>10.5</b> Misspecified Models</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="S10-improving.html"><a href="S10-improving.html#missing-variables"><i class="fa fa-check"></i><b>10.5.1</b> Missing Variables</a></li>
<li class="chapter" data-level="10.5.2" data-path="S10-improving.html"><a href="S10-improving.html#unnecessary-variables"><i class="fa fa-check"></i><b>10.5.2</b> Unnecessary Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html"><i class="fa fa-check"></i><b>11</b> Measures for Model Fit</a>
<ul>
<li class="chapter" data-level="11.1" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html#R-squared"><i class="fa fa-check"></i><b>11.1</b> The Coefficient of Multiple Determination</a></li>
<li class="chapter" data-level="11.2" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html#error-var"><i class="fa fa-check"></i><b>11.2</b> Error Variance</a></li>
<li class="chapter" data-level="11.3" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html#PRESS"><i class="fa fa-check"></i><b>11.3</b> Prediction Error Sum of Squares</a></li>
<li class="chapter" data-level="11.4" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html#AIC"><i class="fa fa-check"></i><b>11.4</b> Akaike’s Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I03-lm-output.html"><a href="I03-lm-output.html"><i class="fa fa-check"></i>Interlude: Understanding the <code>lm()</code> Output</a></li>
<li class="chapter" data-level="" data-path="P03.html"><a href="P03.html"><i class="fa fa-check"></i>Problem Sheet 3</a></li>
<li class="chapter" data-level="12" data-path="S12-automatic.html"><a href="S12-automatic.html"><i class="fa fa-check"></i><b>12</b> Automatic Model Selection</a>
<ul>
<li class="chapter" data-level="12.1" data-path="S12-automatic.html"><a href="S12-automatic.html#exhaustive-search"><i class="fa fa-check"></i><b>12.1</b> Exhaustive Search</a></li>
<li class="chapter" data-level="12.2" data-path="S12-automatic.html"><a href="S12-automatic.html#search-algorithm"><i class="fa fa-check"></i><b>12.2</b> Search Algorithm</a></li>
<li class="chapter" data-level="12.3" data-path="S12-automatic.html"><a href="S12-automatic.html#other-methods"><i class="fa fa-check"></i><b>12.3</b> Other Methods</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="S12-automatic.html"><a href="S12-automatic.html#stepwise-forward-selection"><i class="fa fa-check"></i><b>12.3.1</b> Stepwise Forward Selection</a></li>
<li class="chapter" data-level="12.3.2" data-path="S12-automatic.html"><a href="S12-automatic.html#stepwise-backward-selection"><i class="fa fa-check"></i><b>12.3.2</b> Stepwise Backward Selection</a></li>
<li class="chapter" data-level="12.3.3" data-path="S12-automatic.html"><a href="S12-automatic.html#hybrid-methods"><i class="fa fa-check"></i><b>12.3.3</b> Hybrid Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="S13-factors.html"><a href="S13-factors.html"><i class="fa fa-check"></i><b>13</b> Factors</a>
<ul>
<li class="chapter" data-level="13.1" data-path="S13-factors.html"><a href="S13-factors.html#indicator-variables"><i class="fa fa-check"></i><b>13.1</b> Indicator Variables</a></li>
<li class="chapter" data-level="13.2" data-path="S13-factors.html"><a href="S13-factors.html#interactions"><i class="fa fa-check"></i><b>13.2</b> Interactions</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="S14-examples.html"><a href="S14-examples.html"><i class="fa fa-check"></i><b>14</b> Examples</a>
<ul>
<li class="chapter" data-level="14.1" data-path="S14-examples.html"><a href="S14-examples.html#interactions-example"><i class="fa fa-check"></i><b>14.1</b> Use of Interaction Terms in Modelling</a></li>
<li class="chapter" data-level="14.2" data-path="S14-examples.html"><a href="S14-examples.html#codings"><i class="fa fa-check"></i><b>14.2</b> Alternative Factor Codings</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="S15-multicoll.html"><a href="S15-multicoll.html"><i class="fa fa-check"></i><b>15</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="15.1" data-path="S15-multicoll.html"><a href="S15-multicoll.html#consequences-of-multicollinearity"><i class="fa fa-check"></i><b>15.1</b> Consequences of Multicollinearity</a></li>
<li class="chapter" data-level="15.2" data-path="S15-multicoll.html"><a href="S15-multicoll.html#detecting-multicollinearity"><i class="fa fa-check"></i><b>15.2</b> Detecting Multicollinearity</a></li>
<li class="chapter" data-level="15.3" data-path="S15-multicoll.html"><a href="S15-multicoll.html#mitigations"><i class="fa fa-check"></i><b>15.3</b> Mitigations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P04.html"><a href="P04.html"><i class="fa fa-check"></i>Problem Sheet 4</a></li>
<li class="chapter" data-level="16" data-path="S16-ridge.html"><a href="S16-ridge.html"><i class="fa fa-check"></i><b>16</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="16.1" data-path="S16-ridge.html"><a href="S16-ridge.html#definition-the-estimator"><i class="fa fa-check"></i><b>16.1</b> Definition the Estimator</a></li>
<li class="chapter" data-level="16.2" data-path="S16-ridge.html"><a href="S16-ridge.html#properties-of-the-estimate"><i class="fa fa-check"></i><b>16.2</b> Properties of the Estimate</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="S16-ridge.html"><a href="S16-ridge.html#bias"><i class="fa fa-check"></i><b>16.2.1</b> Bias</a></li>
<li class="chapter" data-level="16.2.2" data-path="S16-ridge.html"><a href="S16-ridge.html#variance"><i class="fa fa-check"></i><b>16.2.2</b> Variance</a></li>
<li class="chapter" data-level="16.2.3" data-path="S16-ridge.html"><a href="S16-ridge.html#mean-squared-error"><i class="fa fa-check"></i><b>16.2.3</b> Mean Squared Error</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="S16-ridge.html"><a href="S16-ridge.html#standardisation"><i class="fa fa-check"></i><b>16.3</b> Standardisation</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="S17-robust.html"><a href="S17-robust.html"><i class="fa fa-check"></i><b>17</b> Robust Regression</a>
<ul>
<li class="chapter" data-level="17.1" data-path="S17-robust.html"><a href="S17-robust.html#outliers"><i class="fa fa-check"></i><b>17.1</b> Outliers</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="S17-robust.html"><a href="S17-robust.html#leverage"><i class="fa fa-check"></i><b>17.1.1</b> Leverage</a></li>
<li class="chapter" data-level="17.1.2" data-path="S17-robust.html"><a href="S17-robust.html#studentised-residuals"><i class="fa fa-check"></i><b>17.1.2</b> Studentised Residuals</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="S17-robust.html"><a href="S17-robust.html#breakdown-points"><i class="fa fa-check"></i><b>17.2</b> Breakdown Points</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="S18-m-est.html"><a href="S18-m-est.html"><i class="fa fa-check"></i><b>18</b> M-Estimators</a>
<ul>
<li class="chapter" data-level="18.1" data-path="S18-m-est.html"><a href="S18-m-est.html#definition"><i class="fa fa-check"></i><b>18.1</b> Definition</a></li>
<li class="chapter" data-level="18.2" data-path="S18-m-est.html"><a href="S18-m-est.html#iterative-methods"><i class="fa fa-check"></i><b>18.2</b> Iterative Methods</a></li>
<li class="chapter" data-level="18.3" data-path="S18-m-est.html"><a href="S18-m-est.html#objective-functions"><i class="fa fa-check"></i><b>18.3</b> Objective Functions</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="S18-m-est.html"><a href="S18-m-est.html#least-squares-method"><i class="fa fa-check"></i><b>18.3.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="18.3.2" data-path="S18-m-est.html"><a href="S18-m-est.html#least-absolute-values"><i class="fa fa-check"></i><b>18.3.2</b> Least Absolute Values</a></li>
<li class="chapter" data-level="18.3.3" data-path="S18-m-est.html"><a href="S18-m-est.html#Huber"><i class="fa fa-check"></i><b>18.3.3</b> Huber’s <span class="math inline">\(t\)</span>-function</a></li>
<li class="chapter" data-level="18.3.4" data-path="S18-m-est.html"><a href="S18-m-est.html#hampels-method"><i class="fa fa-check"></i><b>18.3.4</b> Hampel’s Method</a></li>
<li class="chapter" data-level="18.3.5" data-path="S18-m-est.html"><a href="S18-m-est.html#tukeys-bisquare-method"><i class="fa fa-check"></i><b>18.3.5</b> Tukey’s Bisquare Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="P05.html"><a href="P05.html"><i class="fa fa-check"></i>Problem Sheet 5</a></li>
<li class="chapter" data-level="19" data-path="S19-efficiency.html"><a href="S19-efficiency.html"><i class="fa fa-check"></i><b>19</b> Efficiency of Robust Estimators</a>
<ul>
<li class="chapter" data-level="19.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#efficiency"><i class="fa fa-check"></i><b>19.1</b> Efficiency</a></li>
<li class="chapter" data-level="19.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#robust-estimators"><i class="fa fa-check"></i><b>19.2</b> Robust estimators</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#least-median-of-squares"><i class="fa fa-check"></i><b>19.2.1</b> Least Median of Squares</a></li>
<li class="chapter" data-level="19.2.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#least-trimmed-squares"><i class="fa fa-check"></i><b>19.2.2</b> Least Trimmed Squares</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html"><i class="fa fa-check"></i><b>A</b> Linear Algebra Reminders</a>
<ul>
<li class="chapter" data-level="A.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#vectors"><i class="fa fa-check"></i><b>A.1</b> Vectors</a></li>
<li class="chapter" data-level="A.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-rules"><i class="fa fa-check"></i><b>A.2</b> Matrices</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#transpose"><i class="fa fa-check"></i><b>A.2.1</b> Transpose</a></li>
<li class="chapter" data-level="A.2.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-vector-product"><i class="fa fa-check"></i><b>A.2.2</b> Matrix-vector Product</a></li>
<li class="chapter" data-level="A.2.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-matrix-product"><i class="fa fa-check"></i><b>A.2.3</b> Matrix-matrix Product</a></li>
<li class="chapter" data-level="A.2.4" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#rank"><i class="fa fa-check"></i><b>A.2.4</b> Rank</a></li>
<li class="chapter" data-level="A.2.5" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#trace"><i class="fa fa-check"></i><b>A.2.5</b> Trace</a></li>
<li class="chapter" data-level="A.2.6" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.2.6</b> Matrix Inverse</a></li>
<li class="chapter" data-level="A.2.7" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.2.7</b> Orthogonal Matrices</a></li>
<li class="chapter" data-level="A.2.8" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#positive-definite"><i class="fa fa-check"></i><b>A.2.8</b> Positive Definite Matrices</a></li>
<li class="chapter" data-level="A.2.9" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#idempotent"><i class="fa fa-check"></i><b>A.2.9</b> Idempotent Matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#eigenvalues"><i class="fa fa-check"></i><b>A.3</b> Eigenvalues</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="Sx2-probability.html"><a href="Sx2-probability.html"><i class="fa fa-check"></i><b>B</b> Probability Reminders</a>
<ul>
<li class="chapter" data-level="B.1" data-path="Sx2-probability.html"><a href="Sx2-probability.html#independence"><i class="fa fa-check"></i><b>B.1</b> Independence</a></li>
<li class="chapter" data-level="B.2" data-path="Sx2-probability.html"><a href="Sx2-probability.html#chi-square"><i class="fa fa-check"></i><b>B.2</b> The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="B.3" data-path="Sx2-probability.html"><a href="Sx2-probability.html#t"><i class="fa fa-check"></i><b>B.3</b> The t-distribution</a></li>
</ul></li>
<li class="divider"></li>
<li class="chapter"><span><b>THE END</b></span></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3714 Linear Regression and Robustness</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="S19-efficiency" class="section level1 hasAnchor" number="19">
<h1><span class="header-section-number">Section 19</span> Efficiency of Robust Estimators<a href="S19-efficiency.html#S19-efficiency" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In theoretical statistics, <strong>efficiency</strong> is a measure of quality of an
estimator. A more efficient estimator achieves smaller estimation error
for a given number of samples. Here we give an informal introduction
to the topic of efficiency and discuss the efficiency of different
methods for linear regression.</p>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/ilV2djzOFsA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<div id="efficiency" class="section level2 hasAnchor" number="19.1">
<h2><span class="header-section-number">19.1</span> Efficiency<a href="S19-efficiency.html#efficiency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the section about the <a href="S16-ridge.html#mean-squared-error">Mean Squared Error</a> we have seen that
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{MSE}}\nolimits\bigl( \hat\theta \bigr)
  = \mathop{\mathrm{Var}}\bigl( \hat\theta \bigr) + \mathop{\mathrm{bias}}\bigl( \hat\theta \bigr)^2
\end{equation*}\]</span>
can be used as a measure for the estimation error of an estimator <span class="math inline">\(\hat\theta\)</span>,
and if the estimator is unbiased, this expression simplifies to
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{MSE}}\nolimits\bigl( \hat\theta \bigr)
  = \mathop{\mathrm{Var}}\bigl( \hat\theta \bigr).
\end{equation*}\]</span>
Thus, “good” estimators will have small variance.
Instead of considering the full definition from theoretical statistics
(which requires the concept of Fisher information), here we simply consider
<span class="math display">\[\begin{equation*}
  \mathrm{efficiency}(\hat\theta)
  = \frac{1}{\mathop{\mathrm{Var}}(\hat\theta)}
\end{equation*}\]</span>
as a measure for the efficiency of an estimator. We will use this measure
of efficiency to compare different estimators.</p>
<div class="example">
<p><span id="exm:unlabeled-div-68" class="example"><strong>Example 19.1  </strong></span>Both the sample mean and the sample median can be used as estimators
for the population mean. Here we show that the median has lower efficiency
than the mean.</p>
<p>Suppose <span class="math inline">\(X_1,\ldots, X_n\)</span> are i.i.d. with CDF <span class="math inline">\(F(x)\)</span>, <em>i.e.</em>
<span class="math inline">\(P(X_i \leq x) = F(x)\)</span>. As before, let <span class="math inline">\(X_{(1)} \leq \cdots \leq
X_{(n)}\)</span> be the samples arranged in order of increasing value.
Then the distribution of <span class="math inline">\(X_{(k)}\)</span> has CDF
<span class="math display">\[\begin{equation*}
  P(X_{(k)} \leq x)
  = \sum_{i=k}^n {n\choose i} F(x)^i \bigl(1-F(x)\bigr)^{n-i},
\end{equation*}\]</span>
since <span class="math inline">\(X_{(k)} \leq x\)</span> requires at least <span class="math inline">\(k\)</span> out of <span class="math inline">\(n\)</span> samples to be
less than or equal to <span class="math inline">\(x\)</span>.
If we differentiate this (using product rule and chain rule) we get
the density of <span class="math inline">\(X_{(k)}\)</span>:
<span class="math display">\[\begin{align*}
  f_{(k)}(x)
  &amp;= \frac{d}{dx} P(X_{(k)} \leq x) \\
  &amp;= \cdots \\
  &amp;= f(x)\frac{n!}{(n-k)! (k-1)!} F(x)^{k-1}\bigl(1-F(x)\bigr)^{n-k}.
\end{align*}\]</span></p>
<p>For simplicity we assume that <span class="math inline">\(n\)</span> is odd, say <span class="math inline">\(n = 2m+1\)</span>. In this case the
median is <span class="math inline">\(X_\mathrm{median} = X_{(m+1)}\)</span>. Thus, the density of the median is
<span class="math display">\[\begin{align*}
  f_\mathrm{median}(x)
  &amp;= f_{(m+1)}(x) \\
  &amp;= f(x)\frac{n!}{(n-m-1)! m!} F(x)^m\bigl(1-F(x)\bigr)^{n-m-1} \\
  &amp;= f(x)\frac{n!}{m! m!} F(x)^m\bigl(1-F(x)\bigr)^m.
\end{align*}\]</span>
This density can be used to understand the behaviour of the median.</p>
<p>Now assume that <span class="math inline">\(X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)\)</span>, <em>i.e.</em>
the density is
<span class="math display">\[\begin{equation*}
  f(x)
  = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\Bigl( - \frac{(x-\mu)^2}{2\sigma^2} \Bigr)
\end{equation*}\]</span>
and the CDF is
<span class="math display">\[\begin{equation*}
  F(x) = \Phi\Bigl( \frac{x - \mu}{\sigma} \Bigr),
\end{equation*}\]</span>
where <span class="math inline">\(\Phi\)</span> is the CDF of the standard normal distribution. In this case
we know that
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{Var}}\bigl( \overline X \bigr)
  = \frac{\sigma^2}{n}.
\end{equation*}\]</span>
For comparison, we can use the density of <span class="math inline">\(X_\mathrm{median}\)</span> to show
(in a series of complicated steps) that
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{Var}}\bigl( X_\mathrm{median} \bigr)
  \sim \frac{\pi}{2} \frac{\sigma^2}{n},
\end{equation*}\]</span>
as <span class="math inline">\(n\to\infty\)</span>. Thus, for large <span class="math inline">\(n\)</span>, the relative efficiency
of the median compared to the mean is
<span class="math display">\[\begin{align*}
  \frac{\mathrm{efficiency}(X_\mathrm{median})}{\mathrm{efficiency}(\overline{X})}
  &amp;= \frac{\mathop{\mathrm{Var}}\bigl( \overline X \bigr)}{\mathop{\mathrm{Var}}\bigl( X_\mathrm{median} \bigr)}
      = \frac{\frac{\sigma^2}{n}}{\frac{\pi}{2} \frac{\sigma^2}{n}} \\
  &amp;= \frac{2}{\pi}
      = 0.637.
\end{align*}\]</span>
Thus, the sample median is a less efficient estimator for the mean than
the sample mean is.</p>
</div>
<p>The situation in the example carries over to linear regression: more robust
estimators tend to be less efficient, and there is a trade-off between
robustness and efficiency. Without proof, here we list some examples of this
principle:</p>
<ul>
<li><p>One can show that usually the variance of the least squares
estimator decreases proportionally to <span class="math inline">\(1 / n\)</span>, and thus the efficiency of
the least squares estimator increases proportionally to <span class="math inline">\(n\)</span>.</p></li>
<li><p>The value <span class="math inline">\(t\)</span> in <a href="S18-m-est.html#Huber">Huber’s method</a> can be used to control
the balance between robustness and efficiency. As <span class="math inline">\(t\to\infty\)</span>,
the method converges to least squares regression and becomes
less robust, but at the same time efficiency increases.
Huber suggests the value <span class="math inline">\(t = 1.345\sigma\)</span> and showed that for this
choice the method is <span class="math inline">\(95\%\)</span> efficient for large <span class="math inline">\(n\)</span>, compared
to least squares regression.</p></li>
<li><p>Similarly, the parameter <span class="math inline">\(a\)</span> in <a href="S18-m-est.html#tukeys-bisquare-method">Tukey’s Bisquare Method</a> controls
the trade-off between robustness and efficiency, with smaller <span class="math inline">\(a\)</span> leading
to a more robust method. In the literature it is suggested that
<span class="math inline">\(a = 4.685 \sigma\)</span> leads to <span class="math inline">\(95\%\)</span> efficiency for large <span class="math inline">\(n\)</span>, compared to
least squares regression.</p></li>
<li><p>The efficiency of the <a href="S18-m-est.html#exm:LAV">least absolute values estimator</a> can be shown
to be <span class="math inline">\(64\%\)</span>.</p></li>
</ul>
</div>
<div id="robust-estimators" class="section level2 hasAnchor" number="19.2">
<h2><span class="header-section-number">19.2</span> Robust estimators<a href="S19-efficiency.html#robust-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Since the M-estimator only addresses <span class="math inline">\(y\)</span>-space outliers, the breakdown point
of M-estimation is still <span class="math inline">\(1/n\)</span>. We now consider two estimators which have a
higher breakdown point, but lower efficiency.</p>
<div id="least-median-of-squares" class="section level3 hasAnchor" number="19.2.1">
<h3><span class="header-section-number">19.2.1</span> Least Median of Squares<a href="S19-efficiency.html#least-median-of-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Rather than minimise the sum of the residual sum of squares (or a weighted
version of it), this estimator minimises the median of squared residuals. The
estimate is given by
<span class="math display">\[\begin{equation*}
  \hat\beta^\mathrm{(LMS)}
  := \mathop{\mathrm{arg\,min}}\limits_\beta \mathop{\mathrm{median}}_{i\in\{1,\ldots,n\}} \bigl( y_i- x_i^T \beta \bigr)^2,
\end{equation*}\]</span>
where <span class="math inline">\(x_1, \ldots, x_n\)</span> are the rows of the design matrix <span class="math inline">\(X\)</span>.
This is very robust with respect to outliers, both in <span class="math inline">\(x\)</span>-direction and
<span class="math inline">\(y\)</span>-directions: Since the median is used instead of the sum, up to half
of the squared residuals can increase to infinity while the estimate stays
bounded. Thus the asymptotic breakdown point of the method is <span class="math inline">\(1/2\)</span>.</p>
<p>The least median of squares method has poor asymptotic efficiency <span class="math inline">\(n^{2/3}\)</span>.
In the limit, the relative efficiency compared to ordinary least squares is
<span class="math display">\[\begin{equation*}
  \frac{\mathrm{efficiency}(\hat\beta^\mathrm{(LMS)})}{\mathrm{efficiency}(\hat\beta^\mathrm{(LSQ)})}
  \propto \frac{n^{2/3}}{n}
  \longrightarrow 0
\end{equation*}\]</span>
as <span class="math inline">\(n\to\infty\)</span>.</p>
<p>In R, the LMS estimate can be computed using the function
<code>lqs(..., method = "lms")</code> function from the <code>MASS</code> library.</p>
<div class="example">
<p><span id="exm:unlabeled-div-69" class="example"><strong>Example 19.2  </strong></span>The following code computes an LMS regression estimate. We
introduce artifical outliers by shifting <span class="math inline">\(30\%\)</span> of the data
to the bottom right. These artificial outliers are represented
by the red circles in the plot.</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="S19-efficiency.html#cb300-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;MASS&quot;</span>) <span class="co"># for lqs()</span></span>
<span id="cb300-2"><a href="S19-efficiency.html#cb300-2" tabindex="-1"></a></span>
<span id="cb300-3"><a href="S19-efficiency.html#cb300-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20211207</span>)</span>
<span id="cb300-4"><a href="S19-efficiency.html#cb300-4" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb300-5"><a href="S19-efficiency.html#cb300-5" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb300-6"><a href="S19-efficiency.html#cb300-6" tabindex="-1"></a>y <span class="ot">&lt;-</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb300-7"><a href="S19-efficiency.html#cb300-7" tabindex="-1"></a></span>
<span id="cb300-8"><a href="S19-efficiency.html#cb300-8" tabindex="-1"></a><span class="co"># add 30% outliers</span></span>
<span id="cb300-9"><a href="S19-efficiency.html#cb300-9" tabindex="-1"></a>n.ol <span class="ot">&lt;-</span> <span class="fu">floor</span>(<span class="fl">0.3</span><span class="sc">*</span>n)</span>
<span id="cb300-10"><a href="S19-efficiency.html#cb300-10" tabindex="-1"></a>idx <span class="ot">&lt;-</span> <span class="fu">sample.int</span>(n, n.ol)</span>
<span id="cb300-11"><a href="S19-efficiency.html#cb300-11" tabindex="-1"></a>x[idx] <span class="ot">&lt;-</span> x[idx] <span class="sc">+</span> <span class="dv">15</span></span>
<span id="cb300-12"><a href="S19-efficiency.html#cb300-12" tabindex="-1"></a>y[idx] <span class="ot">&lt;-</span> y[idx] <span class="sc">-</span> <span class="dv">10</span></span>
<span id="cb300-13"><a href="S19-efficiency.html#cb300-13" tabindex="-1"></a></span>
<span id="cb300-14"><a href="S19-efficiency.html#cb300-14" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lqs</span>(y<span class="sc">~</span>x, <span class="at">method=</span><span class="st">&quot;lms&quot;</span>)</span>
<span id="cb300-15"><a href="S19-efficiency.html#cb300-15" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="fu">ifelse</span>(<span class="dv">1</span><span class="sc">:</span>n <span class="sc">%in%</span> idx, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>))</span>
<span id="cb300-16"><a href="S19-efficiency.html#cb300-16" tabindex="-1"></a><span class="fu">abline</span>(m)</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/lms-1.png" width="672" /></p>
</div>
</div>
<div id="least-trimmed-squares" class="section level3 hasAnchor" number="19.2.2">
<h3><span class="header-section-number">19.2.2</span> Least Trimmed Squares<a href="S19-efficiency.html#least-trimmed-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This takes as its objective function the sum of <span class="math inline">\(h\)</span> smallest squared residuals
and was proposed as a remedy to the low asymptotic efficiency of LMS. The least
trimmed squares estimator <span class="math inline">\(\hat\beta^\mathrm{(LTS)}\)</span> is defined as
<span class="math display">\[\begin{equation*}
  \hat\beta^\mathrm{(LTS)}
  := \mathop{\mathrm{arg\,min}}\limits_\beta \sum_{i=1}^k r_{[i]}^2(\beta),
\end{equation*}\]</span>
where <span class="math inline">\(r_{[i]}^2(\beta)\)</span> represents the <span class="math inline">\(i\)</span>th smallest value amongst
<span class="math inline">\(r_i(\beta)^2 = \bigl( y_i- x_i^T\beta \bigr)^2\)</span>.</p>
<p>The value <span class="math inline">\(k\)</span> controls the trade-off between robustness and efficiency and the
value must satisfy <span class="math inline">\(n/2 &lt; k \leq n\)</span>. For a given <span class="math inline">\(k\)</span> the method can tolerate
<span class="math inline">\(n-k\)</span> outliers. The bounday case <span class="math inline">\(k=n\)</span> corresponds to the ordinary least
squares method. The breakdown point of the method is <span class="math inline">\((n - k +1)/n\)</span>.</p>
<p>Computing the LTS estimate is a non-trivial problem, which involves fitting the
least-squares estimate to a carefully chosen subset of the samples. In R, the
LTS estimate can be computed using the function <code>lqs(..., method = "lts")</code>
function from the <code>MASS</code> library.</p>
<div class="example">
<p><span id="exm:unlabeled-div-70" class="example"><strong>Example 19.3  </strong></span>The following code computes an LTS regression estimate. The
data is the same as in the previous example.</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="S19-efficiency.html#cb301-1" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lqs</span>(y<span class="sc">~</span>x, <span class="at">method =</span> <span class="st">&quot;lts&quot;</span>, <span class="at">quantile =</span> <span class="fu">floor</span>(n<span class="sc">/</span><span class="dv">2</span>)<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb301-2"><a href="S19-efficiency.html#cb301-2" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="fu">ifelse</span>(<span class="dv">1</span><span class="sc">:</span>n <span class="sc">%in%</span> idx, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>))</span>
<span id="cb301-3"><a href="S19-efficiency.html#cb301-3" tabindex="-1"></a><span class="fu">abline</span>(m)</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/lts-1.png" width="672" /></p>
</div>
<p>One can show that LTS has an efficiency of approximately <span class="math inline">\(0.08\)</span>
compared to ordinary least squares regression.</p>
<div class="mysummary">
<p><strong>Summary</strong></p>
<ul>
<li>In this section we have informally discussed the efficiency of different estimators.</li>
<li>We have introduced the Least Median of Squares estimator.</li>
<li>We have introduced the Least Trimmed Squares estimator.</li>
</ul>
</div>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="P05.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Sx1-matrices.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH3714.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
