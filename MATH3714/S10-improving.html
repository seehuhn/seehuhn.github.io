<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 10 Improving the Model Fit | MATH3714 Linear Regression and Robustness</title>
  <meta name="description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2023/24" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 10 Improving the Model Fit | MATH3714 Linear Regression and Robustness" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2023/24" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 10 Improving the Model Fit | MATH3714 Linear Regression and Robustness" />
  
  <meta name="twitter:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2023/24" />
  

<meta name="author" content="Jochen Voss" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="S09-plots.html"/>
<link rel="next" href="S11-diagnostics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P96L0SF56N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-P96L0SF56N');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="jvstyle.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">MATH3714 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html"><i class="fa fa-check"></i>About MATH3714</a>
<ul>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#notes"><i class="fa fa-check"></i>Notes and videos</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#lectures"><i class="fa fa-check"></i>Lectures</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#workshops"><i class="fa fa-check"></i>Workshops and Problem Sheets</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#assessments"><i class="fa fa-check"></i>Assessments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="S01-simple.html"><a href="S01-simple.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="S01-simple.html"><a href="S01-simple.html#residual-sum-of-squares"><i class="fa fa-check"></i><b>1.1</b> Residual Sum of Squares</a></li>
<li class="chapter" data-level="1.2" data-path="S01-simple.html"><a href="S01-simple.html#linear-regression-as-a-parameter-estimation-problem"><i class="fa fa-check"></i><b>1.2</b> Linear Regression as a Parameter Estimation Problem</a></li>
<li class="chapter" data-level="1.3" data-path="S01-simple.html"><a href="S01-simple.html#sec:simple-mat"><i class="fa fa-check"></i><b>1.3</b> Matrix Notation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="S02-multiple.html"><a href="S02-multiple.html"><i class="fa fa-check"></i><b>2</b> Least Squares Estimates</a>
<ul>
<li class="chapter" data-level="2.1" data-path="S02-multiple.html"><a href="S02-multiple.html#data-and-models"><i class="fa fa-check"></i><b>2.1</b> Data and Models</a></li>
<li class="chapter" data-level="2.2" data-path="S02-multiple.html"><a href="S02-multiple.html#the-normal-equations"><i class="fa fa-check"></i><b>2.2</b> The Normal Equations</a></li>
<li class="chapter" data-level="2.3" data-path="S02-multiple.html"><a href="S02-multiple.html#fitted-values"><i class="fa fa-check"></i><b>2.3</b> Fitted Values</a></li>
<li class="chapter" data-level="2.4" data-path="S02-multiple.html"><a href="S02-multiple.html#models-without-intercept"><i class="fa fa-check"></i><b>2.4</b> Models Without Intercept</a></li>
<li class="chapter" data-level="2.5" data-path="S02-multiple.html"><a href="S02-multiple.html#example"><i class="fa fa-check"></i><b>2.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html"><i class="fa fa-check"></i>Interlude: Linear Regression in R</a>
<ul>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-fitting"><i class="fa fa-check"></i>Fitting a Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-model"><i class="fa fa-check"></i>Understanding the Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-predict"><i class="fa fa-check"></i>Making Predictions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="S03-cov.html"><a href="S03-cov.html"><i class="fa fa-check"></i><b>3</b> Random Vectors and Covariance</a>
<ul>
<li class="chapter" data-level="3.1" data-path="S03-cov.html"><a href="S03-cov.html#expectation"><i class="fa fa-check"></i><b>3.1</b> Expectation</a></li>
<li class="chapter" data-level="3.2" data-path="S03-cov.html"><a href="S03-cov.html#sec:covariance"><i class="fa fa-check"></i><b>3.2</b> Covariance Matrix</a></li>
<li class="chapter" data-level="3.3" data-path="S03-cov.html"><a href="S03-cov.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>3.3</b> The Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P01.html"><a href="P01.html"><i class="fa fa-check"></i>Problem Sheet 1</a></li>
<li class="chapter" data-level="4" data-path="S04-model.html"><a href="S04-model.html"><i class="fa fa-check"></i><b>4</b> Properties of the Least Squares Estimate</a>
<ul>
<li class="chapter" data-level="4.1" data-path="S04-model.html"><a href="S04-model.html#mean-and-covariance"><i class="fa fa-check"></i><b>4.1</b> Mean and Covariance</a></li>
<li class="chapter" data-level="4.2" data-path="S04-model.html"><a href="S04-model.html#hat-matrix"><i class="fa fa-check"></i><b>4.2</b> Properties of the Hat Matrix</a></li>
<li class="chapter" data-level="4.3" data-path="S04-model.html"><a href="S04-model.html#Cochran"><i class="fa fa-check"></i><b>4.3</b> Cochran’s theorem</a></li>
<li class="chapter" data-level="4.4" data-path="S04-model.html"><a href="S04-model.html#var-est-bias"><i class="fa fa-check"></i><b>4.4</b> Estimating the Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="S05-single.html"><a href="S05-single.html"><i class="fa fa-check"></i><b>5</b> Uncertainty for Individual Regression Coefficients</a>
<ul>
<li class="chapter" data-level="5.1" data-path="S05-single.html"><a href="S05-single.html#measuring-the-estimation-error"><i class="fa fa-check"></i><b>5.1</b> Measuring the Estimation Error</a></li>
<li class="chapter" data-level="5.2" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.3" data-path="S05-single.html"><a href="S05-single.html#hypthesis-tests"><i class="fa fa-check"></i><b>5.3</b> Hypthesis Tests</a></li>
<li class="chapter" data-level="5.4" data-path="S05-single.html"><a href="S05-single.html#r-experiments"><i class="fa fa-check"></i><b>5.4</b> R Experiments</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="S05-single.html"><a href="S05-single.html#fitting-the-model"><i class="fa fa-check"></i><b>5.4.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.4.2" data-path="S05-single.html"><a href="S05-single.html#estimating-the-variance-of-the-error"><i class="fa fa-check"></i><b>5.4.2</b> Estimating the Variance of the Error</a></li>
<li class="chapter" data-level="5.4.3" data-path="S05-single.html"><a href="S05-single.html#estimating-the-standard-errors"><i class="fa fa-check"></i><b>5.4.3</b> Estimating the Standard Errors</a></li>
<li class="chapter" data-level="5.4.4" data-path="S05-single.html"><a href="S05-single.html#hypothesis-tests"><i class="fa fa-check"></i><b>5.4.4</b> Hypothesis tests</a></li>
<li class="chapter" data-level="5.4.5" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals-1"><i class="fa fa-check"></i><b>5.4.5</b> Confidence Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html"><i class="fa fa-check"></i><b>6</b> Estimating Coefficients Simultaneously</a>
<ul>
<li class="chapter" data-level="6.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-dist"><i class="fa fa-check"></i><b>6.1</b> Linear Combinations of Coefficients</a></li>
<li class="chapter" data-level="6.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-CI"><i class="fa fa-check"></i><b>6.2</b> Confidence Regions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#result"><i class="fa fa-check"></i><b>6.2.1</b> Result</a></li>
<li class="chapter" data-level="6.2.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#numerical-experiments"><i class="fa fa-check"></i><b>6.2.2</b> Numerical Experiments</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-test"><i class="fa fa-check"></i><b>6.3</b> Hypothesis Tests</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html"><i class="fa fa-check"></i>Interlude: Loading Data into R</a>
<ul>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-csv-files"><i class="fa fa-check"></i>Importing CSV Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-microsoft-excel-files"><i class="fa fa-check"></i>Importing Microsoft Excel Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#checking-the-imported-data"><i class="fa fa-check"></i>Checking the Imported Data</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#common-problems"><i class="fa fa-check"></i>Common Problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="S07-examples.html"><a href="S07-examples.html"><i class="fa fa-check"></i><b>7</b> Examples</a>
<ul>
<li class="chapter" data-level="7.1" data-path="S07-examples.html"><a href="S07-examples.html#simple-confidence-interval"><i class="fa fa-check"></i><b>7.1</b> Simple Confidence Interval</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles"><i class="fa fa-check"></i><b>7.1.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.1.2" data-path="S07-examples.html"><a href="S07-examples.html#from-the-lm-output"><i class="fa fa-check"></i><b>7.1.2</b> From the <code>lm()</code> Output</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="S07-examples.html"><a href="S07-examples.html#confidence-intervals-for-the-mean"><i class="fa fa-check"></i><b>7.2</b> Confidence Intervals for the Mean</a></li>
<li class="chapter" data-level="7.3" data-path="S07-examples.html"><a href="S07-examples.html#testing-a-single-coefficient"><i class="fa fa-check"></i><b>7.3</b> Testing a Single Coefficient</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-1"><i class="fa fa-check"></i><b>7.3.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.3.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-i"><i class="fa fa-check"></i><b>7.3.2</b> Using the <code>lm()</code> Output, I</a></li>
<li class="chapter" data-level="7.3.3" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-ii"><i class="fa fa-check"></i><b>7.3.3</b> Using the <code>lm()</code> Output, II</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="S07-examples.html"><a href="S07-examples.html#testing-multiple-coefficents"><i class="fa fa-check"></i><b>7.4</b> Testing Multiple Coefficents</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-2"><i class="fa fa-check"></i><b>7.4.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.4.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output"><i class="fa fa-check"></i><b>7.4.2</b> Using the <code>lm()</code> output</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="P02.html"><a href="P02.html"><i class="fa fa-check"></i>Problem Sheet 2</a></li>
<li class="chapter" data-level="8" data-path="S08-influence.html"><a href="S08-influence.html"><i class="fa fa-check"></i><b>8</b> The Influence of Observations</a>
<ul>
<li class="chapter" data-level="8.1" data-path="S08-influence.html"><a href="S08-influence.html#deleting"><i class="fa fa-check"></i><b>8.1</b> Deleting Observations</a></li>
<li class="chapter" data-level="8.2" data-path="S08-influence.html"><a href="S08-influence.html#influence"><i class="fa fa-check"></i><b>8.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="S09-plots.html"><a href="S09-plots.html"><i class="fa fa-check"></i><b>9</b> Diagnostic Plots</a></li>
<li class="chapter" data-level="10" data-path="S10-improving.html"><a href="S10-improving.html"><i class="fa fa-check"></i><b>10</b> Improving the Model Fit</a>
<ul>
<li class="chapter" data-level="10.1" data-path="S10-improving.html"><a href="S10-improving.html#linearising-the-mean"><i class="fa fa-check"></i><b>10.1</b> Linearising the Mean</a></li>
<li class="chapter" data-level="10.2" data-path="S10-improving.html"><a href="S10-improving.html#stabilising-the-variance"><i class="fa fa-check"></i><b>10.2</b> Stabilising the Variance</a></li>
<li class="chapter" data-level="10.3" data-path="S10-improving.html"><a href="S10-improving.html#power-transform"><i class="fa fa-check"></i><b>10.3</b> The Power Transform</a></li>
<li class="chapter" data-level="10.4" data-path="S10-improving.html"><a href="S10-improving.html#candidates-models"><i class="fa fa-check"></i><b>10.4</b> Candidates Models</a></li>
<li class="chapter" data-level="10.5" data-path="S10-improving.html"><a href="S10-improving.html#misspecified-models"><i class="fa fa-check"></i><b>10.5</b> Misspecified Models</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="S10-improving.html"><a href="S10-improving.html#missing-variables"><i class="fa fa-check"></i><b>10.5.1</b> Missing Variables</a></li>
<li class="chapter" data-level="10.5.2" data-path="S10-improving.html"><a href="S10-improving.html#unnecessary-variables"><i class="fa fa-check"></i><b>10.5.2</b> Unnecessary Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html"><i class="fa fa-check"></i><b>11</b> Measures for Model Fit</a>
<ul>
<li class="chapter" data-level="11.1" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html#R-squared"><i class="fa fa-check"></i><b>11.1</b> The Coefficient of Multiple Determination</a></li>
<li class="chapter" data-level="11.2" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html#error-var"><i class="fa fa-check"></i><b>11.2</b> Error Variance</a></li>
<li class="chapter" data-level="11.3" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html#PRESS"><i class="fa fa-check"></i><b>11.3</b> Prediction Error Sum of Squares</a></li>
<li class="chapter" data-level="11.4" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html#AIC"><i class="fa fa-check"></i><b>11.4</b> Akaike’s Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P03.html"><a href="P03.html"><i class="fa fa-check"></i>Problem Sheet 3</a></li>
<li class="chapter" data-level="" data-path="I03-lm-output.html"><a href="I03-lm-output.html"><i class="fa fa-check"></i>Interlude: Understanding the <code>lm()</code> Output</a></li>
<li class="chapter" data-level="12" data-path="S12-automatic.html"><a href="S12-automatic.html"><i class="fa fa-check"></i><b>12</b> Automatic Model Selection</a>
<ul>
<li class="chapter" data-level="12.1" data-path="S12-automatic.html"><a href="S12-automatic.html#exhaustive-search"><i class="fa fa-check"></i><b>12.1</b> Exhaustive Search</a></li>
<li class="chapter" data-level="12.2" data-path="S12-automatic.html"><a href="S12-automatic.html#search-algorithm"><i class="fa fa-check"></i><b>12.2</b> Search Algorithm</a></li>
<li class="chapter" data-level="12.3" data-path="S12-automatic.html"><a href="S12-automatic.html#other-methods"><i class="fa fa-check"></i><b>12.3</b> Other Methods</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="S12-automatic.html"><a href="S12-automatic.html#stepwise-forward-selection"><i class="fa fa-check"></i><b>12.3.1</b> Stepwise Forward Selection</a></li>
<li class="chapter" data-level="12.3.2" data-path="S12-automatic.html"><a href="S12-automatic.html#stepwise-backward-selection"><i class="fa fa-check"></i><b>12.3.2</b> Stepwise Backward Selection</a></li>
<li class="chapter" data-level="12.3.3" data-path="S12-automatic.html"><a href="S12-automatic.html#hybrid-methods"><i class="fa fa-check"></i><b>12.3.3</b> Hybrid Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="S13-factors.html"><a href="S13-factors.html"><i class="fa fa-check"></i><b>13</b> Factors</a>
<ul>
<li class="chapter" data-level="13.1" data-path="S13-factors.html"><a href="S13-factors.html#indicator-variables"><i class="fa fa-check"></i><b>13.1</b> Indicator Variables</a></li>
<li class="chapter" data-level="13.2" data-path="S13-factors.html"><a href="S13-factors.html#interactions"><i class="fa fa-check"></i><b>13.2</b> Interactions</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="S14-examples.html"><a href="S14-examples.html"><i class="fa fa-check"></i><b>14</b> Examples</a>
<ul>
<li class="chapter" data-level="14.1" data-path="S14-examples.html"><a href="S14-examples.html#interactions-example"><i class="fa fa-check"></i><b>14.1</b> Use of Interaction Terms in Modelling</a></li>
<li class="chapter" data-level="14.2" data-path="S14-examples.html"><a href="S14-examples.html#codings"><i class="fa fa-check"></i><b>14.2</b> Alternative Factor Codings</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="S15-multicoll.html"><a href="S15-multicoll.html"><i class="fa fa-check"></i><b>15</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="15.1" data-path="S15-multicoll.html"><a href="S15-multicoll.html#consequences-of-multicollinearity"><i class="fa fa-check"></i><b>15.1</b> Consequences of Multicollinearity</a></li>
<li class="chapter" data-level="15.2" data-path="S15-multicoll.html"><a href="S15-multicoll.html#detecting-multicollinearity"><i class="fa fa-check"></i><b>15.2</b> Detecting Multicollinearity</a></li>
<li class="chapter" data-level="15.3" data-path="S15-multicoll.html"><a href="S15-multicoll.html#mitigations"><i class="fa fa-check"></i><b>15.3</b> Mitigations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P04.html"><a href="P04.html"><i class="fa fa-check"></i>Problem Sheet 4</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html"><i class="fa fa-check"></i>Practical</a>
<ul>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#dataset"><i class="fa fa-check"></i>Dataset</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#tasks"><i class="fa fa-check"></i>Tasks</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="S16-ridge.html"><a href="S16-ridge.html"><i class="fa fa-check"></i><b>16</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="16.1" data-path="S16-ridge.html"><a href="S16-ridge.html#definition-the-estimator"><i class="fa fa-check"></i><b>16.1</b> Definition the Estimator</a></li>
<li class="chapter" data-level="16.2" data-path="S16-ridge.html"><a href="S16-ridge.html#properties-of-the-estimate"><i class="fa fa-check"></i><b>16.2</b> Properties of the Estimate</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="S16-ridge.html"><a href="S16-ridge.html#bias"><i class="fa fa-check"></i><b>16.2.1</b> Bias</a></li>
<li class="chapter" data-level="16.2.2" data-path="S16-ridge.html"><a href="S16-ridge.html#variance"><i class="fa fa-check"></i><b>16.2.2</b> Variance</a></li>
<li class="chapter" data-level="16.2.3" data-path="S16-ridge.html"><a href="S16-ridge.html#mean-squared-error"><i class="fa fa-check"></i><b>16.2.3</b> Mean Squared Error</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="S16-ridge.html"><a href="S16-ridge.html#standardisation"><i class="fa fa-check"></i><b>16.3</b> Standardisation</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="S17-robust.html"><a href="S17-robust.html"><i class="fa fa-check"></i><b>17</b> Robust Regression</a>
<ul>
<li class="chapter" data-level="17.1" data-path="S17-robust.html"><a href="S17-robust.html#outliers"><i class="fa fa-check"></i><b>17.1</b> Outliers</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="S17-robust.html"><a href="S17-robust.html#leverage"><i class="fa fa-check"></i><b>17.1.1</b> Leverage</a></li>
<li class="chapter" data-level="17.1.2" data-path="S17-robust.html"><a href="S17-robust.html#studentised-residuals"><i class="fa fa-check"></i><b>17.1.2</b> Studentised Residuals</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="S17-robust.html"><a href="S17-robust.html#breakdown-points"><i class="fa fa-check"></i><b>17.2</b> Breakdown Points</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="S18-m-est.html"><a href="S18-m-est.html"><i class="fa fa-check"></i><b>18</b> M-Estimators</a>
<ul>
<li class="chapter" data-level="18.1" data-path="S18-m-est.html"><a href="S18-m-est.html#definition"><i class="fa fa-check"></i><b>18.1</b> Definition</a></li>
<li class="chapter" data-level="18.2" data-path="S18-m-est.html"><a href="S18-m-est.html#iterative-methods"><i class="fa fa-check"></i><b>18.2</b> Iterative Methods</a></li>
<li class="chapter" data-level="18.3" data-path="S18-m-est.html"><a href="S18-m-est.html#objective-functions"><i class="fa fa-check"></i><b>18.3</b> Objective Functions</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="S18-m-est.html"><a href="S18-m-est.html#least-squares-method"><i class="fa fa-check"></i><b>18.3.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="18.3.2" data-path="S18-m-est.html"><a href="S18-m-est.html#least-absolute-values"><i class="fa fa-check"></i><b>18.3.2</b> Least Absolute Values</a></li>
<li class="chapter" data-level="18.3.3" data-path="S18-m-est.html"><a href="S18-m-est.html#Huber"><i class="fa fa-check"></i><b>18.3.3</b> Huber’s <span class="math inline">\(t\)</span>-function</a></li>
<li class="chapter" data-level="18.3.4" data-path="S18-m-est.html"><a href="S18-m-est.html#hampels-method"><i class="fa fa-check"></i><b>18.3.4</b> Hampel’s Method</a></li>
<li class="chapter" data-level="18.3.5" data-path="S18-m-est.html"><a href="S18-m-est.html#tukeys-bisquare-method"><i class="fa fa-check"></i><b>18.3.5</b> Tukey’s Bisquare Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="P05.html"><a href="P05.html"><i class="fa fa-check"></i>Problem Sheet 5</a></li>
<li class="chapter" data-level="19" data-path="S19-efficiency.html"><a href="S19-efficiency.html"><i class="fa fa-check"></i><b>19</b> Efficiency of Robust Estimators</a>
<ul>
<li class="chapter" data-level="19.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#efficiency"><i class="fa fa-check"></i><b>19.1</b> Efficiency</a></li>
<li class="chapter" data-level="19.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#robust-estimators"><i class="fa fa-check"></i><b>19.2</b> Robust estimators</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#least-median-of-squares"><i class="fa fa-check"></i><b>19.2.1</b> Least Median of Squares</a></li>
<li class="chapter" data-level="19.2.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#least-trimmed-squares"><i class="fa fa-check"></i><b>19.2.2</b> Least Trimmed Squares</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html"><i class="fa fa-check"></i><b>A</b> Linear Algebra Reminders</a>
<ul>
<li class="chapter" data-level="A.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#vectors"><i class="fa fa-check"></i><b>A.1</b> Vectors</a></li>
<li class="chapter" data-level="A.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-rules"><i class="fa fa-check"></i><b>A.2</b> Matrices</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#transpose"><i class="fa fa-check"></i><b>A.2.1</b> Transpose</a></li>
<li class="chapter" data-level="A.2.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-vector-product"><i class="fa fa-check"></i><b>A.2.2</b> Matrix-vector Product</a></li>
<li class="chapter" data-level="A.2.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-matrix-product"><i class="fa fa-check"></i><b>A.2.3</b> Matrix-matrix Product</a></li>
<li class="chapter" data-level="A.2.4" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#rank"><i class="fa fa-check"></i><b>A.2.4</b> Rank</a></li>
<li class="chapter" data-level="A.2.5" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#trace"><i class="fa fa-check"></i><b>A.2.5</b> Trace</a></li>
<li class="chapter" data-level="A.2.6" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.2.6</b> Matrix Inverse</a></li>
<li class="chapter" data-level="A.2.7" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.2.7</b> Orthogonal Matrices</a></li>
<li class="chapter" data-level="A.2.8" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#positive-definite"><i class="fa fa-check"></i><b>A.2.8</b> Positive Definite Matrices</a></li>
<li class="chapter" data-level="A.2.9" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#idempotent"><i class="fa fa-check"></i><b>A.2.9</b> Idempotent Matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#eigenvalues"><i class="fa fa-check"></i><b>A.3</b> Eigenvalues</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="Sx2-probability.html"><a href="Sx2-probability.html"><i class="fa fa-check"></i><b>B</b> Probability Reminders</a>
<ul>
<li class="chapter" data-level="B.1" data-path="Sx2-probability.html"><a href="Sx2-probability.html#independence"><i class="fa fa-check"></i><b>B.1</b> Independence</a></li>
<li class="chapter" data-level="B.2" data-path="Sx2-probability.html"><a href="Sx2-probability.html#chi-square"><i class="fa fa-check"></i><b>B.2</b> The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="B.3" data-path="Sx2-probability.html"><a href="Sx2-probability.html#t"><i class="fa fa-check"></i><b>B.3</b> The t-distribution</a></li>
</ul></li>
<li class="divider"></li>
<li class="chapter"><span><b>THE END</b></span></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3714 Linear Regression and Robustness</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="S10-improving" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Section 10</span> Improving the Model Fit<a href="S10-improving.html#S10-improving" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!--
TODO:
* This section is too long!
* add more videos
* I believe the variance of \hat\beta_j always increases when
inputs are added.  Is this true?  Can we prove this?

\begin{equation*}
  (X \mid \tilde X)^\top (X \mid \tilde X)
  = \begin{pmatrix}
    X^\top X & X^\top \tilde X \\
    \tilde X^\top X & \tilde X^\top \tilde X
  \end{pmatrix}
\end{equation*}

\begin{equation*}
  \Bigl( (X \mid \tilde X)^\top (X \mid \tilde X) \Bigr)^{-1}_{1:q,1:q}
  = \Bigl( X^\top X - X^\top \tilde X (\tilde X^\top \tilde X)^{-1} \tilde X^\top X \Bigr)^{-1}
\end{equation*}
-->
<p>When we developed the theory for linear models, we used the following
assumptions:</p>
<ul>
<li><p>Linear relationship between inputs and outputs: <span class="math inline">\(y \approx x^\top \beta\)</span></p></li>
<li><p>Independence of errors: the <span class="math inline">\(\varepsilon_i = y_i - (X\beta)_i\)</span> are independent
of each other.</p></li>
<li><p>Normally distributed errors: <span class="math inline">\(\varepsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span> for all <span class="math inline">\(i\)</span>.
In particular, the variance of the <span class="math inline">\(\varepsilon_i\)</span> does not depend on <span class="math inline">\(i\)</span>.</p></li>
</ul>
<p>The <a href="S09-plots.html#S09-plots">diagnostic plots</a> introduced in section<br />
<a href="S09-plots.html#S09-plots">9</a> can often reveal if the data do not fit these modelling
assumptions. In cases where the assumptions are violated, sometimes a linear
model can still be used if the data is transformed first.</p>
<div id="linearising-the-mean" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Linearising the Mean<a href="S10-improving.html#linearising-the-mean" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/H2U9y-8cX1Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>If the model mean is a non-linear function of the inputs, we
can sometimes transform the variables to achieve a linear relationship.
We list some examples of non-linear models which can be transformed
to linear models:</p>
<table style="width:100%;">
<colgroup>
<col width="51%" />
<col width="25%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">nonlinear model</th>
<th align="left">transformation</th>
<th align="left">linear Model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(y \approx \beta_0 x^{\beta_1}\)</span></td>
<td align="left"><span class="math inline">\(x&#39;=\log(x)\)</span>, <span class="math inline">\(y&#39;=\log(y)\)</span></td>
<td align="left"><span class="math inline">\(y&#39; \approx \log(\beta_0) + \beta_1 x&#39;\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(y \approx \beta_0 e^{\beta_1 x}\)</span></td>
<td align="left"><span class="math inline">\(y&#39;=\log y\)</span></td>
<td align="left"><span class="math inline">\(y&#39; \approx \log \beta_0 +\beta_1 x\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(y \approx \beta_0+\beta_1\log x\)</span></td>
<td align="left"><span class="math inline">\(x&#39;=\log x\)</span></td>
<td align="left"><span class="math inline">\(y \approx \beta_0+\beta_1 x&#39;\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(y \approx \frac{x}{\beta_0 x-\beta_1}\)</span></td>
<td align="left"><span class="math inline">\(x&#39;=1/x\)</span>, <span class="math inline">\(y&#39;=1/y\)</span></td>
<td align="left"><span class="math inline">\(y&#39; \approx \beta_0-\beta_1 x&#39;\)</span></td>
</tr>
</tbody>
</table>
<p>In all such cases we also would need to check the residuals of the transformed
models, to see whether linear regression can be used for the transformed model.</p>
</div>
<div id="stabilising-the-variance" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Stabilising the Variance<a href="S10-improving.html#stabilising-the-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/pH9AK_unQzQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>The assumption of constant variance is a basic requirement of regression
analysis. A common reason for the violation of this assumption is for the
response variable <span class="math inline">\(y\)</span> to follow a distribution in which the variance depends
on <span class="math inline">\(y\)</span> or <span class="math inline">\(\mathbb{E}(y)\)</span> and thus on <span class="math inline">\(x\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-34" class="example"><strong>Example 10.1  </strong></span>The error in our model
<span class="math display">\[\begin{equation*}
  Y
  = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \varepsilon
  = x^\top \beta + \varepsilon
\end{equation*}\]</span>
is sometimes called <strong>additive error</strong>, since it is added to the model
mean <span class="math inline">\(x^\top\beta\)</span>. Sometimes the error is instead given in percentages of the
quantity of interest. In these cases we speak of <strong>multiplicative error</strong>.
This can, for example, be modelled as
<span class="math display">\[\begin{equation*}
  Y
  = \bigl(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p\bigr) \exp(\varepsilon)
  = x^\top \beta \exp(\varepsilon).
\end{equation*}\]</span>
For <span class="math inline">\(\varepsilon= 0\)</span> we have <span class="math inline">\(\exp(0) = 1\)</span> and thus <span class="math inline">\(Y = x^\top \beta\)</span>. Similarly,
for small <span class="math inline">\(\varepsilon\)</span> we have <span class="math inline">\(Y \approx x^\top \beta\)</span>, but the variance is now
proportional to <span class="math inline">\((x^\top\beta)^2\)</span> instead of being constant. Also, since the
exponential is nonlinear we only have <span class="math inline">\(\mathbb{E}(Y) \approx x^\top\beta\)</span> instead of
strict equality.</p>
</div>
<p>Some commonly-used variance stabilising transformations are:</p>
<table>
<colgroup>
<col width="60%" />
<col width="40%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">variance</th>
<th align="left">transformation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\sigma^2 = \text{constant}\)</span></td>
<td align="left">no transformation</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\sigma^2 \propto y\)</span></td>
<td align="left"><span class="math inline">\(y&#39; = \sqrt{y}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\sigma^2 \propto y^2\)</span></td>
<td align="left"><span class="math inline">\(y&#39; = \log y\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\sigma^2 \propto y^3\)</span></td>
<td align="left"><span class="math inline">\(y&#39; = \frac{1}{\sqrt{y}}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\sigma^2 \propto y^4\)</span></td>
<td align="left"><span class="math inline">\(y&#39; = \frac{1}{y}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\sigma^2 \propto y(1-y)\)</span> where <span class="math inline">\(y \in [0,1]\)</span></td>
<td align="left"><span class="math inline">\(y&#39; = \arcsin(\sqrt{y})\)</span></td>
</tr>
</tbody>
</table>
<p>Of course we do not have accurate knowledge of the relationship, but it can be
diagnosed from the residual plots and transformations can be selected by
experimenting with different choices. Any of these transformations will also
affect the mean and we need to check the model fit for the transformed data, to
see whether the transformed data can still reasonably be described by a linear
model.</p>
<div class="example">
<p><span id="exm:unlabeled-div-35" class="example"><strong>Example 10.2  </strong></span>The last transformation in the table above corresponds to the case of binomial
sampling: If <span class="math inline">\(x = p\)</span> and <span class="math inline">\(Y \sim B(n, p) / n\)</span> then we have <span class="math inline">\(\mathbb{E}(Y) = n p / n = x\)</span> and a linear model may be appropriate. But we also have <span class="math inline">\(\mathop{\mathrm{Var}}(Y) = p (1 - p) / n \propto \mathbb{E}(Y) \bigl( 1- \mathbb{E}(Y) \bigr)\)</span>, so the assumption of
constant variance is violated.</p>
<p>We try to apply the transformation suggested in the table. The
function <span class="math inline">\(\arcsin\)</span> is the inverse of the sine function. In R
this function is available as <code>asin()</code>. To get some intuition about
this transformation, we plot the function <span class="math inline">\(y \mapsto \arcsin(\sqrt{y})\)</span>:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="S10-improving.html#cb144-1" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">l=</span><span class="dv">101</span>)</span>
<span id="cb144-2"><a href="S10-improving.html#cb144-2" tabindex="-1"></a><span class="fu">plot</span>(y, <span class="fu">asin</span>(<span class="fu">sqrt</span>(y)), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>,</span>
<span id="cb144-3"><a href="S10-improving.html#cb144-3" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="fu">expression</span>(y <span class="sc">*</span> minute))</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/arcsin-transform-1.png" width="672" /></p>
<p>We can see that the transformation is approximately linear for most
of the interval, but has a steeper slope near the edges. The effect of
this is to increase the size of fluctations for small and large
<span class="math inline">\(y\)</span>-values. We now consider residual plots for the original and transformed
data, for a simulated dataset:</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="S10-improving.html#cb145-1" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb145-2"><a href="S10-improving.html#cb145-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb145-3"><a href="S10-improving.html#cb145-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n, <span class="dv">100</span>, x) <span class="sc">/</span> <span class="dv">100</span></span>
<span id="cb145-4"><a href="S10-improving.html#cb145-4" tabindex="-1"></a></span>
<span id="cb145-5"><a href="S10-improving.html#cb145-5" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb145-6"><a href="S10-improving.html#cb145-6" tabindex="-1"></a></span>
<span id="cb145-7"><a href="S10-improving.html#cb145-7" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb145-8"><a href="S10-improving.html#cb145-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">fitted</span>(m1), <span class="fu">resid</span>(m1))</span>
<span id="cb145-9"><a href="S10-improving.html#cb145-9" tabindex="-1"></a></span>
<span id="cb145-10"><a href="S10-improving.html#cb145-10" tabindex="-1"></a>y.prime <span class="ot">&lt;-</span> <span class="fu">asin</span>(<span class="fu">sqrt</span>(y))</span>
<span id="cb145-11"><a href="S10-improving.html#cb145-11" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y.prime <span class="sc">~</span> x)</span>
<span id="cb145-12"><a href="S10-improving.html#cb145-12" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">fitted</span>(m2), <span class="fu">resid</span>(m2))</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/binomial-sampling-1.png" width="672" /></p>
<p>The plot shows that the variance has indeed improved, while linearity
has suffered (an S-shaped curve is now visible). Neither model is perfect and
whether the transformation
is beneficial or not depends on the particular circumstances.</p>
</div>
<div class="example">
<p><span id="exm:electric" class="example"><strong>Example 10.3  </strong></span>An electric utility company is interested in developing a model relating the
peak hour demand (<span class="math inline">\(y\)</span>, measured in kW) to total energy usage during the month
(<span class="math inline">\(x\)</span>, in kWh). A scatter plot of the data is shown below:</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="S10-improving.html#cb146-1" tabindex="-1"></a><span class="co"># data at https://www1.maths.leeds.ac.uk/~voss/2022/MATH3714/electric.dat</span></span>
<span id="cb146-2"><a href="S10-improving.html#cb146-2" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;data/electric.dat&quot;</span>, <span class="at">header=</span><span class="cn">FALSE</span>)</span>
<span id="cb146-3"><a href="S10-improving.html#cb146-3" tabindex="-1"></a><span class="fu">plot</span>(d<span class="sc">$</span>V1, d<span class="sc">$</span>V2,</span>
<span id="cb146-4"><a href="S10-improving.html#cb146-4" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;energy usage [kWh]&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;energy demand [kW]&quot;</span>)</span>
<span id="cb146-5"><a href="S10-improving.html#cb146-5" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(V2 <span class="sc">~</span> ., <span class="at">data =</span> d)</span>
<span id="cb146-6"><a href="S10-improving.html#cb146-6" tabindex="-1"></a><span class="fu">abline</span>(m1)</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/electric1-1.png" width="672" /></p>
<p>A linear relationship looks plausible, but we can see that the spread
of points around the regression line widens as energy usage increases.
this is more clearly visible in a residual plot:</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="S10-improving.html#cb147-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">fitted</span>(m1), <span class="fu">resid</span>(m1),</span>
<span id="cb147-2"><a href="S10-improving.html#cb147-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;fitted values&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;residuals&quot;</span>)</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/electric2-1.png" width="672" /></p>
<p>We try a transformation of the data to stabilise the variance.
From the “wedge” shape on the left hand side of the plot, it is clear
that the variance <span class="math inline">\(\sigma^2\)</span> increases as <span class="math inline">\(y\)</span> increases.
Thus we can try transformations like <span class="math inline">\(y&#39; = \log(y)\)</span> or
<span class="math inline">\(y&#39; = \sqrt{y}\)</span>. Taking the logarithm for illustration, we get</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="S10-improving.html#cb148-1" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(V2) <span class="sc">~</span> ., <span class="at">data =</span> d)</span>
<span id="cb148-2"><a href="S10-improving.html#cb148-2" tabindex="-1"></a><span class="fu">plot</span>(d<span class="sc">$</span>V1, <span class="fu">log</span>(d<span class="sc">$</span>V2),</span>
<span id="cb148-3"><a href="S10-improving.html#cb148-3" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;energy usage [kWh]&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;log-transformed y&quot;</span>)</span>
<span id="cb148-4"><a href="S10-improving.html#cb148-4" tabindex="-1"></a><span class="fu">abline</span>(m2)</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/electric3-1.png" width="672" /></p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="S10-improving.html#cb149-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">fitted</span>(m2), <span class="fu">resid</span>(m2),</span>
<span id="cb149-2"><a href="S10-improving.html#cb149-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;fitted values&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;residuals for log-transform&quot;</span>)</span>
<span id="cb149-3"><a href="S10-improving.html#cb149-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/electric4-1.png" width="672" /></p>
<p>The spread of residuals now looks somewhat more reasonable.</p>
</div>
</div>
<div id="power-transform" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> The Power Transform<a href="S10-improving.html#power-transform" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/THWiYqTEIBU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>A family of transformations for the response variable <span class="math inline">\(y\)</span> is given by the
<strong>power transform</strong>. These transformations only apply to strictly positive data,
<em>i.e.</em> <span class="math inline">\(y &gt; 0\)</span>, and are defined by
<span class="math display">\[\begin{equation*}
  y^{(\lambda)}
  = \begin{cases}
      \frac{\displaystyle y^\lambda-1}{\displaystyle\lambda g^{\lambda-1}} &amp; \mbox{if $\lambda\ne 0$} \\
      g\log y &amp; \mbox{if $\lambda=0$}
    \end{cases}
\end{equation*}\]</span>
where <span class="math inline">\(g = (\prod_{i=1}^n y_i)^{1/n}\)</span> is the geometric mean.</p>
<p>The geometric mean is a constant (it does not depend on <span class="math inline">\(i\)</span>) and is not needed
for the power transform, but it is usually included to make values for
different <span class="math inline">\(\lambda\)</span> more comparable. For <span class="math inline">\(\lambda = 1\)</span> the transform is <span class="math inline">\(y&#39; = y - 1\)</span>. This is a simple shift which has no effect on the fitted model, it just
decreases the intercept by <span class="math inline">\(1\)</span> and leaves the residuals unchanged. Thus, this
case is that same as applying no transformation.</p>
<p>Using Taylor approximation on the numerator in the definition
of <span class="math inline">\(y^{(\lambda)}\)</span> we get
<span class="math display">\[\begin{equation*}
  y^\lambda
  = \exp\bigl( \log(y^\lambda) \bigr)
  = \exp\bigl( \lambda \log(y) \bigr)
  \approx 1 + \lambda\log(y) + O\Bigl(\bigl(\lambda\log(y)\bigr)^2\Bigr),
\end{equation*}\]</span>
where the <span class="math inline">\(O(\cdots)\)</span> stands for terms which are negligible as
<span class="math inline">\(\lambda\)</span> converges to <span class="math inline">\(0\)</span>. Thus the formula given for <span class="math inline">\(\lambda \neq 0\)</span>
converges to the case for <span class="math inline">\(\lambda = 0\)</span> as <span class="math inline">\(\lambda\)</span> converges to <span class="math inline">\(0\)</span>.
This makes the transformation continuous as a function of <span class="math inline">\(\lambda\)</span>.</p>
<p>A heuristic rule to find a range of <span class="math inline">\(\lambda\)</span> for which the power
transform is appropriate is based on how the the residual sum of
squares changes with <span class="math inline">\(\lambda\)</span>: Let
<span class="math display">\[\begin{equation*}
  r(\lambda)
  = \sum_{i=1}^n \bigl( y^{(\lambda)}_i - \hat y^{(\lambda)}_i \bigr)^2,
\end{equation*}\]</span>
where <span class="math inline">\(\hat y^{(\lambda)}_i\)</span> denotes the fitted value for the model
using the transformed data <span class="math inline">\(y^{(\lambda)}_i\)</span>. It is easy to plot
this function numerically. We want to choose <span class="math inline">\(\lambda\)</span> close to
where the function has its minimum. The heuristic rule is to consider
all values of <span class="math inline">\(\lambda\)</span> with
<span class="math display" id="eq:power-cutoff">\[\begin{equation}
  r(\lambda)
  \leq r(\lambda_\mathrm{min}) \Bigl( 1 + \frac{t_{n-p-1}(\alpha/2)^2}{n-p-1} \Bigr),
      \tag{10.1}
\end{equation}\]</span>
where <span class="math inline">\(\lambda_\mathrm{min}\)</span> is the value of <span class="math inline">\(\lambda\)</span> where the residual
sum of squares has its minimum and <span class="math inline">\(t_{n-p-1}(\alpha/2)\)</span> is the
<span class="math inline">\((1-\alpha/2)\)</span>-quantile of the <span class="math inline">\(t(n-p-1)\)</span>-distribution. One can show that this
is an approximate <span class="math inline">\((1-\alpha)\)</span> confidence interval for <span class="math inline">\(\lambda\)</span>. If we want to
interpret the model, it is usually better to select a “simple” <span class="math inline">\(\lambda\)</span>,
<em>e.g.</em> <span class="math inline">\(\lambda=0.5\)</span> rather than using the
“optimal” value <span class="math inline">\(\lambda_\mathrm{min}\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-36" class="example"><strong>Example 10.4  </strong></span>Continuing from example <a href="S10-improving.html#exm:electric">10.3</a> above, we can try a power
transform for the data. We start by plotting <span class="math inline">\(r(\lambda)\)</span> as a function
of <span class="math inline">\(\lambda\)</span>, together with the cutoff suggested by equation <a href="S10-improving.html#eq:power-cutoff">(10.1)</a>:</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="S10-improving.html#cb150-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> d<span class="sc">$</span>V1</span>
<span id="cb150-2"><a href="S10-improving.html#cb150-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> d<span class="sc">$</span>V2</span>
<span id="cb150-3"><a href="S10-improving.html#cb150-3" tabindex="-1"></a>gm <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="fu">mean</span>(<span class="fu">log</span>(y))) <span class="co"># more stable way to compute geometric mean</span></span>
<span id="cb150-4"><a href="S10-improving.html#cb150-4" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">101</span>)</span>
<span id="cb150-5"><a href="S10-improving.html#cb150-5" tabindex="-1"></a>rss <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(lambda))</span>
<span id="cb150-6"><a href="S10-improving.html#cb150-6" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(lambda)) {</span>
<span id="cb150-7"><a href="S10-improving.html#cb150-7" tabindex="-1"></a>    li <span class="ot">&lt;-</span> lambda[i]</span>
<span id="cb150-8"><a href="S10-improving.html#cb150-8" tabindex="-1"></a>    y.prime <span class="ot">&lt;-</span> (y<span class="sc">^</span>li <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> (li <span class="sc">*</span> gm<span class="sc">^</span>(li<span class="dv">-1</span>))</span>
<span id="cb150-9"><a href="S10-improving.html#cb150-9" tabindex="-1"></a>    mi <span class="ot">&lt;-</span> <span class="fu">lm</span>(y.prime <span class="sc">~</span> x)</span>
<span id="cb150-10"><a href="S10-improving.html#cb150-10" tabindex="-1"></a>    rss[i] <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">resid</span>(mi)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb150-11"><a href="S10-improving.html#cb150-11" tabindex="-1"></a>}</span>
<span id="cb150-12"><a href="S10-improving.html#cb150-12" tabindex="-1"></a><span class="fu">plot</span>(lambda, rss, <span class="at">type=</span><span class="st">&quot;l&quot;</span>)</span>
<span id="cb150-13"><a href="S10-improving.html#cb150-13" tabindex="-1"></a></span>
<span id="cb150-14"><a href="S10-improving.html#cb150-14" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(d) <span class="co"># 53</span></span>
<span id="cb150-15"><a href="S10-improving.html#cb150-15" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb150-16"><a href="S10-improving.html#cb150-16" tabindex="-1"></a>cutoff <span class="ot">&lt;-</span> <span class="fu">min</span>(rss) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, n <span class="sc">-</span> p <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> (n <span class="sc">-</span> p <span class="sc">-</span> <span class="dv">1</span>))</span>
<span id="cb150-17"><a href="S10-improving.html#cb150-17" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> cutoff)</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/electric5-1.png" width="672" /></p>
<p>This suggests the range of reasonable <span class="math inline">\(\lambda\)</span> values to be</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="S10-improving.html#cb151-1" tabindex="-1"></a><span class="fu">range</span>(lambda[<span class="fu">which</span>(rss <span class="sc">&lt;=</span> cutoff)])</span></code></pre></div>
<pre class="rOutput"><code>[1] 0.298 0.784</code></pre>
<p>The value <span class="math inline">\(\lambda = 1\)</span> (no transformation) is not contained in the interval,
suggesting that a transformation may be helpful.
Choosing a “simple” value inside the interval and close to the minimum,
we try <span class="math inline">\(\lambda = 0.5\)</span>. This leads to the following transformation:
<span class="math display">\[\begin{equation*}
  y&#39;
  = 2\sqrt{g} (\sqrt{y}-1).
\end{equation*}\]</span>
Up to the shift and scaling, this just takes the square root of the data.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="S10-improving.html#cb153-1" tabindex="-1"></a>y.prime <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(y)</span>
<span id="cb153-2"><a href="S10-improving.html#cb153-2" tabindex="-1"></a><span class="fu">plot</span>(x, y.prime,</span>
<span id="cb153-3"><a href="S10-improving.html#cb153-3" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;energy usage [kWh]&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;sqrt-transformed y&quot;</span>)</span>
<span id="cb153-4"><a href="S10-improving.html#cb153-4" tabindex="-1"></a>m3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y.prime <span class="sc">~</span> x)</span>
<span id="cb153-5"><a href="S10-improving.html#cb153-5" tabindex="-1"></a><span class="fu">abline</span>(m3)</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/electric6-1.png" width="672" /></p>
<p>To see whether the variance of the residuals has improved, we consider
a residual plot:</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="S10-improving.html#cb154-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">fitted</span>(m3), <span class="fu">resid</span>(m3),</span>
<span id="cb154-2"><a href="S10-improving.html#cb154-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;fitted values&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;residuals for sqrt-transform&quot;</span>)</span>
<span id="cb154-3"><a href="S10-improving.html#cb154-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/electric7-1.png" width="672" /></p>
<p>We see that the spread of the residuals has indeed improved, when compared to
fitting the original data.</p>
</div>
</div>
<div id="candidates-models" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> Candidates Models<a href="S10-improving.html#candidates-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here we assume that we are given data with input variables <span class="math inline">\(x_1, \ldots, x_p\)</span>.
The standard model which we have discussed so far adds an intercept to these
inputs, so that there are <span class="math inline">\(q = p+1\)</span> model parameters in total. We can modify
this model in various ways:</p>
<ul>
<li><p>When we discussed hypothesis tests, we allowed for the possibility that
some inputs do not influence the output. A reasonable approach would
be to fit a model with these inputs omitted. Since each input
may either be included in or excluded from the model, independent of
the others, and similarly the intercept may be included or not,
there are <span class="math inline">\(2^{p+1}\)</span> possible models to consider.</p></li>
<li><p>In the section about <a href="S10-improving.html#S10-improving">Improving the Model Fit</a> we have seen that
sometimes it is useful to transform input variables before using them
in the model. This can either happen individually, <em>e.g.</em> <span class="math inline">\(x_2&#39; = \log x_2\)</span>, or collectively, <em>e.g.</em> <span class="math inline">\(x^\ast = x_1 x_2\)</span>. The number of
models we may obtain in this way is unlimited.</p></li>
</ul>
<p>If we want to compare these candidate models in a systematic way,
we need to use efficient methods for comparison, since a often a large
number of models needs to be considered.</p>
</div>
<div id="misspecified-models" class="section level2 hasAnchor" number="10.5">
<h2><span class="header-section-number">10.5</span> Misspecified Models<a href="S10-improving.html#misspecified-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The model is said to be <strong>misspecified</strong>, if the data we are using
comes from a different model than the one we use to estimate
the coefficients.</p>
<div id="missing-variables" class="section level3 hasAnchor" number="10.5.1">
<h3><span class="header-section-number">10.5.1</span> Missing Variables<a href="S10-improving.html#missing-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Assume that the data comes from the model
<span class="math display">\[\begin{equation*}
  Y = \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \varepsilon,
\end{equation*}\]</span>
(we can include the intercept as <span class="math inline">\(x_1 = 1\)</span> if needed),
but we are only using the first <span class="math inline">\(q\)</span> variables <span class="math inline">\(x_1, \ldots, x_q\)</span>,
where <span class="math inline">\(q &lt; p\)</span>, to estimate the coefficients. Then the least squares
estimate of <span class="math inline">\(\beta = (\beta_1, \ldots, \beta_q)\)</span> is given by
<span class="math display">\[\begin{equation*}
  \hat\beta
  = (X^\top X)^{-1} X^\top y
\end{equation*}\]</span>
where <span class="math inline">\(X \in \mathbb{R}^{n\times q}\)</span>,
and our estimate for the error variance <span class="math inline">\(\sigma^2\)</span> is
<span class="math display">\[\begin{equation*}
  \hat\sigma^2
  = \frac{1}{n-q} y^\top (I - H) y,
\end{equation*}\]</span>
where <span class="math inline">\(H = X (X^\top X)^{-1} X^\top\)</span> is the hat matrix computed
without using <span class="math inline">\(x_{q+1}, \ldots, x_p\)</span>.</p>
<p>If we write <span class="math inline">\(\tilde X \in \mathbb{R}^{n\times (p-q)}\)</span> for the matrix with columns
<span class="math inline">\(x_{q+1}, \ldots, x_p\)</span> and <span class="math inline">\(\tilde\beta = (\beta_{q+1}, \ldots, \beta_p) \in \mathbb{R}^{p-q}\)</span>, then the full model can be written as
<span class="math display">\[\begin{equation*}
  Y
  = X \beta + \tilde X \tilde\beta + \varepsilon
\end{equation*}\]</span>
and we get
<span class="math display">\[\begin{align*}
  \hat\beta
  &amp;= (X^\top X)^{-1} X^\top Y \\
  &amp;= (X^\top X)^{-1} X^\top (X \beta + \tilde X \tilde\beta + \varepsilon) \\
  &amp;= (X^\top X)^{-1} X^\top X \beta + (X^\top X)^{-1} X^\top \tilde X \tilde\beta + (X^\top X)^{-1} X^\top \varepsilon\\
  &amp;= \beta + (X^\top X)^{-1} X^\top \tilde X \tilde\beta + (X^\top X)^{-1} X^\top \varepsilon.
\end{align*}\]</span>
Thus, we have
<span class="math display">\[\begin{equation*}
  \mathbb{E}(\hat\beta)
  = \beta + (X^\top X)^{-1} X^\top \tilde X \tilde\beta
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{bias}}(\hat\beta)
  = (X^\top X)^{-1} X^\top \tilde X \tilde\beta.
\end{equation*}\]</span>
This shows that now the estimate is biased in general. There are two
special cases when omitting variables does not introduce a bias:
The first is when all of the omitted coefficients equal zero, <em>i.e.</em> when
we have <span class="math inline">\(\tilde\beta = 0\)</span>. The second case is when the omitted inputs are
orthogonal to the included inputs, since then we have <span class="math inline">\(X^\top \tilde X = 0\)</span>.</p>
<p>Using the formula for <span class="math inline">\(\hat\beta\)</span> we find
<span class="math display">\[\begin{align*}
  \mathop{\mathrm{Cov}}( \hat\beta )
  &amp;= \mathop{\mathrm{Cov}}\Bigl( \beta + (X^\top X)^{-1} X^\top \tilde X \tilde\beta + (X^\top X)^{-1} X^\top \varepsilon\Bigr) \\
  &amp;= \mathop{\mathrm{Cov}}\Bigl( (X^\top X)^{-1} X^\top \varepsilon\Bigr) \\
  &amp;= \sigma^2 (X^\top X)^{-1} X^\top I X (X^\top X)^{-1} \\
  &amp;= \sigma^2 (X^\top X)^{-1}.
\end{align*}\]</span>
Thus, the variance of <span class="math inline">\(\hat\beta\)</span> is not affected by the omitted variables.
Similar to our derivation in the section <a href="S04-model.html#var-est-bias">Estimating the Error Variance</a>
one can show that
<span class="math display">\[\begin{equation*}
  \mathbb{E}\bigl( \hat\sigma^2 \bigr)
  = \sigma^2 + \frac{\tilde\beta^\top \tilde X^\top (I - H) \tilde X \tilde \beta}{n-q}.
\end{equation*}\]</span>
Thus, the estimate of the error variance is in general also biased.</p>
<p>This shows that our estimates can become biased if some of the inputs
are missing from our model. As in the previous section, it may happen
that the MSE of the parameter estimates in the reduced model
is smaller than the MSE in the correct model; this is the case if the
variance of the estimates decreases enough to compensate for the
introduced bias.</p>
</div>
<div id="unnecessary-variables" class="section level3 hasAnchor" number="10.5.2">
<h3><span class="header-section-number">10.5.2</span> Unnecessary Variables<a href="S10-improving.html#unnecessary-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If the data comes from a model with fewer inputs than the ones we are using,
the model is not “misspecified” in the strict sense. It is just the case that
some of the true <span class="math inline">\(\beta_j\)</span> exactly equal zero. In this case, our previous
results show that the least squares estimate is unbiased.</p>
<p>Including additional variables into a model still can cause problems:
One can show that each unnecessary variable added increases the variance of the
estimates <span class="math inline">\(\hat\beta_j\)</span>. Instead of giving a proof of this fact,
we illustrate the effect using a numerical example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-37" class="example"><strong>Example 10.5  </strong></span>Here we consider the <code>stackloss</code> dataset with an added column of
noise. We have seen that
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{Var}}( \hat\beta_j )
  = \sigma^2 \bigl( X^\top X \bigr)^{-1}_{jj}.
\end{equation*}\]</span>
While we don’t know the true value of <span class="math inline">\(\sigma^2\)</span>, we can determine
the relative change in variance when adding a column, since this process
does not change <span class="math inline">\(\sigma^2\)</span> and thus <span class="math inline">\(\sigma^2\)</span> will cancel when we
determine the relative change in variance.</p>
<p>We first compute the diagonal elements of <span class="math inline">\(\bigl( X^\top X \bigr)^{-1}\)</span>
for the original dataset:</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="S10-improving.html#cb155-1" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(stack.loss <span class="sc">~</span> ., <span class="at">data =</span> stackloss)</span>
<span id="cb155-2"><a href="S10-improving.html#cb155-2" tabindex="-1"></a>d1 <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X1) <span class="sc">%*%</span> X1))</span>
<span id="cb155-3"><a href="S10-improving.html#cb155-3" tabindex="-1"></a>d1</span></code></pre></div>
<pre class="rOutput"><code> (Intercept)     Air.Flow   Water.Temp   Acid.Conc. 
13.452726695  0.001728874  0.012875424  0.002322167 </code></pre>
<p>Now we add a column of noise and re-compute the values:</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="S10-improving.html#cb157-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20211115</span>)</span>
<span id="cb157-2"><a href="S10-improving.html#cb157-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X1)</span>
<span id="cb157-3"><a href="S10-improving.html#cb157-3" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">cbind</span>(X1, <span class="fu">rnorm</span>(n))</span>
<span id="cb157-4"><a href="S10-improving.html#cb157-4" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X2) <span class="sc">%*%</span> X2))</span>
<span id="cb157-5"><a href="S10-improving.html#cb157-5" tabindex="-1"></a>d2</span></code></pre></div>
<pre class="rOutput"><code> (Intercept)     Air.Flow   Water.Temp   Acid.Conc.              
14.397515774  0.001730570  0.015195467  0.002796487  0.064828744 </code></pre>
<p>Finally, we determine the change in variance in percent:</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="S10-improving.html#cb159-1" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span> <span class="sc">*</span> (d2[<span class="sc">-</span><span class="dv">5</span>] <span class="sc">-</span> d1) <span class="sc">/</span> d1, <span class="dv">3</span>)</span></code></pre></div>
<pre class="rOutput"><code>(Intercept)    Air.Flow  Water.Temp  Acid.Conc. 
      7.023       0.098      18.019      20.426 </code></pre>
<p>We see that the variances of the <span class="math inline">\(\hat\beta_j\)</span> increased by up to <span class="math inline">\(20\%\)</span>
when we added the unnecessary input.</p>
</div>
<p>The example illustrates that it is important to keep the model as small
as possible.</p>
<div class="mysummary">
<p><strong>Summary</strong></p>
<ul>
<li>We have seen different transformations which can be used
to transform a nonlinear model into a linear one.</li>
<li>We have discussed transformations which can stabilise the
variance of the errors in the model.</li>
<li>We have seen how orthogonal polynomials can be used to avoid
multicollinearity in polynomial regression.</li>
</ul>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="S09-plots.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="S11-diagnostics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH3714.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
