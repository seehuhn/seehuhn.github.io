<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 13 Model selection | MATH3714 Linear Regression and Robustness</title>
  <meta name="description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2022/23" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 13 Model selection | MATH3714 Linear Regression and Robustness" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2022/23" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 13 Model selection | MATH3714 Linear Regression and Robustness" />
  
  <meta name="twitter:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2022/23" />
  

<meta name="author" content="Jochen Voss" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="S12-ridge.html"/>
<link rel="next" href="S14-methods.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P96L0SF56N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-P96L0SF56N');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="jvstyle.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">MATH3714 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html"><i class="fa fa-check"></i>About MATH3714</a>
<ul>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#notes"><i class="fa fa-check"></i>Notes and videos</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#lectures"><i class="fa fa-check"></i>Lectures</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#workshops"><i class="fa fa-check"></i>Workshops and Problem Sheets</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#assessments"><i class="fa fa-check"></i>Assessments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="S01-simple.html"><a href="S01-simple.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="S01-simple.html"><a href="S01-simple.html#residual-sum-of-squares"><i class="fa fa-check"></i><b>1.1</b> Residual Sum of Squares</a></li>
<li class="chapter" data-level="1.2" data-path="S01-simple.html"><a href="S01-simple.html#linear-regression-as-a-parameter-estimation-problem"><i class="fa fa-check"></i><b>1.2</b> Linear Regression as a Parameter Estimation Problem</a></li>
<li class="chapter" data-level="1.3" data-path="S01-simple.html"><a href="S01-simple.html#sec:simple-mat"><i class="fa fa-check"></i><b>1.3</b> Matrix Notation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="S02-multiple.html"><a href="S02-multiple.html"><i class="fa fa-check"></i><b>2</b> Least Squares Estimates</a>
<ul>
<li class="chapter" data-level="2.1" data-path="S02-multiple.html"><a href="S02-multiple.html#data-and-models"><i class="fa fa-check"></i><b>2.1</b> Data and Models</a></li>
<li class="chapter" data-level="2.2" data-path="S02-multiple.html"><a href="S02-multiple.html#the-normal-equations"><i class="fa fa-check"></i><b>2.2</b> The Normal Equations</a></li>
<li class="chapter" data-level="2.3" data-path="S02-multiple.html"><a href="S02-multiple.html#fitted-values"><i class="fa fa-check"></i><b>2.3</b> Fitted Values</a></li>
<li class="chapter" data-level="2.4" data-path="S02-multiple.html"><a href="S02-multiple.html#example"><i class="fa fa-check"></i><b>2.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html"><i class="fa fa-check"></i>Interlude: Linear Regression in R</a>
<ul>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-fitting"><i class="fa fa-check"></i>Fitting a Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-model"><i class="fa fa-check"></i>Understanding the Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-predict"><i class="fa fa-check"></i>Making Predictions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P01.html"><a href="P01.html"><i class="fa fa-check"></i>Problem Sheet 1</a></li>
<li class="chapter" data-level="3" data-path="S03-cov.html"><a href="S03-cov.html"><i class="fa fa-check"></i><b>3</b> Random Vectors and Covariance</a>
<ul>
<li class="chapter" data-level="3.1" data-path="S03-cov.html"><a href="S03-cov.html#expectation"><i class="fa fa-check"></i><b>3.1</b> Expectation</a></li>
<li class="chapter" data-level="3.2" data-path="S03-cov.html"><a href="S03-cov.html#sec:covariance"><i class="fa fa-check"></i><b>3.2</b> Covariance Matrix</a></li>
<li class="chapter" data-level="3.3" data-path="S03-cov.html"><a href="S03-cov.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>3.3</b> The Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="S04-model.html"><a href="S04-model.html"><i class="fa fa-check"></i><b>4</b> Properties of the Least Squares Estimate</a>
<ul>
<li class="chapter" data-level="4.1" data-path="S04-model.html"><a href="S04-model.html#mean-and-covariance"><i class="fa fa-check"></i><b>4.1</b> Mean and Covariance</a></li>
<li class="chapter" data-level="4.2" data-path="S04-model.html"><a href="S04-model.html#hat-matrix"><i class="fa fa-check"></i><b>4.2</b> Properties of the Hat Matrix</a></li>
<li class="chapter" data-level="4.3" data-path="S04-model.html"><a href="S04-model.html#Cochran"><i class="fa fa-check"></i><b>4.3</b> Cochran’s theorem</a></li>
<li class="chapter" data-level="4.4" data-path="S04-model.html"><a href="S04-model.html#var-est-bias"><i class="fa fa-check"></i><b>4.4</b> Estimating the Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="S05-single.html"><a href="S05-single.html"><i class="fa fa-check"></i><b>5</b> Uncertainty for Individual Regression Coefficients</a>
<ul>
<li class="chapter" data-level="5.1" data-path="S05-single.html"><a href="S05-single.html#measuring-the-estimation-error"><i class="fa fa-check"></i><b>5.1</b> Measuring the Estimation Error</a></li>
<li class="chapter" data-level="5.2" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.3" data-path="S05-single.html"><a href="S05-single.html#hypthesis-tests"><i class="fa fa-check"></i><b>5.3</b> Hypthesis Tests</a></li>
<li class="chapter" data-level="5.4" data-path="S05-single.html"><a href="S05-single.html#r-experiments"><i class="fa fa-check"></i><b>5.4</b> R Experiments</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="S05-single.html"><a href="S05-single.html#fitting-the-model"><i class="fa fa-check"></i><b>5.4.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.4.2" data-path="S05-single.html"><a href="S05-single.html#estimating-the-variance-of-the-error"><i class="fa fa-check"></i><b>5.4.2</b> Estimating the Variance of the Error</a></li>
<li class="chapter" data-level="5.4.3" data-path="S05-single.html"><a href="S05-single.html#estimating-the-standard-errors"><i class="fa fa-check"></i><b>5.4.3</b> Estimating the Standard Errors</a></li>
<li class="chapter" data-level="5.4.4" data-path="S05-single.html"><a href="S05-single.html#hypothesis-tests"><i class="fa fa-check"></i><b>5.4.4</b> Hypothesis tests</a></li>
<li class="chapter" data-level="5.4.5" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals-1"><i class="fa fa-check"></i><b>5.4.5</b> Confidence Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html"><i class="fa fa-check"></i><b>6</b> Estimating Coefficients Simultaneously</a>
<ul>
<li class="chapter" data-level="6.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-dist"><i class="fa fa-check"></i><b>6.1</b> Linear Combinations of Coefficients</a></li>
<li class="chapter" data-level="6.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-CI"><i class="fa fa-check"></i><b>6.2</b> Confidence Regions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#result"><i class="fa fa-check"></i><b>6.2.1</b> Result</a></li>
<li class="chapter" data-level="6.2.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#numerical-experiments"><i class="fa fa-check"></i><b>6.2.2</b> Numerical Experiments</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-test"><i class="fa fa-check"></i><b>6.3</b> Hypothesis Tests</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html"><i class="fa fa-check"></i>Interlude: Loading Data into R</a>
<ul>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-csv-files"><i class="fa fa-check"></i>Importing CSV Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-microsoft-excel-files"><i class="fa fa-check"></i>Importing Microsoft Excel Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#checking-the-imported-data"><i class="fa fa-check"></i>Checking the Imported Data</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#common-problems"><i class="fa fa-check"></i>Common Problems</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P02.html"><a href="P02.html"><i class="fa fa-check"></i>Problem Sheet 2</a></li>
<li class="chapter" data-level="7" data-path="S07-examples.html"><a href="S07-examples.html"><i class="fa fa-check"></i><b>7</b> Examples</a>
<ul>
<li class="chapter" data-level="7.1" data-path="S07-examples.html"><a href="S07-examples.html#simple-confidence-interval"><i class="fa fa-check"></i><b>7.1</b> Simple Confidence Interval</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles"><i class="fa fa-check"></i><b>7.1.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.1.2" data-path="S07-examples.html"><a href="S07-examples.html#from-the-lm-output"><i class="fa fa-check"></i><b>7.1.2</b> From the <code>lm()</code> Output</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="S07-examples.html"><a href="S07-examples.html#confidence-intervals-for-the-mean"><i class="fa fa-check"></i><b>7.2</b> Confidence Intervals for the Mean</a></li>
<li class="chapter" data-level="7.3" data-path="S07-examples.html"><a href="S07-examples.html#testing-a-single-coefficient"><i class="fa fa-check"></i><b>7.3</b> Testing a Single Coefficient</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-1"><i class="fa fa-check"></i><b>7.3.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.3.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-i"><i class="fa fa-check"></i><b>7.3.2</b> Using the <code>lm()</code> Output, I</a></li>
<li class="chapter" data-level="7.3.3" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-ii"><i class="fa fa-check"></i><b>7.3.3</b> Using the <code>lm()</code> Output, II</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="S07-examples.html"><a href="S07-examples.html#testing-multiple-coefficents"><i class="fa fa-check"></i><b>7.4</b> Testing Multiple Coefficents</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-2"><i class="fa fa-check"></i><b>7.4.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.4.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output"><i class="fa fa-check"></i><b>7.4.2</b> Using the <code>lm()</code> output</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="S08-diagnostics.html"><a href="S08-diagnostics.html"><i class="fa fa-check"></i><b>8</b> Regression Diagnostics</a>
<ul>
<li class="chapter" data-level="8.1" data-path="S08-diagnostics.html"><a href="S08-diagnostics.html#plots"><i class="fa fa-check"></i><b>8.1</b> Diagnostic Plots</a></li>
<li class="chapter" data-level="8.2" data-path="S08-diagnostics.html"><a href="S08-diagnostics.html#R-squared"><i class="fa fa-check"></i><b>8.2</b> The Coefficient of Multiple Determination</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I03-lm-output.html"><a href="I03-lm-output.html"><i class="fa fa-check"></i>Interlude: Understanding the <code>lm()</code> Output</a></li>
<li class="chapter" data-level="9" data-path="S09-influence.html"><a href="S09-influence.html"><i class="fa fa-check"></i><b>9</b> The Influence of Observations</a>
<ul>
<li class="chapter" data-level="9.1" data-path="S09-influence.html"><a href="S09-influence.html#deleting"><i class="fa fa-check"></i><b>9.1</b> Deleting Observations</a></li>
<li class="chapter" data-level="9.2" data-path="S09-influence.html"><a href="S09-influence.html#influence"><i class="fa fa-check"></i><b>9.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="S10-multicoll.html"><a href="S10-multicoll.html"><i class="fa fa-check"></i><b>10</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="10.1" data-path="S10-multicoll.html"><a href="S10-multicoll.html#consequences-of-multicollinearity"><i class="fa fa-check"></i><b>10.1</b> Consequences of Multicollinearity</a></li>
<li class="chapter" data-level="10.2" data-path="S10-multicoll.html"><a href="S10-multicoll.html#detecting-multicollinearity"><i class="fa fa-check"></i><b>10.2</b> Detecting Multicollinearity</a></li>
<li class="chapter" data-level="10.3" data-path="S10-multicoll.html"><a href="S10-multicoll.html#mitigations"><i class="fa fa-check"></i><b>10.3</b> Mitigations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P03.html"><a href="P03.html"><i class="fa fa-check"></i>Problem Sheet 3</a></li>
<li class="chapter" data-level="11" data-path="S11-improving.html"><a href="S11-improving.html"><i class="fa fa-check"></i><b>11</b> Improving the Model Fit</a>
<ul>
<li class="chapter" data-level="11.1" data-path="S11-improving.html"><a href="S11-improving.html#linearising-the-mean"><i class="fa fa-check"></i><b>11.1</b> Linearising the Mean</a></li>
<li class="chapter" data-level="11.2" data-path="S11-improving.html"><a href="S11-improving.html#stabilising-the-variance"><i class="fa fa-check"></i><b>11.2</b> Stabilising the Variance</a></li>
<li class="chapter" data-level="11.3" data-path="S11-improving.html"><a href="S11-improving.html#power-transform"><i class="fa fa-check"></i><b>11.3</b> The Power Transform</a></li>
<li class="chapter" data-level="11.4" data-path="S11-improving.html"><a href="S11-improving.html#orthogonal-inputs"><i class="fa fa-check"></i><b>11.4</b> Orthogonal Inputs</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="S12-ridge.html"><a href="S12-ridge.html"><i class="fa fa-check"></i><b>12</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="S12-ridge.html"><a href="S12-ridge.html#definition-the-estimator"><i class="fa fa-check"></i><b>12.1</b> Definition the Estimator</a></li>
<li class="chapter" data-level="12.2" data-path="S12-ridge.html"><a href="S12-ridge.html#properties-of-the-estimate"><i class="fa fa-check"></i><b>12.2</b> Properties of the Estimate</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="S12-ridge.html"><a href="S12-ridge.html#bias"><i class="fa fa-check"></i><b>12.2.1</b> Bias</a></li>
<li class="chapter" data-level="12.2.2" data-path="S12-ridge.html"><a href="S12-ridge.html#variance"><i class="fa fa-check"></i><b>12.2.2</b> Variance</a></li>
<li class="chapter" data-level="12.2.3" data-path="S12-ridge.html"><a href="S12-ridge.html#mean-squared-error"><i class="fa fa-check"></i><b>12.2.3</b> Mean Squared Error</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="S12-ridge.html"><a href="S12-ridge.html#standardisation"><i class="fa fa-check"></i><b>12.3</b> Standardisation</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="S13-models.html"><a href="S13-models.html"><i class="fa fa-check"></i><b>13</b> Model selection</a>
<ul>
<li class="chapter" data-level="13.1" data-path="S13-models.html"><a href="S13-models.html#candidates-models"><i class="fa fa-check"></i><b>13.1</b> Candidates Models</a></li>
<li class="chapter" data-level="13.2" data-path="S13-models.html"><a href="S13-models.html#misspecified-models"><i class="fa fa-check"></i><b>13.2</b> Misspecified Models</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="S13-models.html"><a href="S13-models.html#missing-variables"><i class="fa fa-check"></i><b>13.2.1</b> Missing Variables</a></li>
<li class="chapter" data-level="13.2.2" data-path="S13-models.html"><a href="S13-models.html#unnecessary-variables"><i class="fa fa-check"></i><b>13.2.2</b> Unnecessary Variables</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="S13-models.html"><a href="S13-models.html#criteria"><i class="fa fa-check"></i><b>13.3</b> Assessing Models</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="S14-methods.html"><a href="S14-methods.html"><i class="fa fa-check"></i><b>14</b> Automatic Model Selection</a>
<ul>
<li class="chapter" data-level="14.1" data-path="S14-methods.html"><a href="S14-methods.html#exhaustive-search"><i class="fa fa-check"></i><b>14.1</b> Exhaustive Search</a></li>
<li class="chapter" data-level="14.2" data-path="S14-methods.html"><a href="S14-methods.html#search-algorithm"><i class="fa fa-check"></i><b>14.2</b> Search Algorithm</a></li>
<li class="chapter" data-level="14.3" data-path="S14-methods.html"><a href="S14-methods.html#other-methods"><i class="fa fa-check"></i><b>14.3</b> Other Methods</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="S14-methods.html"><a href="S14-methods.html#stepwise-forward-selection"><i class="fa fa-check"></i><b>14.3.1</b> Stepwise Forward Selection</a></li>
<li class="chapter" data-level="14.3.2" data-path="S14-methods.html"><a href="S14-methods.html#stepwise-backward-selection"><i class="fa fa-check"></i><b>14.3.2</b> Stepwise Backward Selection</a></li>
<li class="chapter" data-level="14.3.3" data-path="S14-methods.html"><a href="S14-methods.html#hybrid-methods"><i class="fa fa-check"></i><b>14.3.3</b> Hybrid Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="P04.html"><a href="P04.html"><i class="fa fa-check"></i>Problem Sheet 4</a></li>
<li class="chapter" data-level="15" data-path="S15-factors.html"><a href="S15-factors.html"><i class="fa fa-check"></i><b>15</b> Factors</a>
<ul>
<li class="chapter" data-level="15.1" data-path="S15-factors.html"><a href="S15-factors.html#indicator-variables"><i class="fa fa-check"></i><b>15.1</b> Indicator Variables</a></li>
<li class="chapter" data-level="15.2" data-path="S15-factors.html"><a href="S15-factors.html#interactions"><i class="fa fa-check"></i><b>15.2</b> Interactions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html"><i class="fa fa-check"></i>Practical</a>
<ul>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#dataset"><i class="fa fa-check"></i>Dataset</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#tasks"><i class="fa fa-check"></i>Tasks</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="S16-examples.html"><a href="S16-examples.html"><i class="fa fa-check"></i><b>16</b> Examples</a>
<ul>
<li class="chapter" data-level="16.1" data-path="S16-examples.html"><a href="S16-examples.html#interactions-example"><i class="fa fa-check"></i><b>16.1</b> Use of Interaction Terms in Modelling</a></li>
<li class="chapter" data-level="16.2" data-path="S16-examples.html"><a href="S16-examples.html#codings"><i class="fa fa-check"></i><b>16.2</b> Alternative Factor Codings</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="S17-robust.html"><a href="S17-robust.html"><i class="fa fa-check"></i><b>17</b> Robust Regression</a>
<ul>
<li class="chapter" data-level="17.1" data-path="S17-robust.html"><a href="S17-robust.html#outliers"><i class="fa fa-check"></i><b>17.1</b> Outliers</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="S17-robust.html"><a href="S17-robust.html#leverage"><i class="fa fa-check"></i><b>17.1.1</b> Leverage</a></li>
<li class="chapter" data-level="17.1.2" data-path="S17-robust.html"><a href="S17-robust.html#studentised-residuals"><i class="fa fa-check"></i><b>17.1.2</b> Studentised Residuals</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="S17-robust.html"><a href="S17-robust.html#breakdown-points"><i class="fa fa-check"></i><b>17.2</b> Breakdown Points</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P05.html"><a href="P05.html"><i class="fa fa-check"></i>Problem Sheet 5</a></li>
<li class="chapter" data-level="18" data-path="S18-m-est.html"><a href="S18-m-est.html"><i class="fa fa-check"></i><b>18</b> M-Estimators</a>
<ul>
<li class="chapter" data-level="18.1" data-path="S18-m-est.html"><a href="S18-m-est.html#definition"><i class="fa fa-check"></i><b>18.1</b> Definition</a></li>
<li class="chapter" data-level="18.2" data-path="S18-m-est.html"><a href="S18-m-est.html#iterative-methods"><i class="fa fa-check"></i><b>18.2</b> Iterative Methods</a></li>
<li class="chapter" data-level="18.3" data-path="S18-m-est.html"><a href="S18-m-est.html#objective-functions"><i class="fa fa-check"></i><b>18.3</b> Objective Functions</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="S18-m-est.html"><a href="S18-m-est.html#least-squares-method"><i class="fa fa-check"></i><b>18.3.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="18.3.2" data-path="S18-m-est.html"><a href="S18-m-est.html#least-absolute-values"><i class="fa fa-check"></i><b>18.3.2</b> Least Absolute Values</a></li>
<li class="chapter" data-level="18.3.3" data-path="S18-m-est.html"><a href="S18-m-est.html#Huber"><i class="fa fa-check"></i><b>18.3.3</b> Huber’s <span class="math inline">\(t\)</span>-function</a></li>
<li class="chapter" data-level="18.3.4" data-path="S18-m-est.html"><a href="S18-m-est.html#hampels-method"><i class="fa fa-check"></i><b>18.3.4</b> Hampel’s Method</a></li>
<li class="chapter" data-level="18.3.5" data-path="S18-m-est.html"><a href="S18-m-est.html#tukeys-bisquare-method"><i class="fa fa-check"></i><b>18.3.5</b> Tukey’s Bisquare Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="S19-efficiency.html"><a href="S19-efficiency.html"><i class="fa fa-check"></i><b>19</b> Efficiency of Robust Estimators</a>
<ul>
<li class="chapter" data-level="19.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#efficiency"><i class="fa fa-check"></i><b>19.1</b> Efficiency</a></li>
<li class="chapter" data-level="19.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#robust-estimators"><i class="fa fa-check"></i><b>19.2</b> Robust estimators</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#least-median-of-squares"><i class="fa fa-check"></i><b>19.2.1</b> Least Median of Squares</a></li>
<li class="chapter" data-level="19.2.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#least-trimmed-squares"><i class="fa fa-check"></i><b>19.2.2</b> Least Trimmed Squares</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html"><i class="fa fa-check"></i><b>A</b> Linear Algebra Reminders</a>
<ul>
<li class="chapter" data-level="A.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#vectors"><i class="fa fa-check"></i><b>A.1</b> Vectors</a></li>
<li class="chapter" data-level="A.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-rules"><i class="fa fa-check"></i><b>A.2</b> Matrices</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#transpose"><i class="fa fa-check"></i><b>A.2.1</b> Transpose</a></li>
<li class="chapter" data-level="A.2.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-vector-product"><i class="fa fa-check"></i><b>A.2.2</b> Matrix-vector Product</a></li>
<li class="chapter" data-level="A.2.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-matrix-product"><i class="fa fa-check"></i><b>A.2.3</b> Matrix-matrix Product</a></li>
<li class="chapter" data-level="A.2.4" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#rank"><i class="fa fa-check"></i><b>A.2.4</b> Rank</a></li>
<li class="chapter" data-level="A.2.5" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#trace"><i class="fa fa-check"></i><b>A.2.5</b> Trace</a></li>
<li class="chapter" data-level="A.2.6" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.2.6</b> Matrix Inverse</a></li>
<li class="chapter" data-level="A.2.7" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.2.7</b> Orthogonal Matrices</a></li>
<li class="chapter" data-level="A.2.8" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#positive-definite"><i class="fa fa-check"></i><b>A.2.8</b> Positive Definite Matrices</a></li>
<li class="chapter" data-level="A.2.9" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#idempotent"><i class="fa fa-check"></i><b>A.2.9</b> Idempotent Matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#eigenvalues"><i class="fa fa-check"></i><b>A.3</b> Eigenvalues</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="Sx2-probability.html"><a href="Sx2-probability.html"><i class="fa fa-check"></i><b>B</b> Probability Reminders</a>
<ul>
<li class="chapter" data-level="B.1" data-path="Sx2-probability.html"><a href="Sx2-probability.html#independence"><i class="fa fa-check"></i><b>B.1</b> Independence</a></li>
<li class="chapter" data-level="B.2" data-path="Sx2-probability.html"><a href="Sx2-probability.html#chi-square"><i class="fa fa-check"></i><b>B.2</b> The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="B.3" data-path="Sx2-probability.html"><a href="Sx2-probability.html#t"><i class="fa fa-check"></i><b>B.3</b> The t-distribution</a></li>
</ul></li>
<li class="divider"></li>
<li class="chapter"><span><b>THE END</b></span></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3714 Linear Regression and Robustness</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="S13-models" class="section level1 hasAnchor" number="13">
<h1><span class="header-section-number">Section 13</span> Model selection<a href="S13-models.html#S13-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!--
TODO:
* Rearrange the sections in the following order:
  - 9 Influence

  - 8.1 diagnostics
  - 11.1-11.3 + 13.1-13.2 how to improve models
  - 8.2 + 13.3 quantitative measures to compare model fit
  - 14 automatic model selection

  - 15 Factors
  - 16 Factor Examples

  - 10+11.4 multicollinearity
  - 12 ridge regression

  - 17 robust

* try to shorten this section
* add videos
* I believe the variance of \hat\beta_j always increases when
inputs are added.  Is this true?  Can we prove this?

\begin{equation*}
  (X \mid \tilde X)^\top (X \mid \tilde X)
  = \begin{pmatrix}
    X^\top X & X^\top \tilde X \\
    \tilde X^\top X & \tilde X^\top \tilde X
  \end{pmatrix}
\end{equation*}

\begin{equation*}
  \Bigl( (X \mid \tilde X)^\top (X \mid \tilde X) \Bigr)^{-1}_{1:q,1:q}
  = \Bigl( X^\top X - X^\top \tilde X (\tilde X^\top \tilde X)^{-1} \tilde X^\top X \Bigr)^{-1}
\end{equation*}
-->
<p>The aim of linear regression is to find a model with the following
properties:</p>
<ol style="list-style-type: decimal">
<li>The model gives good predictions <span class="math inline">\(\hat y\)</span>.</li>
<li>The model is easy to interpret.</li>
<li>The modelling assumptions are satisfied.</li>
</ol>
<p>These three aims can sometimes contradict: models with fewer parameters
are often easier to interpret, whereas good predictions sometimes require
a large number of parameters. Some trade-off
between these constraints must be made. In this section we discuss how
to choose the inputs for a model in a systematic way.</p>
<div id="candidates-models" class="section level2 hasAnchor" number="13.1">
<h2><span class="header-section-number">13.1</span> Candidates Models<a href="S13-models.html#candidates-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here we assume that we are given data with input variables <span class="math inline">\(x_1, \ldots, x_p\)</span>.
The standard model which we have discussed so far adds an intercept to these
inputs, so that there are <span class="math inline">\(q = p+1\)</span> model parameters in total. We can modify
this model in various ways:</p>
<ul>
<li><p>When we discussed hypothesis tests, we allowed for the possibility that
some inputs do not influence the output. A reasonable approach would
be to fit a model with these inputs omitted. Since each input
may either be included in or excluded from the model, independent of
the others, and similarly the intercept may be included or not,
there are <span class="math inline">\(2^{p+1}\)</span> possible models to consider.</p></li>
<li><p>In the section about <a href="S11-improving.html#S11-improving">Improving the Model Fit</a> we have seen that
sometimes it is useful to transform input variables before using them
in the model. This can either happen individually, <em>e.g.</em> <span class="math inline">\(x_2&#39; = \log x_2\)</span>, or collectively, <em>e.g.</em> <span class="math inline">\(x^\ast = x_1 x_2\)</span>. The number of
models we may obtain in this way is unlimited.</p></li>
</ul>
<p>If we want to compare these candidate models in a systematic way,
we need to use efficient methods for comparison, since a often a large
number of models needs to be considered.</p>
</div>
<div id="misspecified-models" class="section level2 hasAnchor" number="13.2">
<h2><span class="header-section-number">13.2</span> Misspecified Models<a href="S13-models.html#misspecified-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The model is said to be <strong>misspecified</strong>, if the data we are using
comes from a different model than the one we use to estimate
the coefficients.</p>
<div id="missing-variables" class="section level3 hasAnchor" number="13.2.1">
<h3><span class="header-section-number">13.2.1</span> Missing Variables<a href="S13-models.html#missing-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Assume that the data comes from the model
<span class="math display">\[\begin{equation*}
  Y = \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \varepsilon,
\end{equation*}\]</span>
(we can include the intercept as <span class="math inline">\(x_1 = 1\)</span> if needed),
but we are only using the first <span class="math inline">\(q\)</span> variables <span class="math inline">\(x_1, \ldots, x_q\)</span>,
where <span class="math inline">\(q &lt; p\)</span>, to estimate the coefficients. Then the least squares
estimate of <span class="math inline">\(\beta = (\beta_1, \ldots, \beta_q)\)</span> is given by
<span class="math display">\[\begin{equation*}
  \hat\beta
  = (X^\top X)^{-1} X^\top y
\end{equation*}\]</span>
where <span class="math inline">\(X \in \mathbb{R}^{n\times q}\)</span>,
and our estimate for the error variance <span class="math inline">\(\sigma^2\)</span> is
<span class="math display">\[\begin{equation*}
  \hat\sigma^2
  = \frac{1}{n-q} y^\top (I - H) y,
\end{equation*}\]</span>
where <span class="math inline">\(H = X (X^\top X)^{-1} X^\top\)</span> is the hat matrix computed
without using <span class="math inline">\(x_{q+1}, \ldots, x_p\)</span>.</p>
<p>If we write <span class="math inline">\(\tilde X \in \mathbb{R}^{n\times (p-q)}\)</span> for the matrix with columns
<span class="math inline">\(x_{q+1}, \ldots, x_p\)</span> and <span class="math inline">\(\tilde\beta = (\beta_{q+1}, \ldots, \beta_p) \in \mathbb{R}^{p-q}\)</span>, then the full model can be written as
<span class="math display">\[\begin{equation*}
  Y
  = X \beta + \tilde X \tilde\beta + \varepsilon
\end{equation*}\]</span>
and we get
<span class="math display">\[\begin{align*}
  \hat\beta
  &amp;= (X^\top X)^{-1} X^\top Y \\
  &amp;= (X^\top X)^{-1} X^\top (X \beta + \tilde X \tilde\beta + \varepsilon) \\
  &amp;= (X^\top X)^{-1} X^\top X \beta + (X^\top X)^{-1} X^\top \tilde X \tilde\beta + (X^\top X)^{-1} X^\top \varepsilon\\
  &amp;= \beta + (X^\top X)^{-1} X^\top \tilde X \tilde\beta + (X^\top X)^{-1} X^\top \varepsilon.
\end{align*}\]</span>
Thus, we have
<span class="math display">\[\begin{equation*}
  \mathbb{E}(\hat\beta)
  = \beta + (X^\top X)^{-1} X^\top \tilde X \tilde\beta
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{bias}}(\hat\beta)
  = (X^\top X)^{-1} X^\top \tilde X \tilde\beta.
\end{equation*}\]</span>
This shows that now the estimate is biased in general. There are two
special cases when omitting variables does not introduce a bias:
The first is when all of the omitted coefficients equal zero, <em>i.e.</em> when
we have <span class="math inline">\(\tilde\beta = 0\)</span>. The second case is when the omitted inputs are
orthogonal to the included inputs, since then we have <span class="math inline">\(X^\top \tilde X = 0\)</span>.</p>
<p>Using the formula for <span class="math inline">\(\hat\beta\)</span> we find
<span class="math display">\[\begin{align*}
  \mathop{\mathrm{Cov}}( \hat\beta )
  &amp;= \mathop{\mathrm{Cov}}\Bigl( \beta + (X^\top X)^{-1} X^\top \tilde X \tilde\beta + (X^\top X)^{-1} X^\top \varepsilon\Bigr) \\
  &amp;= \mathop{\mathrm{Cov}}\Bigl( (X^\top X)^{-1} X^\top \varepsilon\Bigr) \\
  &amp;= \sigma^2 (X^\top X)^{-1} X^\top I X (X^\top X)^{-1} \\
  &amp;= \sigma^2 (X^\top X)^{-1}.
\end{align*}\]</span>
Similar to our derivation in the section <a href="S04-model.html#var-est-bias">Estimating the Error Variance</a>
one can show that
<span class="math display">\[\begin{equation*}
  \mathbb{E}\bigl( \hat\sigma^2 \bigr)
  = \sigma^2 + \frac{\tilde\beta^\top \tilde X^\top (I - H) \tilde X \tilde \beta}{n-q}.
\end{equation*}\]</span>
This shows that the estimate of the error variance is in general also biased.</p>
<p>This shows that our estimates can become biased if some of the inputs
are missing from our model. As in the previous section, it may happen
that the MSE of the parameter estimates in the reduced model
is smaller than the MSE in the correct model; this is the case if the
variance of the estimates decreases enough to compensate for the
introduced bias.</p>
</div>
<div id="unnecessary-variables" class="section level3 hasAnchor" number="13.2.2">
<h3><span class="header-section-number">13.2.2</span> Unnecessary Variables<a href="S13-models.html#unnecessary-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If the data comes from a model with fewer inputs than the ones we are using,
the model is not “misspecified” in the strict sense. It is just the case that
some of the true <span class="math inline">\(\beta_j\)</span> exactly equal zero. In this case, our previous
results show that the least squares estimate is unbiased.</p>
<p>Including additional variables into a model still can cause problems:
One can show that each unnecessary variable added increases the variance of the
estimates <span class="math inline">\(\hat\beta_j\)</span>. Instead of giving a proof of this fact,
we illustrate the effect using a numerical example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-49" class="example"><strong>Example 13.1  </strong></span>Here we consider the <code>stackloss</code> dataset with an added column of
noise. We have seen that
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{Var}}( \hat\beta_j )
  = \sigma^2 \bigl( X^\top X \bigr)^{-1}_{jj}.
\end{equation*}\]</span>
While we don’t know the true value of <span class="math inline">\(\sigma^2\)</span>, we can determine
the relative change in variance when adding a column, since this process
does not change <span class="math inline">\(\sigma^2\)</span> and thus <span class="math inline">\(\sigma^2\)</span> will cancel when we
determine the relative change in variance.</p>
<p>We first compute the diagonal elements of <span class="math inline">\(\bigl( X^\top X \bigr)^{-1}\)</span>
for the original dataset:</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="S13-models.html#cb215-1" aria-hidden="true" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(stack.loss <span class="sc">~</span> ., <span class="at">data =</span> stackloss)</span>
<span id="cb215-2"><a href="S13-models.html#cb215-2" aria-hidden="true" tabindex="-1"></a>d1 <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X1) <span class="sc">%*%</span> X1))</span>
<span id="cb215-3"><a href="S13-models.html#cb215-3" aria-hidden="true" tabindex="-1"></a>d1</span></code></pre></div>
<pre class="rOutput"><code> (Intercept)     Air.Flow   Water.Temp   Acid.Conc. 
13.452726695  0.001728874  0.012875424  0.002322167 </code></pre>
<p>Now we add a column of noise and re-compute the values:</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="S13-models.html#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20211115</span>)</span>
<span id="cb217-2"><a href="S13-models.html#cb217-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X1)</span>
<span id="cb217-3"><a href="S13-models.html#cb217-3" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">cbind</span>(X1, <span class="fu">rnorm</span>(n))</span>
<span id="cb217-4"><a href="S13-models.html#cb217-4" aria-hidden="true" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X2) <span class="sc">%*%</span> X2))</span>
<span id="cb217-5"><a href="S13-models.html#cb217-5" aria-hidden="true" tabindex="-1"></a>d2</span></code></pre></div>
<pre class="rOutput"><code> (Intercept)     Air.Flow   Water.Temp   Acid.Conc.              
14.397515774  0.001730570  0.015195467  0.002796487  0.064828744 </code></pre>
<p>Finally, we determine the change in variance in percent:</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="S13-models.html#cb219-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span> <span class="sc">*</span> (d2[<span class="sc">-</span><span class="dv">5</span>] <span class="sc">-</span> d1) <span class="sc">/</span> d1, <span class="dv">3</span>)</span></code></pre></div>
<pre class="rOutput"><code>(Intercept)    Air.Flow  Water.Temp  Acid.Conc. 
      7.023       0.098      18.019      20.426 </code></pre>
<p>We see that the variances of the <span class="math inline">\(\hat\beta_j\)</span> increased by up to <span class="math inline">\(20\%\)</span>
when we added the unnecessary input.</p>
</div>
<p>The example illustrates that it is important to keep the model as small
as possible.</p>
</div>
</div>
<div id="criteria" class="section level2 hasAnchor" number="13.3">
<h2><span class="header-section-number">13.3</span> Assessing Models<a href="S13-models.html#criteria" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A variety of criteria is used in practice to select variables to include
in a regression model.</p>
<ol style="list-style-type: decimal">
<li><p>The <a href="S08-diagnostics.html#def:R-squared">coefficient of multiple determination</a>, <span class="math inline">\(R^2\)</span>, can be used
to compare models. “Good” models have large <span class="math inline">\(R^2\)</span>.
If comparisons are made between models with different
numbers of inputs then the <a href="S08-diagnostics.html#def:adjusted-R-squared">adjusted <span class="math inline">\(R^2\)</span>-value</a>,
<span class="math inline">\(R^2_\mathrm{adj}\)</span>, must be used.</p></li>
<li><p>The estimate for the error variance,
<span class="math display">\[\begin{equation*}
\hat\sigma^2 = \frac{1}{n - q} \sum_{i=1}^n (y_i - \hat y_i)^2
  \end{equation*}\]</span>
can be used, where <span class="math inline">\(q\)</span> is the number of columns of the design matrix.
(For the full model with an intercept we have <span class="math inline">\(q = p+1\)</span>.)
“Good” models have small <span class="math inline">\(\hat\sigma^2\)</span>. Since
<span class="math display">\[\begin{align*}
R^2_\mathrm{adj}
&amp;= 1 - \frac{\frac{1}{n-q}\mathrm{s}_{\hat\varepsilon}^2}{\frac{1}{n-1}\mathrm{s}_y^2} \\
&amp;= 1 - \frac{\frac{n-1}{n-q}\mathrm{s}_{\hat\varepsilon}^2}{\mathrm{s}_y^2} \\
&amp;= 1 - \frac{\frac{1}{n-q} \sum_{i=1}^n (y_i - \hat y_i)^2}{\mathrm{s}_y^2} \\
&amp;= 1 - \frac{\hat\sigma^2}{\mathrm{s}_y^2},
  \end{align*}\]</span>
where <span class="math inline">\(\mathrm{s}_{\hat\varepsilon}^2\)</span> and <span class="math inline">\(\mathrm{s}_y^2\)</span> are the sample variances
as before, minimising <span class="math inline">\(\hat\sigma^2\)</span> is equivalent to
maximising <span class="math inline">\(R^2_\mathrm{adj}\)</span>. Thus, this criterion is equivalent to
the first one.</p></li>
<li><p>The <strong>Prediction Error Sum of Squares (PRESS)</strong> compares each
output <span class="math inline">\(y_i\)</span> to the fitted value <span class="math inline">\(\hat y^{(i)}_i\)</span> for observation i,
computed without using sample <span class="math inline">\(i\)</span>:
<span class="math display">\[\begin{equation*}
  \mathrm{PRESS}
  := \sum_{i=1}^n \bigl( y_i - \hat y^{(i)}_i \bigr)^2.
\end{equation*}\]</span>
In the comparison, <span class="math inline">\(\hat y^{(i)}_i\)</span> is used instead of <span class="math inline">\(\hat y_i\)</span>
so that in models with too many parameters, overfitting does not
result in overoptimistic estimates.
“Good” models have small PRESS. The following lemma helps to compute
PRESS without having to fit <span class="math inline">\(n\)</span> separate models.</p>
<div class="lemma">
<p><span id="lem:PRESS" class="lemma"><strong>Lemma 13.1  </strong></span>We have
<span class="math display">\[\begin{equation*}
  \mathrm{PRESS}
  = \sum_{i=1}^n \Bigl( \frac{\hat\varepsilon_i}{1 - h_{ii}} \Bigr)^2,
\end{equation*}\]</span>
where <span class="math inline">\(h_{ii}\)</span> denotes the diagonal elements of the hat matrix <span class="math inline">\(H\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-50" class="proof"><em>Proof</em>. </span>From equation <a href="S09-influence.html#eq:dbeta-i">(9.3)</a> we know how the estimated regression
coefficients change, when observation <span class="math inline">\(i\)</span> is deleted from the model:
<span class="math display">\[\begin{equation*}
  \hat\beta^{(i)} - \hat\beta
  = - (X^\top X)^{-1} x_i \frac{\hat\varepsilon_i}{1 - h_{ii}}.
\end{equation*}\]</span>
Thus we have
<span class="math display">\[\begin{align*}
  \hat y^{(i)}_i - \hat y_i
  &amp;= \bigl( X \hat\beta^{(i)} - X \hat\beta \bigr)_i \\
  &amp;= - \bigl( X (X^\top X)^{-1} x_i \frac{\hat\varepsilon_i}{1 - h_{ii}} \bigr)_i \\
  &amp;= - x_i^\top (X^\top X)^{-1} x_i \frac{\hat\varepsilon_i}{1 - h_{ii}} \\
  &amp;= - \frac{h_{ii}}{1 - h_{ii}} \hat\varepsilon_i.
\end{align*}\]</span>
From this we get
<span class="math display">\[\begin{align*}
  y_i - \hat y^{(i)}_i
  &amp;= y_i - \hat y_i + \hat y_i - \hat y^{(i)}_i \\
  &amp;= \hat\varepsilon_i + \frac{h_{ii}}{1 - h_{ii}} \hat\varepsilon_i \\
  &amp;= \frac{1}{1 - h_{ii}} \hat\varepsilon_i.
\end{align*}\]</span>
Substituting this expression into the definition of PRESS
completes the proof.</p>
</div></li>
<li><p><strong>Akaike’s Information Criterion (AIC)</strong> is defined as
<span class="math display">\[\begin{equation*}
  \mathrm{AIC}
  = 2q - 2 \log(\hat L),
\end{equation*}\]</span>
where <span class="math inline">\(q\)</span> is the number of parameters in the model, and <span class="math inline">\(\hat L\)</span>
is the maximum of the likelihood function when these <span class="math inline">\(q\)</span> parameters
are chosen optimally. “Good” models have small AIC.</p>
<p>Since we assume <span class="math inline">\(\varepsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span>, i.i.d. and
we have <span class="math inline">\(\varepsilon_i = y_i - x_i^\top\beta\)</span>, the likelihood function
for our model is
<span class="math display">\[\begin{align*}
  L(\beta, \sigma^2; X, y)
  &amp;= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\Bigl( - \frac{(y_i - x_i^\top\beta)^2}{2\sigma^2} \Bigr) \\
  &amp;= \frac{1}{(2\pi\sigma^2)^{n/2}} \exp\Bigl( - \sum_{i=1}^n \frac{(y_i - x_i^\top\beta)^2}{2\sigma^2} \Bigr).
\end{align*}\]</span>
Taking logarithms we get
<span class="math display">\[\begin{align*}
  \log L(\beta, \sigma^2; X, y)
  &amp;= -\frac{n}{2} \log(2\pi\sigma^2) - \sum_{i=1}^n \frac{(y_i - x_i^\top\beta)^2}{2\sigma^2} \\
  &amp;= -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} r(\beta),
\end{align*}\]</span>
where <span class="math inline">\(r(\beta)\)</span> is the residual sum of squares, as introduced
in the section about <a href="S02-multiple.html#S02-multiple">Least Squares Estimates</a>. To compute the AIC
we need to maximise this expression. The values of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>
where the maximum is taken are called the <strong>maximum likelihood estimate (MLE)</strong>
for <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>. From lemma <a href="S02-multiple.html#lem:multiple-LSQ">2.1</a> we know
that for fixed <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(L\)</span> takes its maximum when <span class="math inline">\(\beta\)</span> equals the
least squares estimate <span class="math inline">\(\hat\beta\)</span>. Taking derivatives we can then
find the optimal value of <span class="math inline">\(\sigma^2\)</span>. The result is
<span class="math display">\[\begin{equation*}
  \hat\sigma^2_\mathrm{MLE}
  = \frac1n r(\hat\beta)
  = \frac{1}{n} \sum_{i=1}^n \bigl( y_i - (X \hat\beta)_i \bigr)^2
  = \frac{n-q}{n} \hat\sigma^2.
\end{equation*}\]</span>
Thus we get
<span class="math display">\[\begin{align*}
  \mathrm{AIC}
  &amp;= 2q + n \log(2\pi\hat\sigma^2_\mathrm{MLE}) + \frac{1}{\hat\sigma^2_\mathrm{MLE}} r(\hat\beta) \\
  &amp;= 2q + n \log\bigl( 2\pi r(\hat\beta) / n \bigr) + n \\
  &amp;= 2q + n \log\bigl( \|\hat\varepsilon\|^2 \bigr) + n + n \log( 2\pi / n )
\end{align*}\]</span></p></li>
</ol>
<div class="summary">
<p><strong>Summary</strong></p>
<ul>
<li>We have discussed how different models can be considered by omitting
existing variables or by adding non-linear functions of the inputs
as new variables.</li>
<li>We have seen that removing too many variables can introduce a bias.</li>
<li>We have seen that adding too many variables can increase the variance
of the estimate.</li>
<li>We have considered different criteria for choosing the “best” model.</li>
</ul>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="S12-ridge.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="S14-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH3714.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
