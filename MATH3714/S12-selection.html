<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 12 Model Selection | MATH3714 Linear Regression and Robustness</title>
  <meta name="description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2025/26" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 12 Model Selection | MATH3714 Linear Regression and Robustness" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2025/26" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 12 Model Selection | MATH3714 Linear Regression and Robustness" />
  
  <meta name="twitter:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2025/26" />
  

<meta name="author" content="Jochen Voss" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="P03.html"/>
<link rel="next" href="S13-exhaustive.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="jvstyle.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">MATH3714 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="S01-simple.html"><a href="S01-simple.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="S01-simple.html"><a href="S01-simple.html#residual-sum-of-squares"><i class="fa fa-check"></i><b>1.1</b> Residual Sum of Squares</a></li>
<li class="chapter" data-level="1.2" data-path="S01-simple.html"><a href="S01-simple.html#linear-regression-as-a-parameter-estimation-problem"><i class="fa fa-check"></i><b>1.2</b> Linear Regression as a Parameter Estimation Problem</a></li>
<li class="chapter" data-level="1.3" data-path="S01-simple.html"><a href="S01-simple.html#sec:simple-mat"><i class="fa fa-check"></i><b>1.3</b> Matrix Notation</a></li>
</ul></li>
<li class="part"><span><b>I Theory</b></span></li>
<li class="chapter" data-level="2" data-path="S02-multiple.html"><a href="S02-multiple.html"><i class="fa fa-check"></i><b>2</b> Least Squares Estimates</a>
<ul>
<li class="chapter" data-level="2.1" data-path="S02-multiple.html"><a href="S02-multiple.html#data-and-models"><i class="fa fa-check"></i><b>2.1</b> Data and Models</a></li>
<li class="chapter" data-level="2.2" data-path="S02-multiple.html"><a href="S02-multiple.html#the-normal-equations"><i class="fa fa-check"></i><b>2.2</b> The Normal Equations</a></li>
<li class="chapter" data-level="2.3" data-path="S02-multiple.html"><a href="S02-multiple.html#fitted-values"><i class="fa fa-check"></i><b>2.3</b> Fitted Values</a></li>
<li class="chapter" data-level="2.4" data-path="S02-multiple.html"><a href="S02-multiple.html#example"><i class="fa fa-check"></i><b>2.4</b> Example</a></li>
<li class="chapter" data-level="2.5" data-path="S02-multiple.html"><a href="S02-multiple.html#models-without-intercept"><i class="fa fa-check"></i><b>2.5</b> Models Without Intercept</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html"><i class="fa fa-check"></i>Interlude: Linear Regression in R</a>
<ul>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-fitting"><i class="fa fa-check"></i>Fitting a Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-model"><i class="fa fa-check"></i>Understanding the Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-predict"><i class="fa fa-check"></i>Making Predictions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="S03-cov.html"><a href="S03-cov.html"><i class="fa fa-check"></i><b>3</b> Random Vectors and Covariance</a>
<ul>
<li class="chapter" data-level="3.1" data-path="S03-cov.html"><a href="S03-cov.html#expectation"><i class="fa fa-check"></i><b>3.1</b> Expectation</a></li>
<li class="chapter" data-level="3.2" data-path="S03-cov.html"><a href="S03-cov.html#sec:covariance"><i class="fa fa-check"></i><b>3.2</b> Covariance Matrix</a></li>
<li class="chapter" data-level="3.3" data-path="S03-cov.html"><a href="S03-cov.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>3.3</b> The Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P01.html"><a href="P01.html"><i class="fa fa-check"></i>Problem Sheet 1</a></li>
<li class="chapter" data-level="4" data-path="S04-model.html"><a href="S04-model.html"><i class="fa fa-check"></i><b>4</b> Properties of the Least Squares Estimate</a>
<ul>
<li class="chapter" data-level="4.1" data-path="S04-model.html"><a href="S04-model.html#mean-and-covariance"><i class="fa fa-check"></i><b>4.1</b> Mean and Covariance</a></li>
<li class="chapter" data-level="4.2" data-path="S04-model.html"><a href="S04-model.html#hat-matrix"><i class="fa fa-check"></i><b>4.2</b> Properties of the Hat Matrix</a></li>
<li class="chapter" data-level="4.3" data-path="S04-model.html"><a href="S04-model.html#Cochran"><i class="fa fa-check"></i><b>4.3</b> Cochran’s theorem</a></li>
<li class="chapter" data-level="4.4" data-path="S04-model.html"><a href="S04-model.html#var-est-bias"><i class="fa fa-check"></i><b>4.4</b> Estimating the Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="S05-single.html"><a href="S05-single.html"><i class="fa fa-check"></i><b>5</b> Uncertainty for Individual Regression Coefficients</a>
<ul>
<li class="chapter" data-level="5.1" data-path="S05-single.html"><a href="S05-single.html#measuring-the-estimation-error"><i class="fa fa-check"></i><b>5.1</b> Measuring the Estimation Error</a></li>
<li class="chapter" data-level="5.2" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.3" data-path="S05-single.html"><a href="S05-single.html#hypothesis-tests"><i class="fa fa-check"></i><b>5.3</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="5.4" data-path="S05-single.html"><a href="S05-single.html#r-experiments"><i class="fa fa-check"></i><b>5.4</b> R Experiments</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="S05-single.html"><a href="S05-single.html#fitting-the-model"><i class="fa fa-check"></i><b>5.4.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.4.2" data-path="S05-single.html"><a href="S05-single.html#estimating-the-variance-of-the-error"><i class="fa fa-check"></i><b>5.4.2</b> Estimating the Variance of the Error</a></li>
<li class="chapter" data-level="5.4.3" data-path="S05-single.html"><a href="S05-single.html#estimating-the-standard-errors"><i class="fa fa-check"></i><b>5.4.3</b> Estimating the Standard Errors</a></li>
<li class="chapter" data-level="5.4.4" data-path="S05-single.html"><a href="S05-single.html#hypothesis-tests-1"><i class="fa fa-check"></i><b>5.4.4</b> Hypothesis tests</a></li>
<li class="chapter" data-level="5.4.5" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals-1"><i class="fa fa-check"></i><b>5.4.5</b> Confidence Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html"><i class="fa fa-check"></i><b>6</b> Estimating Coefficients Simultaneously</a>
<ul>
<li class="chapter" data-level="6.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-dist"><i class="fa fa-check"></i><b>6.1</b> Linear Combinations of Coefficients</a></li>
<li class="chapter" data-level="6.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-CI"><i class="fa fa-check"></i><b>6.2</b> Confidence Regions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#result"><i class="fa fa-check"></i><b>6.2.1</b> Result</a></li>
<li class="chapter" data-level="6.2.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#numerical-experiments"><i class="fa fa-check"></i><b>6.2.2</b> Numerical Experiments</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-test"><i class="fa fa-check"></i><b>6.3</b> Hypothesis Tests</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html"><i class="fa fa-check"></i>Interlude: Loading Data into R</a>
<ul>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-csv-files"><i class="fa fa-check"></i>Importing CSV Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-microsoft-excel-files"><i class="fa fa-check"></i>Importing Microsoft Excel Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#checking-the-imported-data"><i class="fa fa-check"></i>Checking the Imported Data</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#common-problems"><i class="fa fa-check"></i>Common Problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="S07-examples.html"><a href="S07-examples.html"><i class="fa fa-check"></i><b>7</b> Examples</a>
<ul>
<li class="chapter" data-level="7.1" data-path="S07-examples.html"><a href="S07-examples.html#simple-confidence-interval"><i class="fa fa-check"></i><b>7.1</b> Simple Confidence Interval</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles"><i class="fa fa-check"></i><b>7.1.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.1.2" data-path="S07-examples.html"><a href="S07-examples.html#from-the-lm-output"><i class="fa fa-check"></i><b>7.1.2</b> From the <code>lm()</code> Output</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="S07-examples.html"><a href="S07-examples.html#confidence-intervals-for-the-mean"><i class="fa fa-check"></i><b>7.2</b> Confidence Intervals for the Mean</a></li>
<li class="chapter" data-level="7.3" data-path="S07-examples.html"><a href="S07-examples.html#testing-a-single-coefficient"><i class="fa fa-check"></i><b>7.3</b> Testing a Single Coefficient</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-1"><i class="fa fa-check"></i><b>7.3.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.3.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-i"><i class="fa fa-check"></i><b>7.3.2</b> Using the <code>lm()</code> Output, I</a></li>
<li class="chapter" data-level="7.3.3" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-ii"><i class="fa fa-check"></i><b>7.3.3</b> Using the <code>lm()</code> Output, II</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="S07-examples.html"><a href="S07-examples.html#testing-multiple-coefficents"><i class="fa fa-check"></i><b>7.4</b> Testing Multiple Coefficents</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-2"><i class="fa fa-check"></i><b>7.4.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.4.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output"><i class="fa fa-check"></i><b>7.4.2</b> Using the <code>lm()</code> output</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="P02.html"><a href="P02.html"><i class="fa fa-check"></i>Problem Sheet 2</a></li>
<li class="chapter" data-level="8" data-path="S08-influence.html"><a href="S08-influence.html"><i class="fa fa-check"></i><b>8</b> The Influence of Observations</a>
<ul>
<li class="chapter" data-level="8.1" data-path="S08-influence.html"><a href="S08-influence.html#deleting"><i class="fa fa-check"></i><b>8.1</b> Deleting Observations</a></li>
<li class="chapter" data-level="8.2" data-path="S08-influence.html"><a href="S08-influence.html#influence"><i class="fa fa-check"></i><b>8.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="part"><span><b>II Linear Regression in Practice</b></span></li>
<li class="chapter" data-level="9" data-path="S09-plots.html"><a href="S09-plots.html"><i class="fa fa-check"></i><b>9</b> Diagnostic Plots</a>
<ul>
<li class="chapter" data-level="9.1" data-path="S09-plots.html"><a href="S09-plots.html#residual-plots"><i class="fa fa-check"></i><b>9.1</b> Residual Plots</a></li>
<li class="chapter" data-level="9.2" data-path="S09-plots.html"><a href="S09-plots.html#q-q-plots"><i class="fa fa-check"></i><b>9.2</b> Q-Q Plots</a></li>
<li class="chapter" data-level="9.3" data-path="S09-plots.html"><a href="S09-plots.html#other-plot-types"><i class="fa fa-check"></i><b>9.3</b> Other Plot Types</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="S10-diagnostics.html"><a href="S10-diagnostics.html"><i class="fa fa-check"></i><b>10</b> Measures for Model Fit</a>
<ul>
<li class="chapter" data-level="10.1" data-path="S10-diagnostics.html"><a href="S10-diagnostics.html#R-squared"><i class="fa fa-check"></i><b>10.1</b> The Coefficient of Multiple Determination</a></li>
<li class="chapter" data-level="10.2" data-path="S10-diagnostics.html"><a href="S10-diagnostics.html#error-var"><i class="fa fa-check"></i><b>10.2</b> Error Variance</a></li>
<li class="chapter" data-level="10.3" data-path="S10-diagnostics.html"><a href="S10-diagnostics.html#PRESS"><i class="fa fa-check"></i><b>10.3</b> Prediction Error Sum of Squares</a></li>
<li class="chapter" data-level="10.4" data-path="S10-diagnostics.html"><a href="S10-diagnostics.html#AIC"><i class="fa fa-check"></i><b>10.4</b> Akaike’s Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I03-lm-output.html"><a href="I03-lm-output.html"><i class="fa fa-check"></i>Interlude: Understanding the <code>lm()</code> Output</a></li>
<li class="chapter" data-level="11" data-path="S11-improving.html"><a href="S11-improving.html"><i class="fa fa-check"></i><b>11</b> Improving the Model Fit</a>
<ul>
<li class="chapter" data-level="11.1" data-path="S11-improving.html"><a href="S11-improving.html#linearising-the-mean"><i class="fa fa-check"></i><b>11.1</b> Linearising the Mean</a></li>
<li class="chapter" data-level="11.2" data-path="S11-improving.html"><a href="S11-improving.html#stabilising-the-variance"><i class="fa fa-check"></i><b>11.2</b> Stabilising the Variance</a></li>
<li class="chapter" data-level="11.3" data-path="S11-improving.html"><a href="S11-improving.html#power-transform"><i class="fa fa-check"></i><b>11.3</b> The Power Transform</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P03.html"><a href="P03.html"><i class="fa fa-check"></i>Problem Sheet 3</a></li>
<li class="chapter" data-level="12" data-path="S12-selection.html"><a href="S12-selection.html"><i class="fa fa-check"></i><b>12</b> Model Selection</a>
<ul>
<li class="chapter" data-level="12.1" data-path="S12-selection.html"><a href="S12-selection.html#candidate-models"><i class="fa fa-check"></i><b>12.1</b> Candidate Models</a></li>
<li class="chapter" data-level="12.2" data-path="S12-selection.html"><a href="S12-selection.html#misspecified-models"><i class="fa fa-check"></i><b>12.2</b> Misspecified Models</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="S12-selection.html"><a href="S12-selection.html#missing-variables"><i class="fa fa-check"></i><b>12.2.1</b> Missing Variables</a></li>
<li class="chapter" data-level="12.2.2" data-path="S12-selection.html"><a href="S12-selection.html#unnecessary-variables"><i class="fa fa-check"></i><b>12.2.2</b> Unnecessary Variables</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="S12-selection.html"><a href="S12-selection.html#choosing-between-models"><i class="fa fa-check"></i><b>12.3</b> Choosing Between Models</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="S12-selection.html"><a href="S12-selection.html#criteria-for-model-selection"><i class="fa fa-check"></i><b>12.3.1</b> Criteria for Model Selection</a></li>
<li class="chapter" data-level="12.3.2" data-path="S12-selection.html"><a href="S12-selection.html#exhaustive-search"><i class="fa fa-check"></i><b>12.3.2</b> Exhaustive Search</a></li>
<li class="chapter" data-level="12.3.3" data-path="S12-selection.html"><a href="S12-selection.html#stepwise-forward-selection"><i class="fa fa-check"></i><b>12.3.3</b> Stepwise Forward Selection</a></li>
<li class="chapter" data-level="12.3.4" data-path="S12-selection.html"><a href="S12-selection.html#stepwise-backward-selection"><i class="fa fa-check"></i><b>12.3.4</b> Stepwise Backward Selection</a></li>
<li class="chapter" data-level="12.3.5" data-path="S12-selection.html"><a href="S12-selection.html#hybrid-methods"><i class="fa fa-check"></i><b>12.3.5</b> Hybrid Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="S13-exhaustive.html"><a href="S13-exhaustive.html"><i class="fa fa-check"></i><b>13</b> Exhaustive Model Search</a>
<ul>
<li class="chapter" data-level="13.1" data-path="S13-exhaustive.html"><a href="S13-exhaustive.html#exhaustive-search-1"><i class="fa fa-check"></i><b>13.1</b> Exhaustive Search</a></li>
<li class="chapter" data-level="13.2" data-path="S13-exhaustive.html"><a href="S13-exhaustive.html#search-algorithm"><i class="fa fa-check"></i><b>13.2</b> Search Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="S14-factors.html"><a href="S14-factors.html"><i class="fa fa-check"></i><b>14</b> Factors</a>
<ul>
<li class="chapter" data-level="14.1" data-path="S14-factors.html"><a href="S14-factors.html#indicator-variables"><i class="fa fa-check"></i><b>14.1</b> Indicator Variables</a></li>
<li class="chapter" data-level="14.2" data-path="S14-factors.html"><a href="S14-factors.html#interactions"><i class="fa fa-check"></i><b>14.2</b> Interactions</a></li>
<li class="chapter" data-level="14.3" data-path="S14-factors.html"><a href="S14-factors.html#interactions-example"><i class="fa fa-check"></i><b>14.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="S15-multicoll.html"><a href="S15-multicoll.html"><i class="fa fa-check"></i><b>15</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="15.1" data-path="S15-multicoll.html"><a href="S15-multicoll.html#consequences-of-multicollinearity"><i class="fa fa-check"></i><b>15.1</b> Consequences of Multicollinearity</a></li>
<li class="chapter" data-level="15.2" data-path="S15-multicoll.html"><a href="S15-multicoll.html#detecting-multicollinearity"><i class="fa fa-check"></i><b>15.2</b> Detecting Multicollinearity</a></li>
<li class="chapter" data-level="15.3" data-path="S15-multicoll.html"><a href="S15-multicoll.html#mitigations"><i class="fa fa-check"></i><b>15.3</b> Mitigations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P04.html"><a href="P04.html"><i class="fa fa-check"></i>Problem Sheet 4</a></li>
<li class="chapter" data-level="16" data-path="S16-ridge.html"><a href="S16-ridge.html"><i class="fa fa-check"></i><b>16</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="16.1" data-path="S16-ridge.html"><a href="S16-ridge.html#definition-the-estimator"><i class="fa fa-check"></i><b>16.1</b> Definition the Estimator</a></li>
<li class="chapter" data-level="16.2" data-path="S16-ridge.html"><a href="S16-ridge.html#properties-of-the-estimate"><i class="fa fa-check"></i><b>16.2</b> Properties of the Estimate</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="S16-ridge.html"><a href="S16-ridge.html#bias"><i class="fa fa-check"></i><b>16.2.1</b> Bias</a></li>
<li class="chapter" data-level="16.2.2" data-path="S16-ridge.html"><a href="S16-ridge.html#variance"><i class="fa fa-check"></i><b>16.2.2</b> Variance</a></li>
<li class="chapter" data-level="16.2.3" data-path="S16-ridge.html"><a href="S16-ridge.html#mean-squared-error"><i class="fa fa-check"></i><b>16.2.3</b> Mean Squared Error</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="S16-ridge.html"><a href="S16-ridge.html#standardisation"><i class="fa fa-check"></i><b>16.3</b> Standardisation</a></li>
</ul></li>
<li class="part"><span><b>III Robust Regression</b></span></li>
<li class="chapter" data-level="17" data-path="S17-robust.html"><a href="S17-robust.html"><i class="fa fa-check"></i><b>17</b> Robust Regression</a>
<ul>
<li class="chapter" data-level="17.1" data-path="S17-robust.html"><a href="S17-robust.html#outliers"><i class="fa fa-check"></i><b>17.1</b> Outliers</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="S17-robust.html"><a href="S17-robust.html#leverage"><i class="fa fa-check"></i><b>17.1.1</b> Leverage</a></li>
<li class="chapter" data-level="17.1.2" data-path="S17-robust.html"><a href="S17-robust.html#studentised-residuals"><i class="fa fa-check"></i><b>17.1.2</b> Studentised Residuals</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="S17-robust.html"><a href="S17-robust.html#breakdown-points"><i class="fa fa-check"></i><b>17.2</b> Breakdown Points</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="S18-m-est.html"><a href="S18-m-est.html"><i class="fa fa-check"></i><b>18</b> M-Estimators</a>
<ul>
<li class="chapter" data-level="18.1" data-path="S18-m-est.html"><a href="S18-m-est.html#definition"><i class="fa fa-check"></i><b>18.1</b> Definition</a></li>
<li class="chapter" data-level="18.2" data-path="S18-m-est.html"><a href="S18-m-est.html#iterative-methods"><i class="fa fa-check"></i><b>18.2</b> Iterative Methods</a></li>
<li class="chapter" data-level="18.3" data-path="S18-m-est.html"><a href="S18-m-est.html#objective-functions"><i class="fa fa-check"></i><b>18.3</b> Objective Functions</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="S18-m-est.html"><a href="S18-m-est.html#least-squares-method"><i class="fa fa-check"></i><b>18.3.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="18.3.2" data-path="S18-m-est.html"><a href="S18-m-est.html#least-absolute-values"><i class="fa fa-check"></i><b>18.3.2</b> Least Absolute Values</a></li>
<li class="chapter" data-level="18.3.3" data-path="S18-m-est.html"><a href="S18-m-est.html#Huber"><i class="fa fa-check"></i><b>18.3.3</b> Huber’s <span class="math inline">\(t\)</span>-function</a></li>
<li class="chapter" data-level="18.3.4" data-path="S18-m-est.html"><a href="S18-m-est.html#hampel"><i class="fa fa-check"></i><b>18.3.4</b> Hampel’s Method</a></li>
<li class="chapter" data-level="18.3.5" data-path="S18-m-est.html"><a href="S18-m-est.html#bisquare"><i class="fa fa-check"></i><b>18.3.5</b> Tukey’s Bisquare Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="S19-efficiency.html"><a href="S19-efficiency.html"><i class="fa fa-check"></i><b>19</b> Efficiency of Robust Estimators</a>
<ul>
<li class="chapter" data-level="19.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#efficiency"><i class="fa fa-check"></i><b>19.1</b> Efficiency</a></li>
<li class="chapter" data-level="19.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#robust-estimators"><i class="fa fa-check"></i><b>19.2</b> Robust estimators</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#LMS"><i class="fa fa-check"></i><b>19.2.1</b> Least Median of Squares</a></li>
<li class="chapter" data-level="19.2.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#LTS"><i class="fa fa-check"></i><b>19.2.2</b> Least Trimmed Squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="I04-robust.html"><a href="I04-robust.html"><i class="fa fa-check"></i>Interlude: Robust Regression in R</a>
<ul>
<li class="chapter" data-level="" data-path="I04-robust.html"><a href="I04-robust.html#m-estimators"><i class="fa fa-check"></i>M-Estimators</a></li>
<li class="chapter" data-level="" data-path="I04-robust.html"><a href="I04-robust.html#lav-estimation"><i class="fa fa-check"></i>LAV Estimation</a></li>
<li class="chapter" data-level="" data-path="I04-robust.html"><a href="I04-robust.html#high-breakdown-point-methods"><i class="fa fa-check"></i>High Breakdown Point Methods</a></li>
<li class="chapter" data-level="" data-path="I04-robust.html"><a href="I04-robust.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P05.html"><a href="P05.html"><i class="fa fa-check"></i>Problem Sheet 5</a></li>
<li class="chapter" data-level="20" data-path="S20-examples.html"><a href="S20-examples.html"><i class="fa fa-check"></i><b>20</b> Examples</a>
<ul>
<li class="chapter" data-level="20.1" data-path="S20-examples.html"><a href="S20-examples.html#comparing-robust-methods"><i class="fa fa-check"></i><b>20.1</b> Comparing Robust Methods</a></li>
<li class="chapter" data-level="20.2" data-path="S20-examples.html"><a href="S20-examples.html#residual-analysis"><i class="fa fa-check"></i><b>20.2</b> Residual Analysis</a></li>
<li class="chapter" data-level="20.3" data-path="S20-examples.html"><a href="S20-examples.html#weight-comparisons"><i class="fa fa-check"></i><b>20.3</b> Weight Comparisons</a></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html"><i class="fa fa-check"></i><b>A</b> Linear Algebra Reminders</a>
<ul>
<li class="chapter" data-level="A.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#vectors"><i class="fa fa-check"></i><b>A.1</b> Vectors</a></li>
<li class="chapter" data-level="A.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-rules"><i class="fa fa-check"></i><b>A.2</b> Matrices</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#transpose"><i class="fa fa-check"></i><b>A.2.1</b> Transpose</a></li>
<li class="chapter" data-level="A.2.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-vector-product"><i class="fa fa-check"></i><b>A.2.2</b> Matrix-vector Product</a></li>
<li class="chapter" data-level="A.2.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-matrix-product"><i class="fa fa-check"></i><b>A.2.3</b> Matrix-matrix Product</a></li>
<li class="chapter" data-level="A.2.4" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#rank"><i class="fa fa-check"></i><b>A.2.4</b> Rank</a></li>
<li class="chapter" data-level="A.2.5" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#trace"><i class="fa fa-check"></i><b>A.2.5</b> Trace</a></li>
<li class="chapter" data-level="A.2.6" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.2.6</b> Matrix Inverse</a></li>
<li class="chapter" data-level="A.2.7" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.2.7</b> Orthogonal Matrices</a></li>
<li class="chapter" data-level="A.2.8" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#positive-definite"><i class="fa fa-check"></i><b>A.2.8</b> Positive Definite Matrices</a></li>
<li class="chapter" data-level="A.2.9" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#idempotent"><i class="fa fa-check"></i><b>A.2.9</b> Idempotent Matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#eigenvalues"><i class="fa fa-check"></i><b>A.3</b> Eigenvalues</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="Sx2-probability.html"><a href="Sx2-probability.html"><i class="fa fa-check"></i><b>B</b> Probability Reminders</a>
<ul>
<li class="chapter" data-level="B.1" data-path="Sx2-probability.html"><a href="Sx2-probability.html#independence"><i class="fa fa-check"></i><b>B.1</b> Independence</a></li>
<li class="chapter" data-level="B.2" data-path="Sx2-probability.html"><a href="Sx2-probability.html#expectation-1"><i class="fa fa-check"></i><b>B.2</b> Expectation</a></li>
<li class="chapter" data-level="B.3" data-path="Sx2-probability.html"><a href="Sx2-probability.html#variance-1"><i class="fa fa-check"></i><b>B.3</b> Variance</a></li>
<li class="chapter" data-level="B.4" data-path="Sx2-probability.html"><a href="Sx2-probability.html#covariance"><i class="fa fa-check"></i><b>B.4</b> Covariance</a></li>
<li class="chapter" data-level="B.5" data-path="Sx2-probability.html"><a href="Sx2-probability.html#well-known-distributions"><i class="fa fa-check"></i><b>B.5</b> Well-Known Distributions</a>
<ul>
<li class="chapter" data-level="B.5.1" data-path="Sx2-probability.html"><a href="Sx2-probability.html#chi-square"><i class="fa fa-check"></i><b>B.5.1</b> The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="B.5.2" data-path="Sx2-probability.html"><a href="Sx2-probability.html#t"><i class="fa fa-check"></i><b>B.5.2</b> The t-distribution</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li class="chapter"><span><b>THE END</b></span></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3714 Linear Regression and Robustness</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="S12-selection" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">Section 12</span> Model Selection<a href="S12-selection.html#S12-selection" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="candidate-models" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> Candidate Models<a href="S12-selection.html#candidate-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here we assume that we are given data with input variables <span class="math inline">\(x_1, \ldots, x_p\)</span>.
The standard model which we have discussed so far adds an intercept to these
inputs, so that there are <span class="math inline">\(q = p+1\)</span> model parameters in total. We can modify
this model in various ways:</p>
<ul>
<li><p>When we discussed hypothesis tests, we allowed for the possibility that
some inputs do not influence the output. A reasonable approach would
be to fit a model with these inputs omitted. Since each input
may either be included in or excluded from the model, independent of
the others, and similarly the intercept may be included or not,
there are <span class="math inline">\(2^{p+1}\)</span> possible models to consider.</p></li>
<li><p>In the section about <a href="S11-improving.html#S11-improving">Improving the Model Fit</a> we have seen that
sometimes it is useful to transform input variables before using them
in the model. This can either happen individually, <em>e.g.</em> <span class="math inline">\(x_2&#39; = \log
x_2\)</span>, or collectively, <em>e.g.</em> <span class="math inline">\(x^\ast = x_1 x_2\)</span>. The number of
models we may obtain in this way is unlimited.</p></li>
</ul>
<p>If we want to compare these candidate models in a systematic way,
we need to use efficient methods for comparison, since often a large
number of models needs to be considered.</p>
</div>
<div id="misspecified-models" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> Misspecified Models<a href="S12-selection.html#misspecified-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The model is said to be <strong>misspecified</strong>, if the data we are using
comes from a different model than the one we use to estimate
the coefficients.</p>
<div id="missing-variables" class="section level3 hasAnchor" number="12.2.1">
<h3><span class="header-section-number">12.2.1</span> Missing Variables<a href="S12-selection.html#missing-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Assume that the data comes from the model
<span class="math display">\[\begin{equation*}
  Y = \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \varepsilon,
\end{equation*}\]</span>
(we can include the intercept as <span class="math inline">\(x_1 = 1\)</span> if needed),
but we are only using the first <span class="math inline">\(q\)</span> variables <span class="math inline">\(x_1, \ldots, x_q\)</span>,
where <span class="math inline">\(q &lt; p\)</span>, to estimate the coefficients. Then the least squares
estimate of <span class="math inline">\(\beta = (\beta_1, \ldots, \beta_q)\)</span> is given by
<span class="math display">\[\begin{equation*}
  \hat\beta
  = (X^\top X)^{-1} X^\top y
\end{equation*}\]</span>
where <span class="math inline">\(X \in \mathbb{R}^{n\times q}\)</span>,
and our estimate for the error variance <span class="math inline">\(\sigma^2\)</span> is
<span class="math display">\[\begin{equation*}
  \hat\sigma^2
  = \frac{1}{n-q} y^\top (I - H) y,
\end{equation*}\]</span>
where <span class="math inline">\(H = X (X^\top X)^{-1} X^\top\)</span> is the hat matrix computed
without using <span class="math inline">\(x_{q+1}, \ldots, x_p\)</span>.</p>
<p>If we write <span class="math inline">\(\tilde X \in \mathbb{R}^{n\times (p-q)}\)</span> for the matrix with columns
<span class="math inline">\(x_{q+1}, \ldots, x_p\)</span> and <span class="math inline">\(\tilde\beta = (\beta_{q+1}, \ldots, \beta_p) \in
\mathbb{R}^{p-q}\)</span>, then the full model can be written as
<span class="math display">\[\begin{equation*}
  Y
  = X \beta + \tilde X \tilde\beta + \varepsilon
\end{equation*}\]</span>
and we get
<span class="math display">\[\begin{align*}
  \hat\beta
  &amp;= (X^\top X)^{-1} X^\top Y \\
  &amp;= (X^\top X)^{-1} X^\top (X \beta + \tilde X \tilde\beta + \varepsilon) \\
  &amp;= (X^\top X)^{-1} X^\top X \beta + (X^\top X)^{-1} X^\top \tilde X \tilde\beta + (X^\top X)^{-1} X^\top \varepsilon\\
  &amp;= \beta + (X^\top X)^{-1} X^\top \tilde X \tilde\beta + (X^\top X)^{-1} X^\top \varepsilon.
\end{align*}\]</span>
Thus, we have
<span class="math display">\[\begin{equation*}
  \mathbb{E}(\hat\beta)
  = \beta + (X^\top X)^{-1} X^\top \tilde X \tilde\beta
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{bias}}(\hat\beta)
  = (X^\top X)^{-1} X^\top \tilde X \tilde\beta.
\end{equation*}\]</span>
This shows that now the estimate is biased in general. There are two
special cases when omitting variables does not introduce a bias:
The first is when all of the omitted coefficients equal zero, <em>i.e.</em> when
we have <span class="math inline">\(\tilde\beta = 0\)</span>. The second case is when the omitted inputs are
orthogonal to the included inputs, since then we have <span class="math inline">\(X^\top \tilde X = 0\)</span>.</p>
<p>Using the formula for <span class="math inline">\(\hat\beta\)</span> we find
<span class="math display">\[\begin{align*}
  \mathop{\mathrm{Cov}}( \hat\beta )
  &amp;= \mathop{\mathrm{Cov}}\Bigl( \beta + (X^\top X)^{-1} X^\top \tilde X \tilde\beta + (X^\top X)^{-1} X^\top \varepsilon\Bigr) \\
  &amp;= \mathop{\mathrm{Cov}}\Bigl( (X^\top X)^{-1} X^\top \varepsilon\Bigr) \\
  &amp;= \sigma^2 (X^\top X)^{-1} X^\top I X (X^\top X)^{-1} \\
  &amp;= \sigma^2 (X^\top X)^{-1}.
\end{align*}\]</span>
Thus, the variance of <span class="math inline">\(\hat\beta\)</span> is not affected by the omitted variables.
Similar to our derivation in the section <a href="S04-model.html#var-est-bias">Estimating the Error Variance</a>
one can show that
<span class="math display">\[\begin{equation*}
  \mathbb{E}\bigl( \hat\sigma^2 \bigr)
  = \sigma^2 + \frac{\tilde\beta^\top \tilde X^\top (I - H) \tilde X \tilde \beta}{n-q}.
\end{equation*}\]</span>
Thus, the estimate of the error variance is in general also biased.</p>
<p>This shows that our estimates can become biased if some of the inputs
are missing from our model. As in the previous section, it may happen
that the MSE of the parameter estimates in the reduced model
is smaller than the MSE in the correct model; this is the case if the
variance of the estimates decreases enough to compensate for the
introduced bias.</p>
</div>
<div id="unnecessary-variables" class="section level3 hasAnchor" number="12.2.2">
<h3><span class="header-section-number">12.2.2</span> Unnecessary Variables<a href="S12-selection.html#unnecessary-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If the data comes from a model with fewer inputs than the ones we are using,
the model is not “misspecified” in the strict sense. It is just the case that
some of the true <span class="math inline">\(\beta_j\)</span> exactly equal zero. In this case, our previous
results show that the least squares estimate is unbiased.</p>
<p>Including additional variables into a model still can cause problems:
One can show that each unnecessary variable added increases the variance of the
estimates <span class="math inline">\(\hat\beta_j\)</span>. Instead of giving a proof of this fact,
we illustrate the effect using a numerical example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-49" class="example"><strong>Example 12.1  </strong></span>Here we consider the <code>stackloss</code> dataset with an added column of
noise. We have seen that
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{Var}}( \hat\beta_j )
  = \sigma^2 \bigl( X^\top X \bigr)^{-1}_{jj}.
\end{equation*}\]</span>
While we don’t know the true value of <span class="math inline">\(\sigma^2\)</span>, we can determine
the relative change in variance when adding a column, since this process
does not change <span class="math inline">\(\sigma^2\)</span> and thus <span class="math inline">\(\sigma^2\)</span> will cancel when we
determine the relative change in variance.</p>
<p>We first compute the diagonal elements of <span class="math inline">\(\bigl( X^\top X \bigr)^{-1}\)</span>
for the original dataset:</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="S12-selection.html#cb175-1" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(stack.loss <span class="sc">~</span> ., <span class="at">data =</span> stackloss)</span>
<span id="cb175-2"><a href="S12-selection.html#cb175-2" tabindex="-1"></a>d1 <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X1) <span class="sc">%*%</span> X1))</span>
<span id="cb175-3"><a href="S12-selection.html#cb175-3" tabindex="-1"></a>d1</span></code></pre></div>
<pre class="routput"><code> (Intercept)     Air.Flow   Water.Temp   Acid.Conc. 
13.452726695  0.001728874  0.012875424  0.002322167 </code></pre>
<p>Now we add a column of noise and re-compute the values:</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="S12-selection.html#cb177-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">20211115</span>)</span>
<span id="cb177-2"><a href="S12-selection.html#cb177-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X1)</span>
<span id="cb177-3"><a href="S12-selection.html#cb177-3" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">cbind</span>(X1, <span class="fu">rnorm</span>(n))</span>
<span id="cb177-4"><a href="S12-selection.html#cb177-4" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X2) <span class="sc">%*%</span> X2))</span>
<span id="cb177-5"><a href="S12-selection.html#cb177-5" tabindex="-1"></a>d2</span></code></pre></div>
<pre class="routput"><code> (Intercept)     Air.Flow   Water.Temp   Acid.Conc.              
14.397515774  0.001730570  0.015195467  0.002796487  0.064828744 </code></pre>
<p>Finally, we determine the change in variance in percent:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="S12-selection.html#cb179-1" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span> <span class="sc">*</span> (d2[<span class="sc">-</span><span class="dv">5</span>] <span class="sc">-</span> d1) <span class="sc">/</span> d1, <span class="dv">3</span>)</span></code></pre></div>
<pre class="routput"><code>(Intercept)    Air.Flow  Water.Temp  Acid.Conc. 
      7.023       0.098      18.019      20.426 </code></pre>
<p>We see that the variances of the <span class="math inline">\(\hat\beta_j\)</span> increased by up to <span class="math inline">\(20\%\)</span>
when we added the unnecessary input.</p>
</div>
<p>The example illustrates that it is important to keep the model as small
as possible.</p>
</div>
</div>
<div id="choosing-between-models" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> Choosing Between Models<a href="S12-selection.html#choosing-between-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="criteria-for-model-selection" class="section level3 hasAnchor" number="12.3.1">
<h3><span class="header-section-number">12.3.1</span> Criteria for Model Selection<a href="S12-selection.html#criteria-for-model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When comparing different models, we need a criterion to decide which
is “best”. Several options exist:</p>
<ul>
<li><p><strong><span class="math inline">\(R^2\)</span></strong> (coefficient of determination): Measures the proportion of
variance explained by the model. However, <span class="math inline">\(R^2\)</span> always increases when
variables are added, even if they are irrelevant. This makes it
unsuitable for model selection.</p></li>
<li><p><strong>Adjusted <span class="math inline">\(R^2\)</span></strong> (written <span class="math inline">\(R^2_\mathrm{adj}\)</span>): Adjusts <span class="math inline">\(R^2\)</span> to
penalize model complexity. This is the criterion we will use
throughout this section and the next.</p></li>
<li><p><strong>AIC</strong> (Akaike Information Criterion) and <strong>BIC</strong> (Bayesian
Information Criterion): Alternative criteria that also penalize
model complexity, discussed in section <a href="S10-diagnostics.html#S10-diagnostics">10</a>.</p></li>
</ul>
<p>In the following subsections, we describe algorithms that use these
criteria to select models efficiently.</p>
</div>
<div id="exhaustive-search" class="section level3 hasAnchor" number="12.3.2">
<h3><span class="header-section-number">12.3.2</span> Exhaustive Search<a href="S12-selection.html#exhaustive-search" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a model with <span class="math inline">\(p\)</span> input variables, there are <span class="math inline">\(2^p\)</span> different choices for
which variables to include in the model (or <span class="math inline">\(2^{p+1}\)</span>, if we also have to
decide whether to include the intercept). This value increases quickly with
<span class="math inline">\(p\)</span>: for <span class="math inline">\(p = 10\)</span> we have <span class="math inline">\(1024\)</span> models to consider, for <span class="math inline">\(p = 20\)</span> there are
<span class="math inline">\(1048576\)</span> possible models, and for large <span class="math inline">\(p\)</span> it becomes infeasible to simply
try all possible models to find the best one.</p>
<p>If <span class="math inline">\(p\)</span> is small enough, we can simply fit all <span class="math inline">\(2^p\)</span> models and choose the one
with the best performance (<em>e.g.</em>, highest <span class="math inline">\(R^2_\mathrm{adj}\)</span>). This guarantees
finding the optimal model. However, for <span class="math inline">\(p \geq 20\)</span> this becomes
computationally infeasible (<span class="math inline">\(2^{20} &gt; 1\)</span> million models).</p>
</div>
<div id="stepwise-forward-selection" class="section level3 hasAnchor" number="12.3.3">
<h3><span class="header-section-number">12.3.3</span> Stepwise Forward Selection<a href="S12-selection.html#stepwise-forward-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here the idea is to start with a minimal model, and then add variables one by
one, until no further (large enough) improvements can be achieved.</p>
<p>Start with only intercept term: <span class="math inline">\(y=\beta_0 + \varepsilon\)</span>, then
consider each of <span class="math inline">\(p\)</span> models:
<span class="math display">\[\begin{equation*}
  y
  = \beta_0 + \beta_j x_j + \varepsilon,
\end{equation*}\]</span>
for <span class="math inline">\(j \in \{1, \ldots, p\}\)</span>.</p>
<p>Choose the model with the smallest residual sum of squares, provided that the
“significance of the fitted model” achieves a specified threshold. The process
continues by adding more variables, one at a time, until either</p>
<ul>
<li>All variables are in the model.</li>
<li>The significance level can not be achieved by any variable not in the model.</li>
</ul>
<p>The “significance of the model” can be examined by considering a
<span class="math inline">\(t\)</span>-test as in lemma <a href="S05-single.html#lem:t-test">5.4</a>.</p>
</div>
<div id="stepwise-backward-selection" class="section level3 hasAnchor" number="12.3.4">
<h3><span class="header-section-number">12.3.4</span> Stepwise Backward Selection<a href="S12-selection.html#stepwise-backward-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here the idea is to start with the full model, and to remove variables
one by one until no good enough candidates for removal are left.</p>
<p>In each step we consider the test statistic <span class="math inline">\(|T_j|\)</span> for the tests
<span class="math display">\[\begin{equation*}
  H_0\colon \beta_j = 0
  \quad \textit{vs.} \quad
  H_1\colon \beta_j \neq 0
\end{equation*}\]</span>
for <span class="math inline">\(j \in \{1, \ldots, p\}\)</span>, again as in lemma <a href="S05-single.html#lem:t-test">5.4</a>.
The method selects <span class="math inline">\(x_j\)</span> corresponding to the smallest <span class="math inline">\(|T_j|\)</span>.
If this is below a given threshold, then remove <span class="math inline">\(x_j\)</span> and re-fit the model.
Repeat until either:</p>
<ul>
<li>No variables are left in the model.</li>
<li>The smallest <span class="math inline">\(|T_j|\)</span> is above the threshold.</li>
</ul>
</div>
<div id="hybrid-methods" class="section level3 hasAnchor" number="12.3.5">
<h3><span class="header-section-number">12.3.5</span> Hybrid Methods<a href="S12-selection.html#hybrid-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Either start with a full model or just intercept. At each stage consider:</p>
<ul>
<li>removing least significant variable already in the model,</li>
<li>adding most significant variable not currently in the model,</li>
</ul>
<p>with significance levels set to avoid a cyclical behaviour.</p>
<div class="mysummary">
<p><strong>Summary</strong></p>
<ul>
<li>Model selection requires choosing a criterion such as <span class="math inline">\(R^2_\mathrm{adj}\)</span>,
AIC, or BIC to compare models.</li>
<li>Stepwise methods (forward, backward, hybrid) provide computationally
efficient heuristics for large <span class="math inline">\(p\)</span>.</li>
<li>These methods do not guarantee finding the optimal model.</li>
<li>Exhaustive search can find the optimal model when <span class="math inline">\(p\)</span> is small to moderate.</li>
</ul>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="P03.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="S13-exhaustive.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": null,
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["MATH3714.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
