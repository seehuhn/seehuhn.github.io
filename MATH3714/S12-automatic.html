<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 12 Automatic Model Selection | MATH3714 Linear Regression and Robustness</title>
  <meta name="description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2023/24" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 12 Automatic Model Selection | MATH3714 Linear Regression and Robustness" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2023/24" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 12 Automatic Model Selection | MATH3714 Linear Regression and Robustness" />
  
  <meta name="twitter:description" content="Lecture notes for the course MATH3714 Linear Regression and Robustness at the University of Leeds, 2023/24" />
  

<meta name="author" content="Jochen Voss" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="I03-lm-output.html"/>
<link rel="next" href="S13-factors.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P96L0SF56N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-P96L0SF56N');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="jvstyle.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">MATH3714 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html"><i class="fa fa-check"></i>About MATH3714</a>
<ul>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#notes"><i class="fa fa-check"></i>Notes and videos</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#lectures"><i class="fa fa-check"></i>Lectures</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#workshops"><i class="fa fa-check"></i>Workshops and Problem Sheets</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="S00-about.html"><a href="S00-about.html#assessments"><i class="fa fa-check"></i>Assessments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="S01-simple.html"><a href="S01-simple.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="1.1" data-path="S01-simple.html"><a href="S01-simple.html#residual-sum-of-squares"><i class="fa fa-check"></i><b>1.1</b> Residual Sum of Squares</a></li>
<li class="chapter" data-level="1.2" data-path="S01-simple.html"><a href="S01-simple.html#linear-regression-as-a-parameter-estimation-problem"><i class="fa fa-check"></i><b>1.2</b> Linear Regression as a Parameter Estimation Problem</a></li>
<li class="chapter" data-level="1.3" data-path="S01-simple.html"><a href="S01-simple.html#sec:simple-mat"><i class="fa fa-check"></i><b>1.3</b> Matrix Notation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="S02-multiple.html"><a href="S02-multiple.html"><i class="fa fa-check"></i><b>2</b> Least Squares Estimates</a>
<ul>
<li class="chapter" data-level="2.1" data-path="S02-multiple.html"><a href="S02-multiple.html#data-and-models"><i class="fa fa-check"></i><b>2.1</b> Data and Models</a></li>
<li class="chapter" data-level="2.2" data-path="S02-multiple.html"><a href="S02-multiple.html#the-normal-equations"><i class="fa fa-check"></i><b>2.2</b> The Normal Equations</a></li>
<li class="chapter" data-level="2.3" data-path="S02-multiple.html"><a href="S02-multiple.html#fitted-values"><i class="fa fa-check"></i><b>2.3</b> Fitted Values</a></li>
<li class="chapter" data-level="2.4" data-path="S02-multiple.html"><a href="S02-multiple.html#models-without-intercept"><i class="fa fa-check"></i><b>2.4</b> Models Without Intercept</a></li>
<li class="chapter" data-level="2.5" data-path="S02-multiple.html"><a href="S02-multiple.html#example"><i class="fa fa-check"></i><b>2.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html"><i class="fa fa-check"></i>Interlude: Linear Regression in R</a>
<ul>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-fitting"><i class="fa fa-check"></i>Fitting a Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-model"><i class="fa fa-check"></i>Understanding the Model</a></li>
<li class="chapter" data-level="" data-path="I01-lm.html"><a href="I01-lm.html#lm-predict"><i class="fa fa-check"></i>Making Predictions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P01.html"><a href="P01.html"><i class="fa fa-check"></i>Problem Sheet 1</a></li>
<li class="chapter" data-level="3" data-path="S03-cov.html"><a href="S03-cov.html"><i class="fa fa-check"></i><b>3</b> Random Vectors and Covariance</a>
<ul>
<li class="chapter" data-level="3.1" data-path="S03-cov.html"><a href="S03-cov.html#expectation"><i class="fa fa-check"></i><b>3.1</b> Expectation</a></li>
<li class="chapter" data-level="3.2" data-path="S03-cov.html"><a href="S03-cov.html#sec:covariance"><i class="fa fa-check"></i><b>3.2</b> Covariance Matrix</a></li>
<li class="chapter" data-level="3.3" data-path="S03-cov.html"><a href="S03-cov.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>3.3</b> The Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="S04-model.html"><a href="S04-model.html"><i class="fa fa-check"></i><b>4</b> Properties of the Least Squares Estimate</a>
<ul>
<li class="chapter" data-level="4.1" data-path="S04-model.html"><a href="S04-model.html#mean-and-covariance"><i class="fa fa-check"></i><b>4.1</b> Mean and Covariance</a></li>
<li class="chapter" data-level="4.2" data-path="S04-model.html"><a href="S04-model.html#hat-matrix"><i class="fa fa-check"></i><b>4.2</b> Properties of the Hat Matrix</a></li>
<li class="chapter" data-level="4.3" data-path="S04-model.html"><a href="S04-model.html#Cochran"><i class="fa fa-check"></i><b>4.3</b> Cochran’s theorem</a></li>
<li class="chapter" data-level="4.4" data-path="S04-model.html"><a href="S04-model.html#var-est-bias"><i class="fa fa-check"></i><b>4.4</b> Estimating the Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="S05-single.html"><a href="S05-single.html"><i class="fa fa-check"></i><b>5</b> Uncertainty for Individual Regression Coefficients</a>
<ul>
<li class="chapter" data-level="5.1" data-path="S05-single.html"><a href="S05-single.html#measuring-the-estimation-error"><i class="fa fa-check"></i><b>5.1</b> Measuring the Estimation Error</a></li>
<li class="chapter" data-level="5.2" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals"><i class="fa fa-check"></i><b>5.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.3" data-path="S05-single.html"><a href="S05-single.html#hypthesis-tests"><i class="fa fa-check"></i><b>5.3</b> Hypthesis Tests</a></li>
<li class="chapter" data-level="5.4" data-path="S05-single.html"><a href="S05-single.html#r-experiments"><i class="fa fa-check"></i><b>5.4</b> R Experiments</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="S05-single.html"><a href="S05-single.html#fitting-the-model"><i class="fa fa-check"></i><b>5.4.1</b> Fitting the model</a></li>
<li class="chapter" data-level="5.4.2" data-path="S05-single.html"><a href="S05-single.html#estimating-the-variance-of-the-error"><i class="fa fa-check"></i><b>5.4.2</b> Estimating the Variance of the Error</a></li>
<li class="chapter" data-level="5.4.3" data-path="S05-single.html"><a href="S05-single.html#estimating-the-standard-errors"><i class="fa fa-check"></i><b>5.4.3</b> Estimating the Standard Errors</a></li>
<li class="chapter" data-level="5.4.4" data-path="S05-single.html"><a href="S05-single.html#hypothesis-tests"><i class="fa fa-check"></i><b>5.4.4</b> Hypothesis tests</a></li>
<li class="chapter" data-level="5.4.5" data-path="S05-single.html"><a href="S05-single.html#confidence-intervals-1"><i class="fa fa-check"></i><b>5.4.5</b> Confidence Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html"><i class="fa fa-check"></i><b>6</b> Estimating Coefficients Simultaneously</a>
<ul>
<li class="chapter" data-level="6.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-dist"><i class="fa fa-check"></i><b>6.1</b> Linear Combinations of Coefficients</a></li>
<li class="chapter" data-level="6.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-CI"><i class="fa fa-check"></i><b>6.2</b> Confidence Regions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#result"><i class="fa fa-check"></i><b>6.2.1</b> Result</a></li>
<li class="chapter" data-level="6.2.2" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#numerical-experiments"><i class="fa fa-check"></i><b>6.2.2</b> Numerical Experiments</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="S06-simultaneous.html"><a href="S06-simultaneous.html#sec:simult-test"><i class="fa fa-check"></i><b>6.3</b> Hypothesis Tests</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html"><i class="fa fa-check"></i>Interlude: Loading Data into R</a>
<ul>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-csv-files"><i class="fa fa-check"></i>Importing CSV Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#importing-microsoft-excel-files"><i class="fa fa-check"></i>Importing Microsoft Excel Files</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#checking-the-imported-data"><i class="fa fa-check"></i>Checking the Imported Data</a></li>
<li class="chapter" data-level="" data-path="I02-read.html"><a href="I02-read.html#common-problems"><i class="fa fa-check"></i>Common Problems</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P02.html"><a href="P02.html"><i class="fa fa-check"></i>Problem Sheet 2</a></li>
<li class="chapter" data-level="7" data-path="S07-examples.html"><a href="S07-examples.html"><i class="fa fa-check"></i><b>7</b> Examples</a>
<ul>
<li class="chapter" data-level="7.1" data-path="S07-examples.html"><a href="S07-examples.html#simple-confidence-interval"><i class="fa fa-check"></i><b>7.1</b> Simple Confidence Interval</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles"><i class="fa fa-check"></i><b>7.1.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.1.2" data-path="S07-examples.html"><a href="S07-examples.html#from-the-lm-output"><i class="fa fa-check"></i><b>7.1.2</b> From the <code>lm()</code> Output</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="S07-examples.html"><a href="S07-examples.html#confidence-intervals-for-the-mean"><i class="fa fa-check"></i><b>7.2</b> Confidence Intervals for the Mean</a></li>
<li class="chapter" data-level="7.3" data-path="S07-examples.html"><a href="S07-examples.html#testing-a-single-coefficient"><i class="fa fa-check"></i><b>7.3</b> Testing a Single Coefficient</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-1"><i class="fa fa-check"></i><b>7.3.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.3.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-i"><i class="fa fa-check"></i><b>7.3.2</b> Using the <code>lm()</code> Output, I</a></li>
<li class="chapter" data-level="7.3.3" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output-ii"><i class="fa fa-check"></i><b>7.3.3</b> Using the <code>lm()</code> Output, II</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="S07-examples.html"><a href="S07-examples.html#testing-multiple-coefficents"><i class="fa fa-check"></i><b>7.4</b> Testing Multiple Coefficents</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="S07-examples.html"><a href="S07-examples.html#from-first-principles-2"><i class="fa fa-check"></i><b>7.4.1</b> From First Principles</a></li>
<li class="chapter" data-level="7.4.2" data-path="S07-examples.html"><a href="S07-examples.html#using-the-lm-output"><i class="fa fa-check"></i><b>7.4.2</b> Using the <code>lm()</code> output</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="S08-influence.html"><a href="S08-influence.html"><i class="fa fa-check"></i><b>8</b> The Influence of Observations</a>
<ul>
<li class="chapter" data-level="8.1" data-path="S08-influence.html"><a href="S08-influence.html#deleting"><i class="fa fa-check"></i><b>8.1</b> Deleting Observations</a></li>
<li class="chapter" data-level="8.2" data-path="S08-influence.html"><a href="S08-influence.html#influence"><i class="fa fa-check"></i><b>8.2</b> Cook’s Distance</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="S09-plots.html"><a href="S09-plots.html"><i class="fa fa-check"></i><b>9</b> Diagnostic Plots</a></li>
<li class="chapter" data-level="10" data-path="S10-improving.html"><a href="S10-improving.html"><i class="fa fa-check"></i><b>10</b> Improving the Model Fit</a>
<ul>
<li class="chapter" data-level="10.1" data-path="S10-improving.html"><a href="S10-improving.html#linearising-the-mean"><i class="fa fa-check"></i><b>10.1</b> Linearising the Mean</a></li>
<li class="chapter" data-level="10.2" data-path="S10-improving.html"><a href="S10-improving.html#stabilising-the-variance"><i class="fa fa-check"></i><b>10.2</b> Stabilising the Variance</a></li>
<li class="chapter" data-level="10.3" data-path="S10-improving.html"><a href="S10-improving.html#power-transform"><i class="fa fa-check"></i><b>10.3</b> The Power Transform</a></li>
<li class="chapter" data-level="10.4" data-path="S10-improving.html"><a href="S10-improving.html#candidates-models"><i class="fa fa-check"></i><b>10.4</b> Candidates Models</a></li>
<li class="chapter" data-level="10.5" data-path="S10-improving.html"><a href="S10-improving.html#misspecified-models"><i class="fa fa-check"></i><b>10.5</b> Misspecified Models</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="S10-improving.html"><a href="S10-improving.html#missing-variables"><i class="fa fa-check"></i><b>10.5.1</b> Missing Variables</a></li>
<li class="chapter" data-level="10.5.2" data-path="S10-improving.html"><a href="S10-improving.html#unnecessary-variables"><i class="fa fa-check"></i><b>10.5.2</b> Unnecessary Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="P03.html"><a href="P03.html"><i class="fa fa-check"></i>Problem Sheet 3</a></li>
<li class="chapter" data-level="11" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html"><i class="fa fa-check"></i><b>11</b> Measures for Model Fit</a>
<ul>
<li class="chapter" data-level="11.1" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html#R-squared"><i class="fa fa-check"></i><b>11.1</b> The Coefficient of Multiple Determination</a></li>
<li class="chapter" data-level="11.2" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html#error-var"><i class="fa fa-check"></i><b>11.2</b> Error Variance</a></li>
<li class="chapter" data-level="11.3" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html#PRESS"><i class="fa fa-check"></i><b>11.3</b> Prediction Error Sum of Squares</a></li>
<li class="chapter" data-level="11.4" data-path="S11-diagnostics.html"><a href="S11-diagnostics.html#AIC"><i class="fa fa-check"></i><b>11.4</b> Akaike’s Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I03-lm-output.html"><a href="I03-lm-output.html"><i class="fa fa-check"></i>Interlude: Understanding the <code>lm()</code> Output</a></li>
<li class="chapter" data-level="12" data-path="S12-automatic.html"><a href="S12-automatic.html"><i class="fa fa-check"></i><b>12</b> Automatic Model Selection</a>
<ul>
<li class="chapter" data-level="12.1" data-path="S12-automatic.html"><a href="S12-automatic.html#exhaustive-search"><i class="fa fa-check"></i><b>12.1</b> Exhaustive Search</a></li>
<li class="chapter" data-level="12.2" data-path="S12-automatic.html"><a href="S12-automatic.html#search-algorithm"><i class="fa fa-check"></i><b>12.2</b> Search Algorithm</a></li>
<li class="chapter" data-level="12.3" data-path="S12-automatic.html"><a href="S12-automatic.html#other-methods"><i class="fa fa-check"></i><b>12.3</b> Other Methods</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="S12-automatic.html"><a href="S12-automatic.html#stepwise-forward-selection"><i class="fa fa-check"></i><b>12.3.1</b> Stepwise Forward Selection</a></li>
<li class="chapter" data-level="12.3.2" data-path="S12-automatic.html"><a href="S12-automatic.html#stepwise-backward-selection"><i class="fa fa-check"></i><b>12.3.2</b> Stepwise Backward Selection</a></li>
<li class="chapter" data-level="12.3.3" data-path="S12-automatic.html"><a href="S12-automatic.html#hybrid-methods"><i class="fa fa-check"></i><b>12.3.3</b> Hybrid Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="S13-factors.html"><a href="S13-factors.html"><i class="fa fa-check"></i><b>13</b> Factors</a>
<ul>
<li class="chapter" data-level="13.1" data-path="S13-factors.html"><a href="S13-factors.html#indicator-variables"><i class="fa fa-check"></i><b>13.1</b> Indicator Variables</a></li>
<li class="chapter" data-level="13.2" data-path="S13-factors.html"><a href="S13-factors.html#interactions"><i class="fa fa-check"></i><b>13.2</b> Interactions</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="S14-examples.html"><a href="S14-examples.html"><i class="fa fa-check"></i><b>14</b> Examples</a>
<ul>
<li class="chapter" data-level="14.1" data-path="S14-examples.html"><a href="S14-examples.html#interactions-example"><i class="fa fa-check"></i><b>14.1</b> Use of Interaction Terms in Modelling</a></li>
<li class="chapter" data-level="14.2" data-path="S14-examples.html"><a href="S14-examples.html#codings"><i class="fa fa-check"></i><b>14.2</b> Alternative Factor Codings</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P04.html"><a href="P04.html"><i class="fa fa-check"></i>Problem Sheet 4</a></li>
<li class="chapter" data-level="15" data-path="S15-multicoll.html"><a href="S15-multicoll.html"><i class="fa fa-check"></i><b>15</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="15.1" data-path="S15-multicoll.html"><a href="S15-multicoll.html#consequences-of-multicollinearity"><i class="fa fa-check"></i><b>15.1</b> Consequences of Multicollinearity</a></li>
<li class="chapter" data-level="15.2" data-path="S15-multicoll.html"><a href="S15-multicoll.html#detecting-multicollinearity"><i class="fa fa-check"></i><b>15.2</b> Detecting Multicollinearity</a></li>
<li class="chapter" data-level="15.3" data-path="S15-multicoll.html"><a href="S15-multicoll.html#mitigations"><i class="fa fa-check"></i><b>15.3</b> Mitigations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html"><i class="fa fa-check"></i>Practical</a>
<ul>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#dataset"><i class="fa fa-check"></i>Dataset</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#tasks"><i class="fa fa-check"></i>Tasks</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="S16-ridge.html"><a href="S16-ridge.html"><i class="fa fa-check"></i><b>16</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="16.1" data-path="S16-ridge.html"><a href="S16-ridge.html#definition-the-estimator"><i class="fa fa-check"></i><b>16.1</b> Definition the Estimator</a></li>
<li class="chapter" data-level="16.2" data-path="S16-ridge.html"><a href="S16-ridge.html#properties-of-the-estimate"><i class="fa fa-check"></i><b>16.2</b> Properties of the Estimate</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="S16-ridge.html"><a href="S16-ridge.html#bias"><i class="fa fa-check"></i><b>16.2.1</b> Bias</a></li>
<li class="chapter" data-level="16.2.2" data-path="S16-ridge.html"><a href="S16-ridge.html#variance"><i class="fa fa-check"></i><b>16.2.2</b> Variance</a></li>
<li class="chapter" data-level="16.2.3" data-path="S16-ridge.html"><a href="S16-ridge.html#mean-squared-error"><i class="fa fa-check"></i><b>16.2.3</b> Mean Squared Error</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="S16-ridge.html"><a href="S16-ridge.html#standardisation"><i class="fa fa-check"></i><b>16.3</b> Standardisation</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="S17-robust.html"><a href="S17-robust.html"><i class="fa fa-check"></i><b>17</b> Robust Regression</a>
<ul>
<li class="chapter" data-level="17.1" data-path="S17-robust.html"><a href="S17-robust.html#outliers"><i class="fa fa-check"></i><b>17.1</b> Outliers</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="S17-robust.html"><a href="S17-robust.html#leverage"><i class="fa fa-check"></i><b>17.1.1</b> Leverage</a></li>
<li class="chapter" data-level="17.1.2" data-path="S17-robust.html"><a href="S17-robust.html#studentised-residuals"><i class="fa fa-check"></i><b>17.1.2</b> Studentised Residuals</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="S17-robust.html"><a href="S17-robust.html#breakdown-points"><i class="fa fa-check"></i><b>17.2</b> Breakdown Points</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P05.html"><a href="P05.html"><i class="fa fa-check"></i>Problem Sheet 5</a></li>
<li class="chapter" data-level="18" data-path="S18-m-est.html"><a href="S18-m-est.html"><i class="fa fa-check"></i><b>18</b> M-Estimators</a>
<ul>
<li class="chapter" data-level="18.1" data-path="S18-m-est.html"><a href="S18-m-est.html#definition"><i class="fa fa-check"></i><b>18.1</b> Definition</a></li>
<li class="chapter" data-level="18.2" data-path="S18-m-est.html"><a href="S18-m-est.html#iterative-methods"><i class="fa fa-check"></i><b>18.2</b> Iterative Methods</a></li>
<li class="chapter" data-level="18.3" data-path="S18-m-est.html"><a href="S18-m-est.html#objective-functions"><i class="fa fa-check"></i><b>18.3</b> Objective Functions</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="S18-m-est.html"><a href="S18-m-est.html#least-squares-method"><i class="fa fa-check"></i><b>18.3.1</b> Least Squares Method</a></li>
<li class="chapter" data-level="18.3.2" data-path="S18-m-est.html"><a href="S18-m-est.html#least-absolute-values"><i class="fa fa-check"></i><b>18.3.2</b> Least Absolute Values</a></li>
<li class="chapter" data-level="18.3.3" data-path="S18-m-est.html"><a href="S18-m-est.html#Huber"><i class="fa fa-check"></i><b>18.3.3</b> Huber’s <span class="math inline">\(t\)</span>-function</a></li>
<li class="chapter" data-level="18.3.4" data-path="S18-m-est.html"><a href="S18-m-est.html#hampels-method"><i class="fa fa-check"></i><b>18.3.4</b> Hampel’s Method</a></li>
<li class="chapter" data-level="18.3.5" data-path="S18-m-est.html"><a href="S18-m-est.html#tukeys-bisquare-method"><i class="fa fa-check"></i><b>18.3.5</b> Tukey’s Bisquare Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="S19-efficiency.html"><a href="S19-efficiency.html"><i class="fa fa-check"></i><b>19</b> Efficiency of Robust Estimators</a>
<ul>
<li class="chapter" data-level="19.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#efficiency"><i class="fa fa-check"></i><b>19.1</b> Efficiency</a></li>
<li class="chapter" data-level="19.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#robust-estimators"><i class="fa fa-check"></i><b>19.2</b> Robust estimators</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="S19-efficiency.html"><a href="S19-efficiency.html#least-median-of-squares"><i class="fa fa-check"></i><b>19.2.1</b> Least Median of Squares</a></li>
<li class="chapter" data-level="19.2.2" data-path="S19-efficiency.html"><a href="S19-efficiency.html#least-trimmed-squares"><i class="fa fa-check"></i><b>19.2.2</b> Least Trimmed Squares</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html"><i class="fa fa-check"></i><b>A</b> Linear Algebra Reminders</a>
<ul>
<li class="chapter" data-level="A.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#vectors"><i class="fa fa-check"></i><b>A.1</b> Vectors</a></li>
<li class="chapter" data-level="A.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-rules"><i class="fa fa-check"></i><b>A.2</b> Matrices</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#transpose"><i class="fa fa-check"></i><b>A.2.1</b> Transpose</a></li>
<li class="chapter" data-level="A.2.2" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-vector-product"><i class="fa fa-check"></i><b>A.2.2</b> Matrix-vector Product</a></li>
<li class="chapter" data-level="A.2.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-matrix-product"><i class="fa fa-check"></i><b>A.2.3</b> Matrix-matrix Product</a></li>
<li class="chapter" data-level="A.2.4" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#rank"><i class="fa fa-check"></i><b>A.2.4</b> Rank</a></li>
<li class="chapter" data-level="A.2.5" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#trace"><i class="fa fa-check"></i><b>A.2.5</b> Trace</a></li>
<li class="chapter" data-level="A.2.6" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#matrix-inverse"><i class="fa fa-check"></i><b>A.2.6</b> Matrix Inverse</a></li>
<li class="chapter" data-level="A.2.7" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#orthogonal-matrices"><i class="fa fa-check"></i><b>A.2.7</b> Orthogonal Matrices</a></li>
<li class="chapter" data-level="A.2.8" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#positive-definite"><i class="fa fa-check"></i><b>A.2.8</b> Positive Definite Matrices</a></li>
<li class="chapter" data-level="A.2.9" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#idempotent"><i class="fa fa-check"></i><b>A.2.9</b> Idempotent Matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="Sx1-matrices.html"><a href="Sx1-matrices.html#eigenvalues"><i class="fa fa-check"></i><b>A.3</b> Eigenvalues</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="Sx2-probability.html"><a href="Sx2-probability.html"><i class="fa fa-check"></i><b>B</b> Probability Reminders</a>
<ul>
<li class="chapter" data-level="B.1" data-path="Sx2-probability.html"><a href="Sx2-probability.html#independence"><i class="fa fa-check"></i><b>B.1</b> Independence</a></li>
<li class="chapter" data-level="B.2" data-path="Sx2-probability.html"><a href="Sx2-probability.html#chi-square"><i class="fa fa-check"></i><b>B.2</b> The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="B.3" data-path="Sx2-probability.html"><a href="Sx2-probability.html#t"><i class="fa fa-check"></i><b>B.3</b> The t-distribution</a></li>
</ul></li>
<li class="divider"></li>
<li class="chapter"><span><b>THE END</b></span></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3714 Linear Regression and Robustness</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="S12-automatic" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">Section 12</span> Automatic Model Selection<a href="S12-automatic.html#S12-automatic" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!--
* explain the `nvmax` argument to `regsubsets()`
-->
<p>For a model with <span class="math inline">\(p\)</span> input variables, there are
<span class="math inline">\(2^p\)</span> different choices for which variables to include in the
model (or <span class="math inline">\(2^{p+1}\)</span>, if we also have to decide whether to include the intercept).
This value increases quickly with <span class="math inline">\(p\)</span>: for <span class="math inline">\(p = 10\)</span> we
have <span class="math inline">\(1024\)</span> models to consider, for <span class="math inline">\(p = 20\)</span> there are <span class="math inline">\(1048576\)</span> possible
models, and for large <span class="math inline">\(p\)</span> it becomes infeasible to simply try all possible
models to find the best one. In this section we consider algorithms
for automatically finding a good selection inputs when <span class="math inline">\(p\)</span> is large.</p>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/pOWYr9RiZWU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<div id="exhaustive-search" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> Exhaustive Search<a href="S12-automatic.html#exhaustive-search" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Efficient algorithms are available to find the best model out of the
<span class="math inline">\(2^p\)</span> possible models for small to moderate <span class="math inline">\(p\)</span>. Here we present
a method which can often find the model with the largest <span class="math inline">\(R^2_\mathrm{adj}\)</span>
without having to fit all <span class="math inline">\(2^p\)</span> models. We consider the least squares
estimate throughout.</p>
<p>We can characterise each possible model by the set <span class="math inline">\(J \subseteq \{1, \ldots p\}\)</span> of variables included in the model. The model includes variable <span class="math inline">\(x_j\)</span>, if
and only if <span class="math inline">\(j \in J\)</span>. Here we assume that the intercept is always included,
so that <span class="math inline">\(J=\emptyset\)</span> corresponds to the model <span class="math inline">\(Y = \beta_0 + \varepsilon\)</span>.</p>
<p>The method described in this section is based on the following three observations:</p>
<ul>
<li><p>In order to <em>maximise</em> <span class="math inline">\(R^2_\mathrm{adj}\)</span>, we can equivalently also
find the <span class="math inline">\(J\)</span> which <em>minimises</em>
<span class="math display">\[\begin{equation*}
  \hat\sigma^2(J)
    := \frac{1}{n - |J| - 1} \sum_{i=1}^n (y_i - \hat y_i)^2,
\end{equation*}\]</span>
where <span class="math inline">\(|J|\)</span> is the number of variables in <span class="math inline">\(J\)</span> and the
<span class="math inline">\(\hat y_i\)</span> are the fitted values for the model with inputs <span class="math inline">\(j \in J\)</span>.
We have seen that these criteria are equivalent in section <a href="S11-diagnostics.html#error-var">11.2</a>.</p></li>
<li><p>For <span class="math inline">\(J \subseteq \{1, \ldots p\}\)</span>, define
<span class="math display">\[\begin{equation*}
  r(J)
    := \sum_{i=1}^n (y_i - \hat y_i)^2
\end{equation*}\]</span>
where <span class="math inline">\(\hat y_i\)</span> are the fitted values for the model corresponding to <span class="math inline">\(J\)</span>.
This gives the residual sum of squares for each model. The we have
<span class="math display">\[\begin{align*}
  \min_J \hat\sigma^2(J)
    &amp;= \min_J \frac{1}{n - |J| - 1} \sum_{i=1}^n (y_i - \hat y_i)^2 \\
    &amp;= \min_{q \in \{0, 1, \ldots, p\}} \min_{J, |J|=q} \frac{1}{n - q - 1} r(J) \\
    &amp;= \min_{q \in \{0, 1, \ldots, p\}} \frac{1}{n - q - 1} \min_{J, |J|=q} r(J).
\end{align*}\]</span>
This means that we can first minimise the residual sum of squares
for each fixed number <span class="math inline">\(q\)</span> of inputs, and then find the <span class="math inline">\(q\)</span>
which gives the best <span class="math inline">\(\hat\sigma^2\)</span> in a second step.</p></li>
<li><p>Adding a variables never increases the residual sum of squares.
Thus we have <span class="math inline">\(r(J) \leq r(K)\)</span> whenever <span class="math inline">\(J \supseteq K\)</span>.
We can use this result to exclude certain models without having to fit them.</p></li>
</ul>
</div>
<div id="search-algorithm" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> Search Algorithm<a href="S12-automatic.html#search-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="videowrap">
<div class="videowrapper">
<iframe width="560" height="315" src="https://www.youtube.com/embed/B0vNhThH8Dc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
</div>
<p>To find the model with optimal adjusted R-squared value, we perform the
following steps. The algorithm is based on the ideas explained in the previous
section.</p>
<ol style="list-style-type: upper-alpha">
<li><p>Let <span class="math inline">\(\varphi_j\)</span> denote the residual sum of squares for the model containing
all variables except <span class="math inline">\(x_j\)</span>:
<span class="math display">\[\begin{equation*}
      \varphi_j := r\bigl( \{ 1, \ldots, p \} \setminus \{ j \} \bigr).
  \end{equation*}\]</span>
Suppose that the <span class="math inline">\(x_j\)</span> are ordered so that
<span class="math display">\[\begin{equation*}
      \varphi_1 \geq \varphi_2 \geq \cdots \geq \varphi_p.
  \end{equation*}\]</span>
Any model <span class="math inline">\(J\)</span> with <span class="math inline">\(j \notin J\)</span> has <span class="math inline">\(r(J) \geq \varphi_j\)</span>.</p></li>
<li><p>Compute <span class="math inline">\(\min_{J, |J|=0} r(J) = r(\emptyset)\)</span>. This is the residual
sum of squares of the model which consists only of the intercept.</p></li>
<li><p>For each <span class="math inline">\(q := 1, \ldots, p-2\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Let
<span class="math display">\[\begin{equation*}
     r := r\bigl( \{1, \ldots, q\} \bigr).
\end{equation*}\]</span>
This is the only model with <span class="math inline">\(q\)</span> inputs where the first
excluded variable has index <span class="math inline">\(k = q+1\)</span>.
If <span class="math inline">\(r \leq \varphi_{k-1} = \varphi_q\)</span>, we know from step A that
<span class="math inline">\(J = \{1, \ldots, q\}\)</span> is the best
model with <span class="math inline">\(q\)</span> inputs, since any other model will exclude
one of the <span class="math inline">\(x_j\)</span> with <span class="math inline">\(j \leq q\)</span>.
In this case we have found
<span class="math display">\[\begin{equation*}
   \min_{J, |J|=q} r(J)
   = r.
\end{equation*}\]</span>
and we continue step B by trying the next value of <span class="math inline">\(q\)</span>.
If <span class="math inline">\(r &gt; \varphi_{k-1}\)</span>, no decision can be taken at this point and we
continue with step 2.</p></li>
<li><p>For <span class="math inline">\(j \in \{q+1, \ldots, p\}\)</span> let
<span class="math display">\[\begin{equation*}
     r_j := r\bigl( \{1, \ldots, q-1\} \cup \{j\} \bigr).
\end{equation*}\]</span>
These are all models with <span class="math inline">\(q\)</span> inputs where the first excluded
variable has index <span class="math inline">\(k = q\)</span>.
If <span class="math inline">\(\min(r, r_1, \ldots, r_q) \leq \varphi_{k-1} = \varphi_{q-1}\)</span>, then we know
from step A that the <span class="math inline">\(J\)</span> corresponing to the minimum is the best model
with <span class="math inline">\(q\)</span> inputs. In this case we continue step B by trying the next
value of <span class="math inline">\(q\)</span>. Otherwise we proceed to step 3.</p></li>
<li><p>Similarly to the previous step, we consider all models with <span class="math inline">\(q\)</span>
variables where the first excluded variable has index <span class="math inline">\(k = q-1\)</span>.
If the best RSS amonst these and the previously considered
models is less than or equal to <span class="math inline">\(\varphi_{k-1}\)</span> we are done
and consider the next <span class="math inline">\(q\)</span>. Otherwise we decrease <span class="math inline">\(k\)</span> until we reach
<span class="math inline">\(k = 1\)</span>. At this point we have considered all models with <span class="math inline">\(q\)</span>
variables and have found <span class="math inline">\(\min_{J, |J|=q} r(J)\)</span>.</p></li>
</ol></li>
<li><p>Compute <span class="math inline">\(\min_{J, |J|=p-1} r(J) = \min_{j \in \{1, \ldots, p\}} \varphi_j\)</span>.</p></li>
<li><p>Compute <span class="math inline">\(\min_{J, |J|=p} r(J) = r\bigl( \{1, \ldots, p\} \bigr)\)</span>.</p></li>
<li><p>Find the <span class="math inline">\(q \in \{0, \ldots, p\}\)</span> for which <span class="math inline">\(\min_{J, |J|=q} r(J) / (n-q-1)\)</span>
is smallest. Output the model which has minimal RSS for this <span class="math inline">\(q\)</span>.</p></li>
</ol>
<p>This algorithm finds the model with the maximal <span class="math inline">\(R^2_\mathrm{adj}\)</span>.
Often, large savings are achieved by the early exits in step C.
Only in the worst case, when all of the comparisons with <span class="math inline">\(\varphi_j\)</span> in step
C fail, this algorithms needs to fit all <span class="math inline">\(2^p\)</span> models.</p>
<div class="example">
<p><span id="exm:qsar-search" class="example"><strong>Example 12.1  </strong></span>To demonstrate the steps of the algorithm, we implement the method
“by hand”. We use the QSAR dataset, which we have already seen
in section <a href="S07-examples.html#S07-examples">7</a>:</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="S12-automatic.html#cb176-1" tabindex="-1"></a><span class="co"># data from https://archive.ics.uci.edu/ml/datasets/QSAR+aquatic+toxicity</span></span>
<span id="cb176-2"><a href="S12-automatic.html#cb176-2" tabindex="-1"></a>qsar <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/qsar_aquatic_toxicity.csv&quot;</span>,</span>
<span id="cb176-3"><a href="S12-automatic.html#cb176-3" tabindex="-1"></a>                 <span class="at">sep =</span> <span class="st">&quot;;&quot;</span>, <span class="at">header =</span> <span class="cn">FALSE</span>)</span>
<span id="cb176-4"><a href="S12-automatic.html#cb176-4" tabindex="-1"></a>fields <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb176-5"><a href="S12-automatic.html#cb176-5" tabindex="-1"></a>    <span class="st">&quot;TPSA&quot;</span>,</span>
<span id="cb176-6"><a href="S12-automatic.html#cb176-6" tabindex="-1"></a>    <span class="st">&quot;SAacc&quot;</span>,</span>
<span id="cb176-7"><a href="S12-automatic.html#cb176-7" tabindex="-1"></a>    <span class="st">&quot;H050&quot;</span>,</span>
<span id="cb176-8"><a href="S12-automatic.html#cb176-8" tabindex="-1"></a>    <span class="st">&quot;MLOGP&quot;</span>,</span>
<span id="cb176-9"><a href="S12-automatic.html#cb176-9" tabindex="-1"></a>    <span class="st">&quot;RDCHI&quot;</span>,</span>
<span id="cb176-10"><a href="S12-automatic.html#cb176-10" tabindex="-1"></a>    <span class="st">&quot;GATS1p&quot;</span>,</span>
<span id="cb176-11"><a href="S12-automatic.html#cb176-11" tabindex="-1"></a>    <span class="st">&quot;nN&quot;</span>,</span>
<span id="cb176-12"><a href="S12-automatic.html#cb176-12" tabindex="-1"></a>    <span class="st">&quot;C040&quot;</span>,</span>
<span id="cb176-13"><a href="S12-automatic.html#cb176-13" tabindex="-1"></a>    <span class="st">&quot;LC50&quot;</span></span>
<span id="cb176-14"><a href="S12-automatic.html#cb176-14" tabindex="-1"></a>)</span>
<span id="cb176-15"><a href="S12-automatic.html#cb176-15" tabindex="-1"></a><span class="fu">names</span>(qsar) <span class="ot">&lt;-</span> fields</span></code></pre></div>
<p>To make it easy to add/remove columns automatically, we first construct
the design matrix, remove columns as needed, and then use the resulting
matrix in the call to <code>lm()</code> (instead of specifying the terms to include
by name).</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="S12-automatic.html#cb177-1" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(LC50 <span class="sc">~</span> ., <span class="at">data =</span> qsar)</span>
<span id="cb177-2"><a href="S12-automatic.html#cb177-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> X[, <span class="sc">-</span><span class="dv">1</span>] <span class="co"># remove the intercept, since lm() will re-add it later</span></span>
<span id="cb177-3"><a href="S12-automatic.html#cb177-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb177-4"><a href="S12-automatic.html#cb177-4" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb177-5"><a href="S12-automatic.html#cb177-5" tabindex="-1"></a></span>
<span id="cb177-6"><a href="S12-automatic.html#cb177-6" tabindex="-1"></a>y <span class="ot">&lt;-</span> qsar<span class="sc">$</span>LC50</span>
<span id="cb177-7"><a href="S12-automatic.html#cb177-7" tabindex="-1"></a></span>
<span id="cb177-8"><a href="S12-automatic.html#cb177-8" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X) <span class="co"># full model</span></span>
<span id="cb177-9"><a href="S12-automatic.html#cb177-9" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre class="rOutput"><code>
Call:
lm(formula = y ~ X)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.4934 -0.7579 -0.1120  0.5829  4.9778 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.698887   0.244554  11.036  &lt; 2e-16 ***
XTPSA        0.027201   0.002661  10.220  &lt; 2e-16 ***
XSAacc      -0.015081   0.002091  -7.211 1.90e-12 ***
XH050        0.040619   0.059787   0.679 0.497186    
XMLOGP       0.446108   0.063296   7.048 5.60e-12 ***
XRDCHI       0.513928   0.135565   3.791 0.000167 ***
XGATS1p     -0.571313   0.153882  -3.713 0.000227 ***
XnN         -0.224751   0.048301  -4.653 4.12e-06 ***
XC040        0.003194   0.077972   0.041 0.967340    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.203 on 537 degrees of freedom
Multiple R-squared:  0.4861,    Adjusted R-squared:  0.4785 
F-statistic:  63.5 on 8 and 537 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Comparing the output to what we have seen in section <a href="S07-examples.html#S07-examples">7</a>
shows that the new method of calling <code>lm()</code> gives the same results
as before. Now we follow the steps of the algorithm.</p>
<p><strong>A.</strong> The values <span class="math inline">\(\varphi_1, \ldots, \varphi_p\)</span> can be computed as follows:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="S12-automatic.html#cb179-1" tabindex="-1"></a>phi <span class="ot">&lt;-</span> <span class="fu">numeric</span>(p) <span class="co"># pre-allocate an empty vector</span></span>
<span id="cb179-2"><a href="S12-automatic.html#cb179-2" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>p) {</span>
<span id="cb179-3"><a href="S12-automatic.html#cb179-3" tabindex="-1"></a>    idx <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">:</span>p)[<span class="sc">-</span>j] <span class="co"># all variables except x[j]</span></span>
<span id="cb179-4"><a href="S12-automatic.html#cb179-4" tabindex="-1"></a>    m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X[,idx])</span>
<span id="cb179-5"><a href="S12-automatic.html#cb179-5" tabindex="-1"></a>    phi[j] <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">resid</span>(m)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb179-6"><a href="S12-automatic.html#cb179-6" tabindex="-1"></a>}</span>
<span id="cb179-7"><a href="S12-automatic.html#cb179-7" tabindex="-1"></a></span>
<span id="cb179-8"><a href="S12-automatic.html#cb179-8" tabindex="-1"></a><span class="co"># change the order of columns in X, so that the phi_j are decreasing</span></span>
<span id="cb179-9"><a href="S12-automatic.html#cb179-9" tabindex="-1"></a>jj <span class="ot">&lt;-</span> <span class="fu">order</span>(phi, <span class="at">decreasing =</span> <span class="cn">TRUE</span>)</span>
<span id="cb179-10"><a href="S12-automatic.html#cb179-10" tabindex="-1"></a>X <span class="ot">&lt;-</span> X[, jj]</span>
<span id="cb179-11"><a href="S12-automatic.html#cb179-11" tabindex="-1"></a>phi <span class="ot">&lt;-</span> phi[jj]</span>
<span id="cb179-12"><a href="S12-automatic.html#cb179-12" tabindex="-1"></a><span class="fu">plot</span>(phi, <span class="at">xlab =</span> <span class="st">&quot;j&quot;</span>, <span class="at">ylab =</span> <span class="fu">expression</span>(varphi[j]))</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/exhaustiveA-1.png" width="672" /></p>
<p>The plot shows the residual sum of squares for the model with input <span class="math inline">\(x_j\)</span>
omitted, for <span class="math inline">\(j\)</span> ranging from 1 to 8.</p>
<p><strong>B.</strong> Next, we compute the residual
sum of squares of the model which consists only of the intercept.
This is the case <span class="math inline">\(q = 0\)</span>.</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="S12-automatic.html#cb180-1" tabindex="-1"></a>all.q <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span>p</span>
<span id="cb180-2"><a href="S12-automatic.html#cb180-2" tabindex="-1"></a>best.rss <span class="ot">&lt;-</span> <span class="fu">numeric</span>(p <span class="sc">+</span> <span class="dv">1</span>) <span class="co"># For storing the best RSS for q = 0, ..., p,</span></span>
<span id="cb180-3"><a href="S12-automatic.html#cb180-3" tabindex="-1"></a>best.model <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, p <span class="sc">+</span> <span class="dv">1</span>) <span class="co"># and the corresponding models.</span></span>
<span id="cb180-4"><a href="S12-automatic.html#cb180-4" tabindex="-1"></a></span>
<span id="cb180-5"><a href="S12-automatic.html#cb180-5" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="dv">1</span>)</span>
<span id="cb180-6"><a href="S12-automatic.html#cb180-6" tabindex="-1"></a>best.rss[<span class="dv">0</span> <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">resid</span>(m)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb180-7"><a href="S12-automatic.html#cb180-7" tabindex="-1"></a>best.model[[<span class="dv">0</span> <span class="sc">+</span> <span class="dv">1</span>]] <span class="ot">&lt;-</span> <span class="fu">integer</span>(<span class="dv">0</span>) <span class="co"># a vector of length 0 (no columns)</span></span>
<span id="cb180-8"><a href="S12-automatic.html#cb180-8" tabindex="-1"></a></span>
<span id="cb180-9"><a href="S12-automatic.html#cb180-9" tabindex="-1"></a>count <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="co"># number of models fitted so far</span></span></code></pre></div>
<p><strong>C.</strong> Now we consider <span class="math inline">\(q \in \{1, \ldots, p-2\}\)</span>. The algorithm groups
these cases by the position <span class="math inline">\(k\)</span> of the first column omitted in the model,
starting with <span class="math inline">\(k = q+1\)</span> and then decreasing <span class="math inline">\(k\)</span> in each step.
We use the function <code>combn</code> from the <code>sets</code> package
to get all possible choices of
<span class="math inline">\(q - k + 1\)</span> columns out of <span class="math inline">\(\{k+1, \ldots, p\}\)</span>.</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="S12-automatic.html#cb181-1" tabindex="-1"></a><span class="fu">library</span>(sets) <span class="co"># this defines &quot;combn()&quot;</span></span>
<span id="cb181-2"><a href="S12-automatic.html#cb181-2" tabindex="-1"></a><span class="cf">for</span> (q <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(p<span class="dv">-2</span>)) {</span>
<span id="cb181-3"><a href="S12-automatic.html#cb181-3" tabindex="-1"></a>    best.rss[q <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="cn">Inf</span></span>
<span id="cb181-4"><a href="S12-automatic.html#cb181-4" tabindex="-1"></a></span>
<span id="cb181-5"><a href="S12-automatic.html#cb181-5" tabindex="-1"></a>    <span class="co"># Consider all sets of q columns, ...</span></span>
<span id="cb181-6"><a href="S12-automatic.html#cb181-6" tabindex="-1"></a>    <span class="cf">for</span> (k <span class="cf">in</span> (q<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">1</span>) {</span>
<span id="cb181-7"><a href="S12-automatic.html#cb181-7" tabindex="-1"></a>        <span class="co"># ... where the first omitted column is k.</span></span>
<span id="cb181-8"><a href="S12-automatic.html#cb181-8" tabindex="-1"></a></span>
<span id="cb181-9"><a href="S12-automatic.html#cb181-9" tabindex="-1"></a>        <span class="co"># We have to include 1, ..., k-1, and ...</span></span>
<span id="cb181-10"><a href="S12-automatic.html#cb181-10" tabindex="-1"></a>        a <span class="ot">&lt;-</span> <span class="fu">seq_len</span>(k<span class="dv">-1</span>)</span>
<span id="cb181-11"><a href="S12-automatic.html#cb181-11" tabindex="-1"></a></span>
<span id="cb181-12"><a href="S12-automatic.html#cb181-12" tabindex="-1"></a>        <span class="co"># ... for the remaining q - (k-1) inputs, we try all</span></span>
<span id="cb181-13"><a href="S12-automatic.html#cb181-13" tabindex="-1"></a>        <span class="co"># possible combinations.</span></span>
<span id="cb181-14"><a href="S12-automatic.html#cb181-14" tabindex="-1"></a>        bb <span class="ot">&lt;-</span> <span class="fu">combn</span>((k<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>p, q <span class="sc">-</span> k <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb181-15"><a href="S12-automatic.html#cb181-15" tabindex="-1"></a>        <span class="cf">for</span> (l <span class="cf">in</span> <span class="fu">seq_len</span>(<span class="fu">ncol</span>(bb))) {</span>
<span id="cb181-16"><a href="S12-automatic.html#cb181-16" tabindex="-1"></a>            b <span class="ot">&lt;-</span> bb[, l]</span>
<span id="cb181-17"><a href="S12-automatic.html#cb181-17" tabindex="-1"></a>            included <span class="ot">&lt;-</span> <span class="fu">c</span>(a, b)</span>
<span id="cb181-18"><a href="S12-automatic.html#cb181-18" tabindex="-1"></a></span>
<span id="cb181-19"><a href="S12-automatic.html#cb181-19" tabindex="-1"></a>            m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X[, included])</span>
<span id="cb181-20"><a href="S12-automatic.html#cb181-20" tabindex="-1"></a>            count <span class="ot">&lt;-</span> count <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb181-21"><a href="S12-automatic.html#cb181-21" tabindex="-1"></a>            rss <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">resid</span>(m)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb181-22"><a href="S12-automatic.html#cb181-22" tabindex="-1"></a></span>
<span id="cb181-23"><a href="S12-automatic.html#cb181-23" tabindex="-1"></a>            <span class="cf">if</span> (rss <span class="sc">&lt;</span> best.rss[q <span class="sc">+</span> <span class="dv">1</span>]) {</span>
<span id="cb181-24"><a href="S12-automatic.html#cb181-24" tabindex="-1"></a>                best.rss[q <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> rss</span>
<span id="cb181-25"><a href="S12-automatic.html#cb181-25" tabindex="-1"></a>                best.model[[q <span class="sc">+</span> <span class="dv">1</span>]] <span class="ot">&lt;-</span> included</span>
<span id="cb181-26"><a href="S12-automatic.html#cb181-26" tabindex="-1"></a>            }</span>
<span id="cb181-27"><a href="S12-automatic.html#cb181-27" tabindex="-1"></a>        }</span>
<span id="cb181-28"><a href="S12-automatic.html#cb181-28" tabindex="-1"></a></span>
<span id="cb181-29"><a href="S12-automatic.html#cb181-29" tabindex="-1"></a>        <span class="cf">if</span> (k <span class="sc">&gt;</span> <span class="dv">1</span> <span class="sc">&amp;&amp;</span> best.rss[q] <span class="sc">&lt;=</span> phi[k<span class="dv">-1</span>]) {</span>
<span id="cb181-30"><a href="S12-automatic.html#cb181-30" tabindex="-1"></a>            <span class="co"># If we reach this point, we know that we found the best</span></span>
<span id="cb181-31"><a href="S12-automatic.html#cb181-31" tabindex="-1"></a>            <span class="co"># arrangement and we can exit the loop over k early.</span></span>
<span id="cb181-32"><a href="S12-automatic.html#cb181-32" tabindex="-1"></a>            <span class="co"># This is what makes the algorithm efficient.</span></span>
<span id="cb181-33"><a href="S12-automatic.html#cb181-33" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb181-34"><a href="S12-automatic.html#cb181-34" tabindex="-1"></a>        }</span>
<span id="cb181-35"><a href="S12-automatic.html#cb181-35" tabindex="-1"></a>    }</span>
<span id="cb181-36"><a href="S12-automatic.html#cb181-36" tabindex="-1"></a>}</span></code></pre></div>
<p><strong>D.</strong> We already fitted all models with <span class="math inline">\(p-1\)</span> inputs, when we computed <code>phi</code>.
Since we sorted the models, the best of these models is last in the list.
This covers the case <span class="math inline">\(q = p - 1\)</span>.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="S12-automatic.html#cb182-1" tabindex="-1"></a>best.rss[(p <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">min</span>(phi)</span>
<span id="cb182-2"><a href="S12-automatic.html#cb182-2" tabindex="-1"></a>omitted <span class="ot">&lt;-</span> jj[<span class="fu">length</span>(jj)]</span>
<span id="cb182-3"><a href="S12-automatic.html#cb182-3" tabindex="-1"></a>best.model[[(p <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="dv">1</span>]] <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">:</span>p)[<span class="sc">-</span>omitted]</span>
<span id="cb182-4"><a href="S12-automatic.html#cb182-4" tabindex="-1"></a>count <span class="ot">&lt;-</span> count <span class="sc">+</span> <span class="fu">length</span>(phi)</span></code></pre></div>
<p><strong>E.</strong> Finally, for <span class="math inline">\(q = p\)</span> we fit the full model:</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="S12-automatic.html#cb183-1" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X)</span>
<span id="cb183-2"><a href="S12-automatic.html#cb183-2" tabindex="-1"></a>best.rss[p <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">resid</span>(m)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb183-3"><a href="S12-automatic.html#cb183-3" tabindex="-1"></a>best.model[[p <span class="sc">+</span> <span class="dv">1</span>]] <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>p</span>
<span id="cb183-4"><a href="S12-automatic.html#cb183-4" tabindex="-1"></a>count <span class="ot">&lt;-</span> count <span class="sc">+</span> <span class="dv">1</span></span></code></pre></div>
<p><strong>F.</strong> Now we can find the model with the best <span class="math inline">\(R^2_\mathrm{adj}\)</span>:</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="S12-automatic.html#cb184-1" tabindex="-1"></a><span class="fu">plot</span>(all.q, best.rss <span class="sc">/</span> (n <span class="sc">-</span> all.q <span class="sc">-</span> <span class="dv">1</span>),</span>
<span id="cb184-2"><a href="S12-automatic.html#cb184-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;q&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;residual variance&quot;</span>)</span>
<span id="cb184-3"><a href="S12-automatic.html#cb184-3" tabindex="-1"></a>best.q <span class="ot">&lt;-</span> all.q[<span class="fu">which.min</span>(best.rss <span class="sc">/</span> (n <span class="sc">-</span> all.q <span class="sc">-</span> <span class="dv">1</span>))]</span>
<span id="cb184-4"><a href="S12-automatic.html#cb184-4" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> best.q)</span></code></pre></div>
<p><img src="MATH3714_files/figure-html/exhaustiveB-1.png" width="672" /></p>
<p>We see that the model with <span class="math inline">\(q = 6\)</span> variables has the the
lowest <span class="math inline">\(\hat\sigma^2\)</span> and thus the highest <span class="math inline">\(R^2_\mathrm{adj}\)</span>.
The values for <span class="math inline">\(q = 7\)</span> and <span class="math inline">\(q = 8\)</span> are very close to
optimal. The best model uses the following variables:</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="S12-automatic.html#cb185-1" tabindex="-1"></a><span class="fu">colnames</span>(X)[best.model[[<span class="dv">6</span> <span class="sc">+</span> <span class="dv">1</span>]]]</span></code></pre></div>
<pre class="rOutput"><code>[1] &quot;TPSA&quot;   &quot;SAacc&quot;  &quot;MLOGP&quot;  &quot;nN&quot;     &quot;RDCHI&quot;  &quot;GATS1p&quot;</code></pre>
<p>We can fit the optimal model from the original data:</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="S12-automatic.html#cb187-1" tabindex="-1"></a>m.best <span class="ot">&lt;-</span> <span class="fu">lm</span>(LC50 <span class="sc">~</span> TPSA <span class="sc">+</span> SAacc <span class="sc">+</span> MLOGP <span class="sc">+</span> nN <span class="sc">+</span> RDCHI <span class="sc">+</span> GATS1p,</span>
<span id="cb187-2"><a href="S12-automatic.html#cb187-2" tabindex="-1"></a>             <span class="at">data =</span> qsar)</span>
<span id="cb187-3"><a href="S12-automatic.html#cb187-3" tabindex="-1"></a><span class="fu">summary</span>(m.best)</span></code></pre></div>
<pre class="rOutput"><code>
Call:
lm(formula = LC50 ~ TPSA + SAacc + MLOGP + nN + RDCHI + GATS1p, 
    data = qsar)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.4986 -0.7668 -0.1165  0.5529  4.9758 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.758246   0.228483  12.072  &lt; 2e-16 ***
TPSA         0.026858   0.002608  10.300  &lt; 2e-16 ***
SAacc       -0.014267   0.001660  -8.596  &lt; 2e-16 ***
MLOGP        0.434578   0.060611   7.170 2.49e-12 ***
nN          -0.218445   0.047101  -4.638 4.43e-06 ***
RDCHI        0.514758   0.133430   3.858 0.000128 ***
GATS1p      -0.602971   0.146920  -4.104 4.69e-05 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.201 on 539 degrees of freedom
Multiple R-squared:  0.4857,    Adjusted R-squared:  0.4799 
F-statistic: 84.82 on 6 and 539 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>This shows that <span class="math inline">\(R^2_\mathrm{adj}\)</span> has indeed marginally improved,
from <span class="math inline">\(0.4785\)</span> to <span class="math inline">\(0.4799\)</span></p>
<p>To conclude, we check that the complicated algorithm indeed saved some work:</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="S12-automatic.html#cb189-1" tabindex="-1"></a>total <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">^</span>p</span>
<span id="cb189-2"><a href="S12-automatic.html#cb189-2" tabindex="-1"></a><span class="fu">cat</span>(count,</span>
<span id="cb189-3"><a href="S12-automatic.html#cb189-3" tabindex="-1"></a>    <span class="st">&quot; models fitted (&quot;</span>, <span class="fu">round</span>(<span class="dv">100</span> <span class="sc">*</span> count <span class="sc">/</span> total, <span class="dv">1</span>), <span class="st">&quot;%)</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb189-4"><a href="S12-automatic.html#cb189-4" tabindex="-1"></a>    <span class="at">sep =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<pre class="rOutput"><code>88 models fitted (34.4%)</code></pre>
<p>This shows that we only needed to fit 88 of the 256 models under consideration.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-43" class="example"><strong>Example 12.2  </strong></span>We can perform the analysis from the previous example automatically,
using the function <code>regsubsets()</code> from the <code>leaps</code> package:</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="S12-automatic.html#cb191-1" tabindex="-1"></a><span class="fu">library</span>(leaps)</span>
<span id="cb191-2"><a href="S12-automatic.html#cb191-2" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(LC50 <span class="sc">~</span> ., <span class="at">data =</span> qsar,</span>
<span id="cb191-3"><a href="S12-automatic.html#cb191-3" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;exhaustive&quot;</span>)</span>
<span id="cb191-4"><a href="S12-automatic.html#cb191-4" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre class="rOutput"><code>Subset selection object
Call: regsubsets.formula(LC50 ~ ., data = qsar, method = &quot;exhaustive&quot;)
8 Variables  (and intercept)
       Forced in Forced out
TPSA       FALSE      FALSE
SAacc      FALSE      FALSE
H050       FALSE      FALSE
MLOGP      FALSE      FALSE
RDCHI      FALSE      FALSE
GATS1p     FALSE      FALSE
nN         FALSE      FALSE
C040       FALSE      FALSE
1 subsets of each size up to 8
Selection Algorithm: exhaustive
         TPSA SAacc H050 MLOGP RDCHI GATS1p nN  C040
1  ( 1 ) &quot; &quot;  &quot; &quot;   &quot; &quot;  &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot; &quot; &quot; 
2  ( 1 ) &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot; &quot; &quot; 
3  ( 1 ) &quot;*&quot;  &quot;*&quot;   &quot; &quot;  &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot; &quot; &quot; 
4  ( 1 ) &quot;*&quot;  &quot;*&quot;   &quot; &quot;  &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot;*&quot; &quot; &quot; 
5  ( 1 ) &quot;*&quot;  &quot;*&quot;   &quot; &quot;  &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot; &quot; &quot; 
6  ( 1 ) &quot;*&quot;  &quot;*&quot;   &quot; &quot;  &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot; &quot; &quot; 
7  ( 1 ) &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot; &quot; &quot; 
8  ( 1 ) &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot; &quot;*&quot; </code></pre>
<p>This shows the best model of each size. The only step left is
to decide which <span class="math inline">\(q\)</span> to use. This choice depends on the cost-complexity
tradeoff. Here we consider <span class="math inline">\(R^2_\mathrm{adj}\)</span> again:</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="S12-automatic.html#cb193-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">summary</span>(m)<span class="sc">$</span>adjr2, <span class="dv">4</span>)</span></code></pre></div>
<pre class="rOutput"><code>[1] 0.2855 0.3860 0.4441 0.4588 0.4666 0.4799 0.4795 0.4785</code></pre>
<p>This gives the <span class="math inline">\(R^2_\mathrm{adj}\)</span> for the optimal model of each size
again. At the end of the list we recognise the values <code>0.4799</code> and
<code>0.4785</code> from our previous analysis.</p>
</div>
</div>
<div id="other-methods" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> Other Methods<a href="S12-automatic.html#other-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are other, approximate methods available which can be used
when the number <span class="math inline">\(p\)</span> of model parameters is too large for an exhaustives
search.</p>
<div id="stepwise-forward-selection" class="section level3 hasAnchor" number="12.3.1">
<h3><span class="header-section-number">12.3.1</span> Stepwise Forward Selection<a href="S12-automatic.html#stepwise-forward-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here the idea is to start with a minimal model, and then add variables one by
one, until no further (large enough) improvements can be achieved.</p>
<p>Start with only intercept term: <span class="math inline">\(y=\beta_0 + \varepsilon\)</span>, then
consider each of <span class="math inline">\(p\)</span> models:
<span class="math display">\[\begin{equation*}
  y
  = \beta_0 + \beta_j x_j + \varepsilon,
\end{equation*}\]</span>
for <span class="math inline">\(j \in \{1, \ldots, p\}\)</span>.</p>
<p>Choose the model with the smallest residual sum of squares, provided that the
“significance of the fitted model” achieves a specified threshold. The process
continues by adding more variables, one at a time, until either</p>
<ul>
<li>All variables are in the model.</li>
<li>The significance level can not be achieved by any variable not in the model.</li>
</ul>
<p>The “significance of the model” can be examined by considering a
<span class="math inline">\(t\)</span>-test as in lemma <a href="S05-single.html#lem:t-test">5.4</a>.</p>
</div>
<div id="stepwise-backward-selection" class="section level3 hasAnchor" number="12.3.2">
<h3><span class="header-section-number">12.3.2</span> Stepwise Backward Selection<a href="S12-automatic.html#stepwise-backward-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here the idea is to start with the full model, and to remove variables
one by one until no good enough candidates for removal are left.</p>
<p>In each step we consider the test statistic <span class="math inline">\(|T_j|\)</span> for the tests
<span class="math display">\[\begin{equation*}
  H_0\colon \beta_j = 0
  \quad \textit{vs.} \quad
  H_1\colon \beta_j \neq 0
\end{equation*}\]</span>
for <span class="math inline">\(j \in \{1, \ldots, p\}\)</span>, again as in lemma <a href="S05-single.html#lem:t-test">5.4</a>.
The method selects <span class="math inline">\(x_j\)</span> corresponding to the smallest <span class="math inline">\(|T_j|\)</span>.
If this is below a given threshold, then remove <span class="math inline">\(x_j\)</span> and re-fit the model.
Repeat until either:</p>
<ul>
<li>No variables are left in the model.</li>
<li>The smallest <span class="math inline">\(|T_j|\)</span> is above the threshold.</li>
</ul>
</div>
<div id="hybrid-methods" class="section level3 hasAnchor" number="12.3.3">
<h3><span class="header-section-number">12.3.3</span> Hybrid Methods<a href="S12-automatic.html#hybrid-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Either start with a full model or just intercept. At each stage consider:</p>
<ul>
<li>removing least significant variable already in the model,</li>
<li>adding most significant variable not currently in the model,</li>
</ul>
<p>with significance levels set to avoid a cyclical behaviour.</p>
<div class="mysummary">
<p><strong>Summary</strong></p>
<ul>
<li>We discussed different algorithms to perform model selection in practice.</li>
</ul>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="I03-lm-output.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="S13-factors.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH3714.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
