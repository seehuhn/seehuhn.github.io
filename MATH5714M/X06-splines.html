<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 6 Spline Smoothing | MATH5714 Linear Regression, Robustness and Smoothing</title>
  <meta name="description" content="Lecture notes for the course MATH5714 Linear Regression, Robustness and Smoothing at the University of Leeds, 2025/26" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 6 Spline Smoothing | MATH5714 Linear Regression, Robustness and Smoothing" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for the course MATH5714 Linear Regression, Robustness and Smoothing at the University of Leeds, 2025/26" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 6 Spline Smoothing | MATH5714 Linear Regression, Robustness and Smoothing" />
  
  <meta name="twitter:description" content="Lecture notes for the course MATH5714 Linear Regression, Robustness and Smoothing at the University of Leeds, 2025/26" />
  

<meta name="author" content="Jochen Voss" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="X05-locpoly.html"/>
<link rel="next" href="X07-nearest.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="jvstyle.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">MATH5714M Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="X01-KDE.html"><a href="X01-KDE.html"><i class="fa fa-check"></i><b>1</b> Kernel Density Estimation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="X01-KDE.html"><a href="X01-KDE.html#probability-densities"><i class="fa fa-check"></i><b>1.1</b> Probability Densities</a></li>
<li class="chapter" data-level="1.2" data-path="X01-KDE.html"><a href="X01-KDE.html#histograms"><i class="fa fa-check"></i><b>1.2</b> Histograms</a></li>
<li class="chapter" data-level="1.3" data-path="X01-KDE.html"><a href="X01-KDE.html#kernels"><i class="fa fa-check"></i><b>1.3</b> Kernels</a></li>
<li class="chapter" data-level="1.4" data-path="X01-KDE.html"><a href="X01-KDE.html#kernel-density-estimation"><i class="fa fa-check"></i><b>1.4</b> Kernel Density Estimation</a></li>
<li class="chapter" data-level="1.5" data-path="X01-KDE.html"><a href="X01-KDE.html#kernel-density-estimation-in-r"><i class="fa fa-check"></i><b>1.5</b> Kernel Density Estimation in R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="I01-vectorise.html"><a href="I01-vectorise.html"><i class="fa fa-check"></i>Interlude: Vectorisation in R</a>
<ul>
<li class="chapter" data-level="" data-path="I01-vectorise.html"><a href="I01-vectorise.html#direct-implementation"><i class="fa fa-check"></i>Direct Implementation</a></li>
<li class="chapter" data-level="" data-path="I01-vectorise.html"><a href="I01-vectorise.html#vectorization"><i class="fa fa-check"></i>Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="X02-error.html"><a href="X02-error.html"><i class="fa fa-check"></i><b>2</b> The Error of Kernel Density Estimates</a>
<ul>
<li class="chapter" data-level="2.1" data-path="X02-error.html"><a href="X02-error.html#a-statistical-model"><i class="fa fa-check"></i><b>2.1</b> A Statistical Model</a></li>
<li class="chapter" data-level="2.2" data-path="X02-error.html"><a href="X02-error.html#moments-of-kernels"><i class="fa fa-check"></i><b>2.2</b> Moments of Kernels</a></li>
<li class="chapter" data-level="2.3" data-path="X02-error.html"><a href="X02-error.html#X02-error-MSE"><i class="fa fa-check"></i><b>2.3</b> Mean Squared Error</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="X02-error.html"><a href="X02-error.html#result"><i class="fa fa-check"></i><b>2.3.1</b> Result</a></li>
<li class="chapter" data-level="2.3.2" data-path="X02-error.html"><a href="X02-error.html#X02-error-bias"><i class="fa fa-check"></i><b>2.3.2</b> Bias</a></li>
<li class="chapter" data-level="2.3.3" data-path="X02-error.html"><a href="X02-error.html#X02-error-var"><i class="fa fa-check"></i><b>2.3.3</b> Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="X02-error.html"><a href="X02-error.html#optimal-bandwidth-for-pointwise-mse"><i class="fa fa-check"></i><b>2.4</b> Optimal Bandwidth for Pointwise MSE</a></li>
<li class="chapter" data-level="2.5" data-path="X02-error.html"><a href="X02-error.html#IMSE"><i class="fa fa-check"></i><b>2.5</b> Integrated Error</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="X03-practice.html"><a href="X03-practice.html"><i class="fa fa-check"></i><b>3</b> Kernel Density Estimation in Practice</a>
<ul>
<li class="chapter" data-level="3.1" data-path="X03-practice.html"><a href="X03-practice.html#choice-of-kernel"><i class="fa fa-check"></i><b>3.1</b> Choice of Kernel</a></li>
<li class="chapter" data-level="3.2" data-path="X03-practice.html"><a href="X03-practice.html#bwsel"><i class="fa fa-check"></i><b>3.2</b> Bandwidth Selection</a></li>
<li class="chapter" data-level="3.3" data-path="X03-practice.html"><a href="X03-practice.html#higher-dimensions"><i class="fa fa-check"></i><b>3.3</b> Higher Dimensions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="P01.html"><a href="P01.html"><i class="fa fa-check"></i>Problem Sheet 1</a></li>
<li class="chapter" data-level="4" data-path="X04-smoothing.html"><a href="X04-smoothing.html"><i class="fa fa-check"></i><b>4</b> Kernel Smoothing</a>
<ul>
<li class="chapter" data-level="4.1" data-path="X04-smoothing.html"><a href="X04-smoothing.html#the-nadaraya-watson-estimator"><i class="fa fa-check"></i><b>4.1</b> The Nadaraya-Watson Estimator</a></li>
<li class="chapter" data-level="4.2" data-path="X04-smoothing.html"><a href="X04-smoothing.html#estimation-error"><i class="fa fa-check"></i><b>4.2</b> Estimation Error</a>
<ul>
<li class="chapter" data-level="" data-path="X04-smoothing.html"><a href="X04-smoothing.html#denominator"><i class="fa fa-check"></i>Denominator</a></li>
<li class="chapter" data-level="" data-path="X04-smoothing.html"><a href="X04-smoothing.html#numerator"><i class="fa fa-check"></i>Numerator</a></li>
<li class="chapter" data-level="" data-path="X04-smoothing.html"><a href="X04-smoothing.html#mean-squared-error"><i class="fa fa-check"></i>Mean Squared Error</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="X05-locpoly.html"><a href="X05-locpoly.html"><i class="fa fa-check"></i><b>5</b> Local Polynomial Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="X05-locpoly.html"><a href="X05-locpoly.html#linear-regression-with-weights"><i class="fa fa-check"></i><b>5.1</b> Linear Regression with Weights</a></li>
<li class="chapter" data-level="5.2" data-path="X05-locpoly.html"><a href="X05-locpoly.html#polynomial-regression"><i class="fa fa-check"></i><b>5.2</b> Polynomial Regression</a></li>
<li class="chapter" data-level="5.3" data-path="X05-locpoly.html"><a href="X05-locpoly.html#polynomial-regression-with-weights"><i class="fa fa-check"></i><b>5.3</b> Polynomial Regression with Weights</a></li>
<li class="chapter" data-level="5.4" data-path="X05-locpoly.html"><a href="X05-locpoly.html#special-cases"><i class="fa fa-check"></i><b>5.4</b> Special Cases</a>
<ul>
<li class="chapter" data-level="" data-path="X05-locpoly.html"><a href="X05-locpoly.html#p-0"><i class="fa fa-check"></i><span class="math inline">\(p = 0\)</span></a></li>
<li class="chapter" data-level="" data-path="X05-locpoly.html"><a href="X05-locpoly.html#p-1"><i class="fa fa-check"></i><span class="math inline">\(p = 1\)</span></a></li>
<li class="chapter" data-level="" data-path="X05-locpoly.html"><a href="X05-locpoly.html#p-2"><i class="fa fa-check"></i><span class="math inline">\(p = 2\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="X06-splines.html"><a href="X06-splines.html"><i class="fa fa-check"></i><b>6</b> Spline Smoothing</a>
<ul>
<li class="chapter" data-level="6.1" data-path="X06-splines.html"><a href="X06-splines.html#smoothing-splines"><i class="fa fa-check"></i><b>6.1</b> Smoothing Splines</a></li>
<li class="chapter" data-level="6.2" data-path="X06-splines.html"><a href="X06-splines.html#cubic-splines"><i class="fa fa-check"></i><b>6.2</b> Cubic Splines</a></li>
<li class="chapter" data-level="6.3" data-path="X06-splines.html"><a href="X06-splines.html#degrees-of-freedom"><i class="fa fa-check"></i><b>6.3</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="6.4" data-path="X06-splines.html"><a href="X06-splines.html#smoothing-splines-in-r"><i class="fa fa-check"></i><b>6.4</b> Smoothing Splines in R</a></li>
<li class="chapter" data-level="6.5" data-path="X06-splines.html"><a href="X06-splines.html#comparison-with-kernel-methods"><i class="fa fa-check"></i><b>6.5</b> Comparison with Kernel Methods</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="X07-nearest.html"><a href="X07-nearest.html"><i class="fa fa-check"></i><b>7</b> <em>k</em>-Nearest Neighbour Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="X07-nearest.html"><a href="X07-nearest.html#definition-of-the-estimator"><i class="fa fa-check"></i><b>7.1</b> Definition of the Estimator</a></li>
<li class="chapter" data-level="7.2" data-path="X07-nearest.html"><a href="X07-nearest.html#properties"><i class="fa fa-check"></i><b>7.2</b> Properties</a></li>
<li class="chapter" data-level="7.3" data-path="X07-nearest.html"><a href="X07-nearest.html#numerical-experiment"><i class="fa fa-check"></i><b>7.3</b> Numerical Experiment</a></li>
<li class="chapter" data-level="7.4" data-path="X07-nearest.html"><a href="X07-nearest.html#variants-of-the-method"><i class="fa fa-check"></i><b>7.4</b> Variants of the Method</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="X08-xval.html"><a href="X08-xval.html"><i class="fa fa-check"></i><b>8</b> Cross-validation</a>
<ul>
<li class="chapter" data-level="8.1" data-path="X08-xval.html"><a href="X08-xval.html#regression"><i class="fa fa-check"></i><b>8.1</b> Regression</a></li>
<li class="chapter" data-level="8.2" data-path="X08-xval.html"><a href="X08-xval.html#kernel-density-estimation-1"><i class="fa fa-check"></i><b>8.2</b> Kernel Density Estimation</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="X09-examples.html"><a href="X09-examples.html"><i class="fa fa-check"></i><b>9</b> Examples</a>
<ul>
<li class="chapter" data-level="9.1" data-path="X09-examples.html"><a href="X09-examples.html#kernel-density-estimation-2"><i class="fa fa-check"></i><b>9.1</b> Kernel Density Estimation</a></li>
<li class="chapter" data-level="9.2" data-path="X09-examples.html"><a href="X09-examples.html#kernel-regression"><i class="fa fa-check"></i><b>9.2</b> Kernel Regression</a></li>
<li class="chapter" data-level="9.3" data-path="X09-examples.html"><a href="X09-examples.html#k-nearest-neighbour-regression"><i class="fa fa-check"></i><b>9.3</b> <em>k</em>-Nearest Neighbour Regression</a></li>
</ul></li>
<li class="divider"></li>
<li class="chapter"><span><b>THE END</b></span></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH5714 Linear Regression, Robustness and Smoothing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="X06-splines" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Section 6</span> Spline Smoothing<a href="X06-splines.html#X06-splines" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In the previous sections we have seen how kernel methods and <span class="math inline">\(k\)</span>-nearest
neighbour methods can be used to estimate the regression function <span class="math inline">\(m\colon
\mathbb{R}\to \mathbb{R}\)</span> from data <span class="math inline">\((x_1, y_1), \ldots, (x_n, y_n)\)</span>. While these methods
work well in many situations, they have some limitations:</p>
<ul>
<li>The choice of bandwidth <span class="math inline">\(h\)</span> or neighbourhood size <span class="math inline">\(k\)</span> can be difficult.</li>
<li>Kernel methods can suffer from boundary effects, where the estimate
is biased near the edges of the data range.</li>
<li>All methods are local, averaging nearby observations, which can lead
to increased variance.</li>
</ul>
<p>In this section we introduce an alternative approach based on <strong>spline
smoothing</strong>. Instead of local averaging, we fit a smooth function globally
to the entire dataset, using a penalty term to control the smoothness.</p>
<div id="smoothing-splines" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Smoothing Splines<a href="X06-splines.html#smoothing-splines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The key idea of spline smoothing is to find a function <span class="math inline">\(m\)</span> which balances
two competing goals:</p>
<ol style="list-style-type: decimal">
<li>The function should fit the data well, <em>i.e.</em> the residual sum of
squares <span class="math inline">\(\sum_{i=1}^n \bigl( y_i - m(x_i) \bigr)^2\)</span> should be small.</li>
<li>The function should be smooth, <em>i.e.</em> it should not have too much
curvature.</li>
</ol>
<p>To measure the curvature of a function <span class="math inline">\(m\)</span>, we use the integral of the
squared second derivative:
<span class="math display">\[\begin{equation*}
  \int_{-\infty}^\infty \bigl( m&#39;&#39;(x) \bigr)^2 \,dx.
\end{equation*}\]</span>
This integral is large if <span class="math inline">\(m\)</span> has high curvature, and small if <span class="math inline">\(m\)</span> is
nearly linear. (For a linear function <span class="math inline">\(m(x) = a + bx\)</span> we have <span class="math inline">\(m&#39;&#39;(x) = 0\)</span>
and thus the integral equals zero.)</p>
<div class="definition">
<p><span id="def:smoothing-spline" class="definition"><strong>Definition 6.1  </strong></span>The <strong>smoothing spline</strong> estimate for the regression function <span class="math inline">\(m\)</span> is the
function <span class="math inline">\(\hat m_\lambda\)</span> which minimizes
<span class="math display" id="eq:spline-penalty">\[\begin{equation}
  \sum_{i=1}^n \bigl( y_i - m(x_i) \bigr)^2 + \lambda \int_{-\infty}^\infty \bigl( m&#39;&#39;(x) \bigr)^2 \,dx
  \tag{6.1}
\end{equation}\]</span>
over all twice differentiable functions <span class="math inline">\(m\colon \mathbb{R}\to \mathbb{R}\)</span>.
The parameter <span class="math inline">\(\lambda \geq 0\)</span> is called the <strong>smoothing parameter</strong>.</p>
</div>
<p>The smoothing parameter <span class="math inline">\(\lambda\)</span> controls the trade-off between fitting
the data and smoothness:</p>
<ul>
<li>For <span class="math inline">\(\lambda = 0\)</span>, only the residual sum of squares matters, and the
solution is the interpolating function which passes through all data
points.</li>
<li>For <span class="math inline">\(\lambda \to \infty\)</span>, only the smoothness matters, and the solution
is the linear regression line (which has zero curvature).</li>
<li>For intermediate values of <span class="math inline">\(\lambda\)</span>, the solution balances fit and
smoothness.</li>
</ul>
<p>This is similar to ridge regression (see <a href="https://seehuhn.github.io/MATH3714/S16-ridge.html">section 16 of the level 3
notes</a>), where a penalty
term <span class="math inline">\(\lambda \|\beta\|^2\)</span> is added to the residual sum of squares to
control the size of the coefficients.</p>
</div>
<div id="cubic-splines" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Cubic Splines<a href="X06-splines.html#cubic-splines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we can state the solution to the optimization problem in
definition <a href="X06-splines.html#def:smoothing-spline">6.1</a>, we need to introduce the concept
of a <strong>cubic spline</strong>.</p>
<div class="definition">
<p><span id="def:cubic-spline" class="definition"><strong>Definition 6.2  </strong></span>A <strong>cubic spline</strong> with knots <span class="math inline">\(\kappa_1 &lt; \kappa_2 &lt; \cdots &lt; \kappa_k\)</span> is
a function <span class="math inline">\(s\colon \mathbb{R}\to \mathbb{R}\)</span> such that</p>
<ul>
<li><span class="math inline">\(s\)</span> is a cubic polynomial on each interval <span class="math inline">\((-\infty, \kappa_1)\)</span>,
<span class="math inline">\((\kappa_1, \kappa_2)\)</span>, …, <span class="math inline">\((\kappa_{k-1}, \kappa_k)\)</span>, and
<span class="math inline">\((\kappa_k, \infty)\)</span>, and</li>
<li><span class="math inline">\(s\)</span> is twice continuously differentiable, <em>i.e.</em> <span class="math inline">\(s\)</span>, <span class="math inline">\(s&#39;\)</span> and <span class="math inline">\(s&#39;&#39;\)</span>
are all continuous.</li>
</ul>
<p>A <strong>natural cubic spline</strong> is a cubic spline which is linear on the
intervals <span class="math inline">\((-\infty, \kappa_1)\)</span> and <span class="math inline">\((\kappa_k, \infty)\)</span>.</p>
</div>
<p>The points <span class="math inline">\(\kappa_1, \ldots, \kappa_k\)</span> are called <strong>knots</strong>. At each
knot, the function <span class="math inline">\(s\)</span> transitions from one cubic polynomial to another,
but the transition is smooth because <span class="math inline">\(s\)</span>, <span class="math inline">\(s&#39;\)</span> and <span class="math inline">\(s&#39;&#39;\)</span> are all continuous.</p>
<div class="example">
<p><span id="exm:unlabeled-div-14" class="example"><strong>Example 6.1  </strong></span>TODO: add a simple example showing a cubic spline with 2-3 knots</p>
</div>
<p>The following theorem shows that the solution to the smoothing spline
problem is always a natural cubic spline.</p>
<div class="theorem">
<p><span id="thm:spline-solution" class="theorem"><strong>Theorem 6.1  </strong></span>The solution <span class="math inline">\(\hat m_\lambda\)</span> to the optimization problem in
definition <a href="X06-splines.html#def:smoothing-spline">6.1</a> is a natural cubic spline with
knots at the data points <span class="math inline">\(x_1, \ldots, x_n\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em>. </span>We sketch the main idea of the proof, showing that the solution must be
a cubic spline. (The full proof, including the boundary conditions which
make the spline “natural”, is more technical.)</p>
<p>Suppose <span class="math inline">\(\hat m_\lambda\)</span> is the solution but <span class="math inline">\(\hat m_\lambda\)</span> is not a
cubic polynomial on some interval <span class="math inline">\([x_i, x_{i+1}]\)</span>. We will show that
this leads to a contradiction.</p>
<p>Let <span class="math inline">\(p\)</span> be the cubic polynomial which interpolates <span class="math inline">\(\hat m_\lambda\)</span>,
<span class="math inline">\(\hat m_\lambda&#39;\)</span>, <span class="math inline">\(\hat m_\lambda&#39;&#39;\)</span> at <span class="math inline">\(x_i\)</span> and <span class="math inline">\(\hat m_\lambda&#39;&#39;\)</span>
at <span class="math inline">\(x_{i+1}\)</span>. (Such a polynomial exists and is unique.) Now consider
the function <span class="math inline">\(\tilde m\)</span> which equals <span class="math inline">\(\hat m_\lambda\)</span> outside
<span class="math inline">\([x_i, x_{i+1}]\)</span> and equals <span class="math inline">\(p\)</span> on <span class="math inline">\([x_i, x_{i+1}]\)</span>.</p>
<p>Since <span class="math inline">\(\tilde m\)</span> agrees with <span class="math inline">\(\hat m_\lambda\)</span> at all data points, the
residual sum of squares is the same for both functions. However, the
penalty term is smaller for <span class="math inline">\(\tilde m\)</span>:
<span class="math display">\[\begin{equation*}
  \int_{-\infty}^\infty \bigl( \tilde m&#39;&#39;(x) \bigr)^2 \,dx
  &lt; \int_{-\infty}^\infty \bigl( \hat m_\lambda&#39;&#39;(x) \bigr)^2 \,dx.
\end{equation*}\]</span>
This is because <span class="math inline">\(p&#39;&#39;\)</span> is linear (the second derivative of a cubic
polynomial), and among all functions with given values at the endpoints
of an interval, the linear function has the smallest integral of the
square.</p>
<p>This contradicts the assumption that <span class="math inline">\(\hat m_\lambda\)</span> minimizes
equation <a href="X06-splines.html#eq:spline-penalty">(6.1)</a>. Therefore, <span class="math inline">\(\hat m_\lambda\)</span> must be
a cubic polynomial on each interval <span class="math inline">\([x_i, x_{i+1}]\)</span>.</p>
<p>A similar argument shows that <span class="math inline">\(\hat m_\lambda\)</span> must be twice continuously
differentiable at the knots, completing the proof that <span class="math inline">\(\hat m_\lambda\)</span>
is a cubic spline.</p>
</div>
<p>The theorem shows that the smoothing spline has knots at <em>all</em> data
points <span class="math inline">\(x_1, \ldots, x_n\)</span>. This might seem surprising, since a spline
with <span class="math inline">\(n\)</span> knots has <span class="math inline">\(n + 4\)</span> degrees of freedom (four coefficients for each
of the <span class="math inline">\(n+1\)</span> pieces, minus <span class="math inline">\(3n\)</span> constraints from continuity and
smoothness, giving <span class="math inline">\(4(n+1) - 3n = n + 4\)</span>). However, the penalty term
<span class="math inline">\(\lambda \int (m&#39;&#39;)^2 dx\)</span> prevents overfitting by penalizing functions
with high curvature.</p>
</div>
<div id="degrees-of-freedom" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Degrees of Freedom<a href="X06-splines.html#degrees-of-freedom" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As with linear regression, we can write the smoothing spline estimate in
matrix form. Let <span class="math inline">\(y = (y_1, \ldots, y_n)^\top\)</span> be the vector of
responses, and let <span class="math inline">\(\hat y = (\hat m_\lambda(x_1), \ldots,
\hat m_\lambda(x_n))^\top\)</span> be the vector of fitted values. Then we can
write
<span class="math display" id="eq:smoother-matrix">\[\begin{equation}
  \hat y
  = S_\lambda y,  \tag{6.2}
\end{equation}\]</span>
where <span class="math inline">\(S_\lambda \in \mathbb{R}^{n\times n}\)</span> is the <strong>smoother matrix</strong>. This is
analogous to the hat matrix <span class="math inline">\(H = X(X^\top X)^{-1}X^\top\)</span> in linear
regression (see <a href="https://seehuhn.github.io/MATH3714/S02-multiple.html">section 2 of the level 3
notes</a>).</p>
<p>The smoother matrix <span class="math inline">\(S_\lambda\)</span> depends on the smoothing parameter
<span class="math inline">\(\lambda\)</span> and on the data points <span class="math inline">\(x_1, \ldots, x_n\)</span>, but not on the
responses <span class="math inline">\(y_1, \ldots, y_n\)</span>. We will not give the explicit formula for
<span class="math inline">\(S_\lambda\)</span> here, but note that it can be computed efficiently.</p>
<div class="definition">
<p><span id="def:df-spline" class="definition"><strong>Definition 6.3  </strong></span>The <strong>effective degrees of freedom</strong> of the smoothing spline estimate is
<span class="math display">\[\begin{equation*}
  \mathop{\mathrm{df}}\nolimits(\lambda)
  = \mathop{\mathrm{tr}}\nolimits(S_\lambda),
\end{equation*}\]</span>
where <span class="math inline">\(\mathop{\mathrm{tr}}\nolimits\)</span> denotes the trace of a matrix (the sum of the diagonal
elements).</p>
</div>
<p>The effective degrees of freedom measure the complexity of the fitted
model:</p>
<ul>
<li>For <span class="math inline">\(\lambda = 0\)</span>, the smoother matrix is <span class="math inline">\(S_0 = I\)</span> (the identity
matrix), and we have <span class="math inline">\(\mathop{\mathrm{df}}\nolimits(0) = n\)</span>. This corresponds to interpolation,
where we use all <span class="math inline">\(n\)</span> data points.</li>
<li>For <span class="math inline">\(\lambda \to \infty\)</span>, the smoother matrix converges to the hat
matrix of linear regression, and we have <span class="math inline">\(\mathop{\mathrm{df}}\nolimits(\infty) = 2\)</span>. This
corresponds to fitting a straight line.</li>
<li>For intermediate values of <span class="math inline">\(\lambda\)</span>, we have <span class="math inline">\(2 &lt; \mathop{\mathrm{df}}\nolimits(\lambda) &lt; n\)</span>.</li>
</ul>
<p>The effective degrees of freedom provide an alternative way to specify
the amount of smoothing: instead of choosing <span class="math inline">\(\lambda\)</span> directly, we can
choose a target value for <span class="math inline">\(\mathop{\mathrm{df}}\nolimits(\lambda)\)</span> and then find the corresponding
<span class="math inline">\(\lambda\)</span>.</p>
</div>
<div id="smoothing-splines-in-r" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Smoothing Splines in R<a href="X06-splines.html#smoothing-splines-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Smoothing splines can be computed in R using the built-in function
<a href="https://rdrr.io/r/stats/smooth.spline.html"><code>smooth.spline()</code></a>. The
function takes the following arguments:</p>
<ul>
<li><code>x</code> and <code>y</code>: the data points.</li>
<li><code>spar</code>: the smoothing parameter. This is related to <span class="math inline">\(\lambda\)</span> but uses
a different scale. If not specified, a default value is chosen.</li>
<li><code>df</code>: the target degrees of freedom. If specified, the function finds
the value of <code>spar</code> which gives the desired degrees of freedom.</li>
</ul>
<p>The return value is an object which contains the fitted spline. The most
important components are:</p>
<ul>
<li><code>$x</code> and <code>$y</code>: the fitted spline evaluated at the data points (or at a
grid of points if the optional argument <code>all.knots = FALSE</code> is used).</li>
<li><code>$df</code>: the effective degrees of freedom of the fitted spline.</li>
<li><code>$lambda</code>: the smoothing parameter <span class="math inline">\(\lambda\)</span> (on a different scale than
<code>spar</code>).</li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-16" class="example"><strong>Example 6.2  </strong></span>TODO: add example using faithful dataset, comparing to kernel methods</p>
</div>
</div>
<div id="comparison-with-kernel-methods" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Comparison with Kernel Methods<a href="X06-splines.html#comparison-with-kernel-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Both smoothing splines and kernel methods can be used to estimate the
regression function <span class="math inline">\(m\)</span> from data. The two approaches have different
strengths and weaknesses.</p>
<p><strong>Advantages of smoothing splines:</strong></p>
<ul>
<li><strong>Global method:</strong> The spline is fitted to the entire dataset at once,
which can give better results at the boundaries of the data range.</li>
<li><strong>Automatic smoothness:</strong> The solution is guaranteed to be twice
continuously differentiable.</li>
<li><strong>Efficient computation:</strong> The spline can be computed by solving a
system of linear equations, without the need to perform local fits at
many points.</li>
</ul>
<p><strong>Advantages of kernel methods:</strong></p>
<ul>
<li><strong>Local adaptation:</strong> Kernel methods can adapt to local features of the
data, for example by using different bandwidths in different regions
(as in <span class="math inline">\(k\)</span>-NN methods).</li>
<li><strong>Geometric interpretation:</strong> The weighted average interpretation of
kernel methods is easier to understand than the penalized least squares
formulation.</li>
<li><strong>Robustness:</strong> Kernel methods can be made robust to outliers by using
robust kernels or by trimming extreme values.</li>
</ul>
<p>In practice, the choice between smoothing splines and kernel methods
often depends on the specific application and the properties of the data.</p>
<div class="example">
<p><span id="exm:unlabeled-div-17" class="example"><strong>Example 6.3  </strong></span>TODO: add comparison example showing spline vs. NW vs. local linear</p>
</div>
<div class="mysummary">
<p><strong>Summary</strong></p>
<ul>
<li>Smoothing splines balance fit and smoothness using a penalty term.</li>
<li>The solution is a natural cubic spline with knots at the data points.</li>
<li>The smoothing parameter <span class="math inline">\(\lambda\)</span> controls the trade-off between fit and
smoothness.</li>
<li>Effective degrees of freedom measure model complexity.</li>
<li>Smoothing splines are a global alternative to local kernel methods.</li>
</ul>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="X05-locpoly.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="X07-nearest.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": null,
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["MATH5714M.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
