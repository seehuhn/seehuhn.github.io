[["index.html", "MATH5714 Linear Regression, Robustness and Smoothing Preface", " MATH5714 Linear Regression, Robustness and Smoothing Jochen Voss University of Leeds, Semester 1, 2025/26 Preface The MATH5714M module includes all the material from MATH3714 (Linear Regression and Robustness), plus additional material on smoothing. The level 3 component covers linear regression and its extensions. In simple linear regression, we fit a line through data points \\((x_1, y_1), \\ldots, (x_n, y_n) \\in\\mathbb{R}^2\\) using the model \\[\\begin{equation*} y_i = \\alpha + \\beta x_i + \\varepsilon_i \\end{equation*}\\] for all \\(i \\in \\{1, 2, \\ldots, n\\}\\), where the aim is to find values for the intercept \\(\\alpha\\) and the slope \\(\\beta\\) such that the fitted line is as close as possible to the data points. However, linear models are not always appropriate. Figure 0.1 illustrates an example where a straight line model would be inadequate. For such data, we need more flexible methods that can capture non-linear patterns. Figure 0.1: An illustration of a dataset where a linear (straight line) model is not appropriate. The data represents a series of measurements of head acceleration in a simulated motorcycle accident, used to test crash helmets (the mcycle dataset from the MASS R package). The level 5 component introduces techniques for fitting smooth curves through data—methods collectively known as smoothing. Unlike linear regression, which imposes a straight line on all the data, smoothing methods work locally: they estimate the relationship at different points using nearby observations, then blend these local estimates together to create a smooth curve. The red curve in Figure 0.1 shows such a smoothed fit, which captures the complex pattern in the data without assuming any particular functional form. We will approach these smoothing methods gradually. We begin with the simpler problem of “kernel density estimation”: given a sample from an unknown distribution, how can we estimate the probability density? This introduces fundamental concepts (kernels, bandwidth selection, and the bias-variance trade-off) that apply throughout all smoothing methods. We then extend these ideas to regression smoothing, where we estimate the relationship between variables rather than a probability distribution. "],["X01-KDE.html", "Section 1 Kernel Density Estimation 1.1 Probability Densities 1.2 Histograms 1.3 Kernels 1.4 Kernel Density Estimation 1.5 Kernel Density Estimation in R", " Section 1 Kernel Density Estimation In this section, we suppose we are given data \\(x_1,\\ldots, x_n \\in\\mathbb{R}\\) from an unknown probability density \\(f\\), and our goal is to estimate \\(f\\) from the data. As explained in the preface, this seemingly simpler problem introduces key concepts (kernels, bandwidth selection) that we will later apply to regression smoothing. 1.1 Probability Densities Before we consider how to estimate a density, let us recall what a density is. A random variable \\(X \\in \\mathbb{R}\\) has density \\(f\\colon \\mathbb{R}\\to [0, \\infty)\\) if \\[\\begin{equation*} P\\bigl(X \\in [a,b]\\bigr) = \\int_a^b f(x) \\,dx \\end{equation*}\\] for all \\(a, b\\in\\mathbb{R}\\) with \\(a &lt; b\\). Densities are also called “probability densities” or “probability density functions”. Graphically, the integral \\(\\int_a^b f(x) \\,dx\\) can be interpreted as the area under the graph of \\(f\\). This is illustrated in figure 1.1. Figure 1.1: An illustration of how the area under the graph of a density can be interpreted as a probability. A function \\(f\\) is the density of a random variable \\(X\\) if and only if \\(f \\geq 0\\) and \\[\\begin{equation*} \\int_{-\\infty}^\\infty f(x) \\,dx = 1. \\end{equation*}\\] In the following, we will see how to estimate \\(f\\) from data. 1.2 Histograms A commonly used technique to approximate the density of a sample is to plot a histogram. To illustrate this, we use the faithful dataset built into R, which contains waiting times between eruptions and the duration of the eruption for the Old Faithful geyser in the Yellowstone National Park. (You can type help(faithful) in R to learn more about this dataset.) Here we focus on the waiting times only. A simple histogram for this dataset is shown in figure 1.2. x &lt;- faithful$waiting hist(x, probability = TRUE, main = NULL, xlab = &quot;time between eruptions [mins]&quot;) Figure 1.2: This figure shows how a histogram can be used to approximate a probability density. From the plot one can see that the density of the waiting times distribution seems to be bi-modal with peaks around 53 and 80 minutes. The histogram splits the data range into “buckets”, with bar heights proportional to the number of samples in each bucket. As we have seen, the area under the graph of the density \\(f\\) over the interval \\([a, b]\\) corresponds to the probability \\(P\\bigl(X \\in [a,b]\\bigr)\\). For the histogram to approximate the density, we need to scale the height \\(h_{a,b}\\) of the bucket \\([a, b]\\) so that the area in the histogram is also close to this probability. Since we don’t know the probability \\(P\\bigl(X \\in [a,b]\\bigr)\\) exactly, we have to approximate it as \\[\\begin{equation*} P\\bigl(X \\in [a,b]\\bigr) \\approx \\frac{n_{a,b}}{n}, \\end{equation*}\\] where \\(n_{a,b}\\) is the number of samples in the bucket \\([a,b]\\), and \\(n\\) is the total number of samples. So we need \\[\\begin{align*} (b-a) \\cdot h_{a,b} &amp;= \\mbox{area of the histogram bar} \\\\ &amp;\\approx \\mbox{area of the density} \\\\ &amp;= P\\bigl(X \\in [a,b]\\bigr) \\\\ &amp;\\approx \\frac{n_{a,b}}{n}. \\end{align*}\\] and thus we choose \\[\\begin{equation*} h_{a,b} = \\frac{1}{(b - a) n} n_{a,b}. \\end{equation*}\\] We can write the count \\(n_{a,b}\\) as a sum over the data: \\[\\begin{equation*} n_{a,b} = \\sum_{i=1}^n I_{[a,b]}(x_i), \\end{equation*}\\] where \\(I_{[a,b]}(x) = 1\\) if \\(x \\in [a,b]\\) and \\(I_{[a,b]}(x) = 0\\) otherwise. Substituting this into the formula for the histogram, we get \\[\\begin{equation} h_{a,b} = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{b - a} I_{[a,b]}(x_i). \\tag{1.1} \\end{equation}\\] This shows that the histogram can be written as an average of indicator functions. Histograms have several limitations: Histograms are only meaningful if the buckets are chosen well. If the buckets are too large, not much of the structure of \\(f\\) can be resolved. If the buckets are too small, the estimate \\(P\\bigl(X \\in [a,b]\\bigr) \\approx n_{a,b}/n\\) will be poor and the histogram will no longer resemble the shape of \\(f\\). Even if reasonable bucket sizes are chosen, the result can depend quite strongly on the exact locations of these buckets. Histograms have sharp edges at bucket boundaries, creating discontinuities in the density estimate even when the true density \\(f\\) is smooth. Kernel density estimation addresses these issues by using smooth weight functions, called “kernels”, that gradually decrease as we move away from \\(x\\). Each sample \\(x_i\\) contributes to the estimate at \\(x\\) with a weight that depends on the distance \\(|x - x_i|\\). Nearby samples get higher weight, distant samples get lower weight, and the transition is smooth. 1.3 Kernels To make this idea precise, we first define what we mean by a kernel. Definition 1.1 A kernel is a function \\(K\\colon \\mathbb{R}\\to \\mathbb{R}\\) such that \\(\\int_{-\\infty}^\\infty K(x) \\,dx = 1\\), \\(K(x) = K(-x)\\) (i.e. \\(K\\) is symmetric) and \\(K(x) \\geq 0\\) (i.e. \\(K\\) is positive) for all \\(x\\in \\mathbb{R}\\). The first condition ensures that \\(K\\) must decay as \\(|x|\\) becomes large. The second condition, symmetry, ensures that \\(K\\) is centred around \\(0\\), and the third condition, positivity, makes \\(K\\) a probability density. (While most authors list all three properties shown above, sometimes the third condition is omitted.) There are many different kernels in use. Some examples are listed below; a more exhaustive list is available on Wikipedia. Uniform Kernel: \\[\\begin{equation*} K(x) = \\begin{cases} 1/2 &amp; \\mbox{if $-1 \\leq x \\leq 1$} \\\\ 0 &amp; \\mbox{otherwise} \\end{cases} \\end{equation*}\\] Triangular Kernel: \\[\\begin{equation} K(x) = \\begin{cases} 1-|x| &amp; \\mbox{if $-1 \\leq x \\leq 1$} \\\\ 0 &amp; \\mbox{otherwise} \\end{cases} \\tag{1.2} \\end{equation}\\] Epanechnikov Kernel: \\[\\begin{equation*} K(x) = \\begin{cases} \\frac34 (1-x^2) &amp; \\mbox{if $-1 \\leq x \\leq 1$} \\\\ 0 &amp; \\mbox{otherwise} \\end{cases} \\end{equation*}\\] Gaussian Kernel: \\[\\begin{equation*} K(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigl(-x^2/2\\bigr) \\end{equation*}\\] The “width” of the kernel determines how smooth the resulting estimate is. A narrow kernel gives more weight to nearby samples and produces a more detailed (but potentially noisier) estimate, while a wider kernel averages over more distant samples and produces a smoother estimate. To control the width, we use a bandwidth parameter \\(h &gt; 0\\) and define the rescaled kernel \\(K_h\\) by \\[\\begin{equation*} K_h(y) = \\frac{1}{h} K(y/h) \\end{equation*}\\] for all \\(y\\in \\mathbb{R}\\). The bandwidth parameter \\(h\\) controls the width: small \\(h\\) gives a narrow kernel, large \\(h\\) gives a wide kernel. Lemma 1.1 If \\(K\\) is a kernel, then \\(K_h\\) is also a kernel. Proof. We need to verify the three properties from definition 1.1. Using the substitution \\(z = y/h\\) we find \\[\\begin{align*} \\int_{-\\infty}^\\infty K_h(y) \\,dy &amp;= \\int_{-\\infty}^\\infty \\frac{1}{h} K(y/h) \\,dy \\\\ &amp;= \\int_{-\\infty}^\\infty \\frac{1}{h} K(z) \\,h \\,dz \\\\ &amp;= \\int_{-\\infty}^\\infty K(z) \\,dz \\\\ &amp;= 1. \\end{align*}\\] For the second property, we have \\[\\begin{equation*} K_h(-y) = \\frac{1}{h} K(-y/h) = \\frac{1}{h} K(y/h) = K_h(y), \\end{equation*}\\] where we used the symmetry of \\(K\\). Finally, since \\(K(y/h) \\geq 0\\) for all \\(y\\), we also have \\(K_h(y) = \\frac{1}{h} K(y/h) \\geq 0\\) for all \\(y\\). Remark. Note that the choice of kernel \\(K\\) and bandwidth \\(h\\) is not unique. For a given kernel shape, different authors use different normalisations. For example, we could replace our triangular kernel \\(K\\) from equation (1.2) with \\(\\tilde K(x) = K(x/\\sqrt{6}) / \\sqrt{6}\\), which has variance 1. If we simultaneously replace \\(h\\) with \\(\\tilde h = h/\\sqrt{6}\\), then the rescaled kernel remains unchanged: \\(\\tilde K_{\\tilde h} = K_h\\). 1.4 Kernel Density Estimation We can now define the kernel density estimate. Definition 1.2 For a kernel \\(K\\), bandwidth \\(h &gt; 0\\) and \\(x \\in \\mathbb{R}\\), the kernel density estimate for \\(f(x)\\) is given by \\[\\begin{equation*} \\hat f_h(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i), \\end{equation*}\\] where \\(K_h\\) is the rescaled kernel defined above. Note the similarity to the histogram formula (1.1): both are averages of weight functions applied to the data. The key difference is that the kernel \\(K_h(x - x_i)\\) is centred around each data point \\(x_i\\), whereas the indicator function \\(I_{[a,b]}(x_i)\\) is fixed at the bucket location \\([a,b]\\). Similar to the bucket width in histograms, the bandwidth parameter \\(h\\) controls how smooth the density estimate \\(\\hat f_h\\) is: small \\(h\\) gives narrow kernels that put weight only on very nearby samples, while large \\(h\\) gives wide kernels that average over more distant samples. Later sections will discuss how to choose the kernel \\(K\\) for good properties. Remark. If the kernel \\(K\\) is continuous, then the rescaled kernel \\(K_h\\) and the kernel density estimate \\(\\hat f_h\\) are also continuous. This is a useful property, as it means that kernel density estimates avoid the discontinuities that histograms exhibit at bucket boundaries. 1.5 Kernel Density Estimation in R Kernel density estimates can be computed in R using the built-in density() function. If x is a vector containing the data, then density(x) computes a basic kernel density estimate, using the Gaussian kernel. The function has a number of optional arguments, which can be used to control details of the estimate: bw = ... can be used to control the bandwidth \\(h\\). If no numeric value is given, a heuristic is used. R standardizes all kernels to have variance 1, so the bw parameter differs from our bandwidth \\(h\\) by a kernel-dependent constant factor (see the remark after Lemma 1.1). Specifically, bw=1 corresponds to the case where the rescaled kernel \\(K_h\\) has variance 1. kernel = ... can be used to choose the kernel. Choices include \"rectangular\" (the uniform kernel), \"triangular\", \"epanechnikov\" and \"gaussian\". The default value is \"gaussian\". See help(density) for details. To illustrate the use of density(), we use a dataset about the annual amount of snow falling in Buffalo, New York for different years: # downloaded from https://teaching.seehuhn.de/data/buffalo/ x &lt;- read.csv(&quot;data/buffalo.csv&quot;) snowfall &lt;- x$snowfall The return value of density is an R object which contains information about the kernel density estimate. m &lt;- density(snowfall) str(m) ## List of 8 ## $ x : num [1:512] -4.17 -3.72 -3.26 -2.81 -2.35 ... ## $ y : num [1:512] 4.28e-06 4.94e-06 5.68e-06 6.50e-06 7.42e-06 ... ## $ bw : num 9.72 ## $ n : int 109 ## $ old.coords: logi FALSE ## $ call : language density.default(x = snowfall) ## $ data.name : chr &quot;snowfall&quot; ## $ has.na : logi FALSE ## - attr(*, &quot;class&quot;)= chr &quot;density&quot; The fields $x and $y contain the \\(x\\) and \\(y\\) coordinates, respectively, of points on the \\(x \\mapsto \\hat f_h(x)\\) curve, which approximates \\(f\\). The field $bw shows the numeric value for the bandwidth chosen by the heuristic. The returned object can also directly be used as an argument to plot() and lines(), to add the graph of \\(\\hat f_h\\) to a plot. The following code illustrates the use of density() with different bandwidth parameters. The result is shown in figure 1.3. par(mfrow = c(2,2)) for (bw in c(1, 2, 4, 8)) { plot(density(snowfall, bw = bw, kernel = &quot;triangular&quot;, n = 1000), xlim = c(25,200), ylim = c(0, 0.025), xlab = paste(&quot;bandwidth =&quot;, bw), main = NA) } Figure 1.3: This figure illustrates the effect of the bandwidth parameter on the kernel density estimate. Small bandwidths give a more detailed estimate, but also show more noise. Large bandwidths give a smoother estimate, but may miss local structure of the density. Summary Histograms approximate densities using indicator functions on fixed buckets. Kernel density estimates replace fixed buckets with smooth kernels centred on each data point. The bandwidth parameter controls the smoothness of the kernel density estimate. In R, the density() function computes kernel density estimates. "],["I01-vectorise.html", "Interlude: Vectorisation in R Direct Implementation Vectorization", " Interlude: Vectorisation in R In this section we explore techniques for writing efficient R code, using kernel density estimation as an example. We start with a straightforward implementation using for-loops, then show how to speed it up by replacing loops with operations on entire vectors and matrices at once. These techniques apply to many computational problems in statistics. For our experiments we will use the same “snowfall” dataset as in the previous section. # downloaded from https://teaching.seehuhn.de/data/buffalo/ x &lt;- read.csv(&quot;data/buffalo.csv&quot;) snowfall &lt;- x$snowfall Direct Implementation The kernel density estimate for given data \\(x_1, \\ldots, x_n \\in\\mathbb{R}\\) is \\[\\begin{equation*} \\hat f_h(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K\\Bigl( \\frac{x - x_i}{h} \\Bigr). \\end{equation*}\\] Our aim is to implement this formula in R. We use the triangular kernel from equation (1.2): K &lt;- function(x) pmax(1 - abs(x), 0) The use of pmax() allows us to evaluate K() for several \\(x\\)-values in parallel: K(c(-1, -0.5, 0, 0.5, 1)) ## [1] 0.0 0.5 1.0 0.5 0.0 With this in place, we can now easily compute the kernel density estimate. As mentioned in the previous section, the choice of kernel normalization and bandwidth is not unique: rescaling the kernel and adjusting the bandwidth can give the same \\(K_h\\). The R function density() uses a specific convention: the bw parameter is defined to be the standard deviation of the smoothing kernel. For the triangular kernel, this means that density() uses a rescaled version with variance 1, which differs from our kernel in equation (1.2) by a factor of \\(1/\\sqrt{6}\\). Thus, to get output comparable to bw=4 (bottom left panel in figure 1.3), we use \\(h = 4\\sqrt{6}\\): h &lt;- 4 * sqrt(6) # bandwidth x &lt;- 100 # the point at which to estimate f mean(1/h * K((x - snowfall) / h)) ## [1] 0.0108237 To plot the estimated density, we typically evaluate \\(\\hat f_h\\) on a grid of \\(x\\)-values. Since \\(x_1, \\ldots, x_n\\) denotes the input data, we use \\(\\tilde x_1, \\ldots, \\tilde x_m\\) for the evaluation points: x.tilde &lt;- seq(25, 200, by = 1) f.hat &lt;- numeric(length(x.tilde)) # empty vector to store the results for (j in 1:length(x.tilde)) { f.hat[j] &lt;- mean(1/h * K((x.tilde[j] - snowfall) / h)) } Figure 1.4 shows the result is similar to figure 1.3. plot(x.tilde, f.hat, type = &quot;l&quot;, xlab = &quot;snowfall&quot;, ylab = &quot;density&quot;, ylim = c(0, 0.025)) Figure 1.4: Manually computed kernel density estimate for the snowfall data, corresponding to bw=4 in density(). This code is slower than density() (as we will see below), but gives us full control over the computation. Vectorization We now explore how to speed up our implementation. First, we encapsulate the code above into a function: KDE1 &lt;- function(x.tilde, x, K, h) { f.hat &lt;- numeric(length(x.tilde)) for (j in 1:length(x.tilde)) { f.hat[j] &lt;- mean(1/h * K((x.tilde[j] - x) / h)) } f.hat } A common source of inefficiency in R is the use of loops. The for-loop in KDE1() computes differences \\(\\tilde x_j - x_i\\) for all \\(j\\). We have already avoided a second loop by treating x.tilde[j] - x as a vector operation, using pmax() in K, and using mean() for the sum. This approach of replacing explicit loops with operations on entire vectors is called vectorisation. To eliminate the loop over j, we use outer(), which applies a function to all pairs of elements from two vectors. The result is a matrix with rows corresponding to the first vector and columns to the second. For example: x &lt;- c(1, 2, 3) y &lt;- c(1, 2) outer(x, y, &quot;-&quot;) ## [,1] [,2] ## [1,] 0 -1 ## [2,] 1 0 ## [3,] 2 1 We use outer(x.tilde, x, \"-\") to compute all \\(\\tilde x_j - x_i\\) at once, giving an \\(m \\times n\\) matrix to which we apply K(). This fully vectorised implementation avoids all explicit loops: KDE2 &lt;- function(x.tilde, x, K, h) { dx &lt;- outer(x.tilde, x, &quot;-&quot;) rowMeans(1/h * K(dx / h)) } We can verify that both functions return the same result: KDE1(c(50, 100, 150), snowfall, K, h) ## [1] 0.0078239091 0.0108236983 0.0007448277 KDE2(c(50, 100, 150), snowfall, K, h) ## [1] 0.0078239091 0.0108236983 0.0007448277 There is another, minor optimisation we can make. Instead of dividing the \\(m\\times n\\) matrix elements of dx by h, we can achieve the same effect by dividing the vectors x and x.tilde, i.e. only \\(m+n\\) numbers, by \\(h\\). This reduces the number of operations required. Similarly, instead of multiplying the \\(m\\times n\\) kernel values by \\(1/h\\), we can divide the \\(m\\) row means by \\(h\\): KDE3 &lt;- function(x.tilde, x, K, h) { dx &lt;- outer(x.tilde / h, x / h, &quot;-&quot;) rowMeans(K(dx)) / h } Again, KDE3() returns the same result: KDE3(c(50, 100, 150), snowfall, K, h) ## [1] 0.0078239091 0.0108236983 0.0007448277 We measure execution times using microbenchmark: library(&quot;microbenchmark&quot;) microbenchmark( KDE1 = KDE1(x.tilde, snowfall, K, h), KDE2 = KDE2(x.tilde, snowfall, K, h), KDE3 = KDE3(x.tilde, snowfall, K, h), times = 1000) ## Unit: microseconds ## expr min lq mean median uq max neval ## KDE1 1423.274 1444.348 1497.1400 1451.8510 1465.258 5860.540 1000 ## KDE2 193.151 194.627 237.5744 195.2625 197.251 2461.640 1000 ## KDE3 173.635 175.070 209.3162 175.8080 178.022 3967.283 1000 The output shows execution times for 1000 runs. The introduction of outer() in KDE2() gives a substantial speedup, and KDE3() improves further. Finally, we compare to density(), using arguments from, to, and n to specify the same grid \\(\\tilde x\\): KDE.builtin &lt;- function(x.tilde, x, kernel_name, h) { density(x, kernel = kernel_name, bw = h / sqrt(6), from = min(x.tilde), to = max(x.tilde), n = length(x.tilde))$y } microbenchmark(density = KDE.builtin(x.tilde, snowfall, &quot;triangular&quot;, h), times = 1000) ## Unit: microseconds ## expr min lq mean median uq max neval ## density 155.062 157.7065 186.297 169.002 179.826 9034.063 1000 The result shows that density() is faster than KDE3(), but by a factor of less than 2. The reason is that density() uses a more efficient algorithm based on Fast Fourier Transform to compute an approximation to the kernel density estimate. By comparing the outputs of KDE3() and KDE.builtin() we can see that the approximation is very accurate: f.hat1 &lt;- KDE3(x.tilde, snowfall, K, h) f.hat2 &lt;- KDE.builtin(x.tilde, snowfall, &quot;triangular&quot;, h) max(abs(f.hat1 - f.hat2)) ## [1] 1.935582e-05 Summary Vectorisation is the technique of replacing explicit loops with operations on entire vectors. Vectorised code dramatically improves performance in R. The outer() function enables fully vectorised implementations for pairwise operations. Further speedups can be achieved by reducing the number of operations on large matrices. "],["X02-error.html", "Section 2 The Error of Kernel Density Estimates 2.1 A Statistical Model 2.2 Moments of Kernels 2.3 The Roughness of Kernels 2.4 Mean Squared Error 2.5 Optimal Bandwidth for Pointwise MSE 2.6 Integrated Error", " Section 2 The Error of Kernel Density Estimates In the previous section we introduced the kernel density estimate \\[\\begin{equation} \\hat f_h(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i) \\tag{2.1} \\end{equation}\\] for estimating the density \\(f\\), and we argued that \\(\\hat f_h(x) \\approx f(x)\\). The aim of the current section is to quantify the error of this approximation and to understand how this error depends on the true density \\(f\\) and on the bandwidth \\(h &gt; 0\\). 2.1 A Statistical Model As usual, we will make a statistical model for the data \\(x_1, \\ldots, x_n\\), and then use this model to analyse how well the estimator performs. The statistical model we will consider here is extremely simple: we model the \\(x_i\\) using random variables \\[\\begin{equation} X_1, \\ldots, X_n \\sim f, \\tag{2.2} \\end{equation}\\] which we assume to be independent and identically distributed (i.i.d.). Here, the notation \\(X \\sim f\\), where \\(f\\) is a probability density, simply denotes that the random variable \\(X\\) has density \\(f\\). It is important to not confuse \\(x\\) (the point where we are evaluating the densities during our analysis) with the data \\(x_i\\). A statistical model describes the data, so here we get random variables \\(X_1, \\ldots, X_n\\) to describe the behaviour of \\(x_1, \\ldots, x_n\\), but it does not describe \\(x\\). The number \\(x\\) is not part of the data, so will never be modelled by a random variable. While the model is very simple, for example it is much simpler than the model we use in the level 3 part of the module for linear regression, the associated parameter estimation problem is more challenging. The only “parameter” in this model is the function \\(f \\colon\\mathbb{R}\\to \\mathbb{R}\\) instead of just a vector of numbers. The space of all possible density functions \\(f\\) is infinite dimensional, so this is a more challenging estimation problem than the one we consider, for example, for linear regression. Since \\(f\\) is not a “parameter” in the usual sense, sometimes this problem is called a “non-parametric” estimation problem. Our estimate for the density \\(f\\) is the function \\(\\hat f_h\\colon \\mathbb{R}\\to \\mathbb{R}\\), where \\(\\hat f_h(x)\\) is given by (2.1) for every \\(x \\in\\mathbb{R}\\). 2.2 Moments of Kernels For our analysis we will need to consider properties of \\(K\\) and \\(K_h\\) in more detail. Definition 2.1 The \\(k\\)th moment of a kernel \\(K\\), for \\(k \\in \\mathbb{N}_0 = \\{0, 1, 2, \\ldots\\}\\), is given by \\[\\begin{equation*} \\mu_k(K) = \\int_{-\\infty}^\\infty x^k K(x) \\,dx. \\end{equation*}\\] The second moment \\(\\mu_2\\) is sometimes also called the variance of the kernel \\(K\\). Using the properties of \\(K\\), we find the following results: Since \\(x^0 = 1\\) for all \\(x\\in\\mathbb{R}\\), the \\(0\\)th moment is \\(\\mu_0(K) = \\int_{-\\infty}^\\infty K(x) \\,dx = 1\\) for every kernel \\(K\\). Since \\(K\\) is symmetric, the function \\(x \\mapsto x K(x)\\) is antisymmetric and we have \\[\\begin{equation*} \\mu_1(K) = \\int_{-\\infty}^\\infty x K(x) \\,dx = 0 \\end{equation*}\\] for every kernel \\(K\\). The moments of the rescaled kernel \\(K_h\\) can be computed from the moments of \\(K\\). Lemma 2.1 Let \\(K\\) be a kernel, \\(k \\in \\mathbb{N}_0\\) and \\(h &gt; 0\\). Then \\[\\begin{equation*} \\mu_k(K_h) = h^k \\mu_k(K). \\end{equation*}\\] Proof. We have \\[\\begin{align*} \\mu_k(K_h) &amp;= \\int_{-\\infty}^\\infty x^k K_h(x) \\,dx \\\\ &amp;= \\int_{-\\infty}^\\infty x^k \\frac1h K\\Bigl(\\frac{x}{h}\\Bigr) \\,dx. \\end{align*}\\] Using the substitution \\(y = x/h\\) we find \\[\\begin{align*} \\mu_k(K_h) &amp;= \\int_{-\\infty}^\\infty (hy)^k \\frac1h K(y) \\, h \\,dy \\\\ &amp;= h^k \\int_{-\\infty}^\\infty y^k K(y) \\,dy \\\\ &amp;= h^k \\mu_k(K). \\end{align*}\\] This completes the proof. By lemma 1.1, if \\(K\\) is a kernel, then \\(K_h\\) is also a kernel which implies that \\(K_h\\) is a probability density. If \\(Y\\) is a random variable with density \\(K_h\\), written as \\(Y \\sim K_h\\) in short, then we find \\[\\begin{equation*} \\mathbb{E}(Y) = \\int y K_h(y) \\,dy = \\mu_1(K_h) = 0 \\end{equation*}\\] and \\[\\begin{equation} \\mathop{\\mathrm{Var}}(Y) = \\mathbb{E}(Y^2) = \\int y^2 K_h(y) \\,dy = \\mu_2(K_h) = h^2 \\, \\mu_2(K). \\tag{2.3} \\end{equation}\\] Thus, \\(Y\\) is centred and the variance of \\(Y\\) is proportional to \\(h^2\\). 2.3 The Roughness of Kernels In addition to the moments of a kernel, we will need one more kernel property for our analysis. Definition 2.2 The roughness of a kernel \\(K\\) is given by \\[\\begin{equation*} R(K) := \\int_{-\\infty}^\\infty K(x)^2 \\,dx. \\end{equation*}\\] The roughness measures how concentrated the kernel is: a kernel with higher roughness has more mass concentrated in a smaller region, while a kernel with lower roughness is more spread out. Similar to the moments, the roughness of the rescaled kernel \\(K_h\\) can be expressed in terms of the roughness of \\(K\\). Lemma 2.2 Let \\(K\\) be a kernel and \\(h &gt; 0\\). Then \\[\\begin{equation*} R(K_h) = \\frac{1}{h} R(K). \\end{equation*}\\] Proof. We have \\[\\begin{align*} R(K_h) &amp;= \\int_{-\\infty}^\\infty K_h(x)^2 \\,dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\Bigl( \\frac{1}{h} K\\Bigl(\\frac{x}{h}\\Bigr) \\Bigr)^2 \\,dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\frac{1}{h^2} K\\Bigl(\\frac{x}{h}\\Bigr)^2 \\,dx. \\end{align*}\\] Using the substitution \\(y = x/h\\) we find \\[\\begin{align*} R(K_h) &amp;= \\int_{-\\infty}^\\infty \\frac{1}{h^2} K(y)^2 \\, h \\,dy \\\\ &amp;= \\frac{1}{h} \\int_{-\\infty}^\\infty K(y)^2 \\,dy \\\\ &amp;= \\frac{1}{h} R(K). \\end{align*}\\] This completes the proof. 2.4 Mean Squared Error In this section we will quantify the error of the kernel density estimate \\(\\hat f_h(x)\\) for estimating \\(f(x)\\) by computing its mean squared error. The mean squared error combines two sources of error: bias (systematic error) and variance (random error due to sampling). We will see how the bandwidth \\(h\\) affects both of these components, and how they must be balanced to minimise the overall error. 2.4.1 Result The following theorem characterises the mean squared error of the kernel density estimate for small bandwidth \\(h\\). Theorem 2.1 Let \\(X_1, \\ldots, X_n\\) be i.i.d. random variables with density \\(f\\), and let \\(\\hat f_h(x)\\) be the kernel density estimate defined in (2.1). Then, as \\(h \\downarrow 0\\), the mean squared error of \\(\\hat f_h(x)\\) is given by \\[\\begin{equation} \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) = \\frac{1}{nh} f(x) R(K) + \\frac14 \\mu_2(K)^2 f&#39;&#39;(x)^2 h^4 + o(1/nh) + o(h^4), \\tag{2.4} \\end{equation}\\] where \\(R(K) = \\int K(x)^2 \\,dx\\) is the roughness of the kernel and \\(\\mu_2(K) = \\int x^2 K(x) \\,dx\\) is the second moment of the kernel. Here we use little-o notation: the term \\(o(1/nh)\\) represents error terms of the form \\(e_1(h)/n\\) where \\(e_1(h) = o(1/h)\\) as \\(h \\downarrow 0\\), meaning that \\(e_1(h) / (1/h) \\to 0\\), and the constants in the definition of \\(o(1/h)\\) do not depend on \\(n\\). Similarly, \\(o(h^4)\\) represents error terms \\(e_2(h)\\) which do not depend on \\(n\\) and satisfy \\(e_2(h) / h^4 \\to 0\\) as \\(h \\downarrow 0\\). Thus \\(o(1/nh)\\) really means ``\\(o(1/h)\\), where the constants are proportional to \\(1/n\\)’’, and the constants in the definition of \\(o(h^4)\\) do not depend on \\(n\\). The theorem shows that the MSE has two main terms with opposing behaviour: the first term \\(\\frac{1}{nh} f(x) R(K)\\) comes from the variance of the estimate and decreases as \\(h\\) increases, while the second term \\(\\frac14 \\mu_2(K)^2 f&#39;&#39;(x)^2 h^4\\) comes from the squared bias and increases as \\(h\\) increases. These competing effects lead to an optimal choice of bandwidth that balances bias and variance. We will not give a complete formal proof of this theorem, but we will derive the main components in the following subsections. First we compute the bias of the estimate and show that it is approximately \\(\\frac{\\mu_2(K) f&#39;&#39;(x)}{2} h^2\\) for small \\(h\\). Then we compute the variance and show that it is approximately \\(\\frac{f(x) R(K)}{nh}\\) for small \\(h\\). Combining these results using the standard identity \\(\\mathop{\\mathrm{MSE}}\\nolimits= \\mathop{\\mathrm{Var}}+ \\mathop{\\mathrm{bias}}^2\\) yields the theorem. 2.4.2 Bias 2.4.2.1 The Bias of the Estimate As usual, the bias of our estimate is the difference between what the estimator gives on average and the truth. For our estimation problem we get \\[\\begin{equation*} \\mathop{\\mathrm{bias}}\\bigl(\\hat f_h(x)\\bigr) = \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) - f(x). \\end{equation*}\\] The expectation on the right-hand side averages over the randomness in the data, by using \\(X_1, \\ldots, X_n\\) from the model in place of the data. Substituting in the definition of \\(\\hat f_h(x)\\) from equation (2.1) we find \\[\\begin{align*} \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) &amp;= \\mathbb{E}\\Bigl( \\frac{1}{n} \\sum_{i=1}^n K_h(x - X_i) \\Bigr) \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}\\bigl( K_h(x - X_i) \\bigr) \\end{align*}\\] and since the \\(X_i\\) are identically distributed, we can replace all \\(X_i\\) with \\(X_1\\) (or any other of them) to get \\[\\begin{align*} \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) &amp;= \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}\\bigl( K_h(x - X_1) \\bigr) \\\\ &amp;= \\frac{1}{n} n \\, \\mathbb{E}\\bigl( K_h(x - X_1) \\bigr) \\\\ &amp;= \\mathbb{E}\\bigl( K_h(x - X_1) \\bigr). \\end{align*}\\] Since the model assumes \\(X_1\\) (and all the other \\(X_i\\)) to have density \\(f\\), we can write this expectation as an integral to get \\[\\begin{align*} \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) &amp;= \\int_{-\\infty}^\\infty K_h(x - y) \\, f(y) \\, dy \\\\ &amp;= \\int_{-\\infty}^\\infty f(y) \\, K_h(y - x) \\, dy \\\\ &amp;= \\int_{-\\infty}^\\infty f(z+x) \\, K_h(z) \\, dz \\end{align*}\\] where we used the symmetry of \\(K_h\\) and the substitution \\(z = y - x\\). 2.4.2.2 The Bias for Small Bandwidth Considering again the formula \\[\\begin{equation*} \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) = \\int_{-\\infty}^\\infty f(x+y) \\, K_h(y) \\, dy, \\end{equation*}\\] we see that we can interpret this integral as an expectation with respect to a random variable \\(Y \\sim K_h\\): \\[\\begin{equation} \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) = \\mathbb{E}\\bigl( f(x+Y) \\bigr). \\tag{2.5} \\end{equation}\\] Equation (2.3) shows that for \\(h \\downarrow 0\\) the random variable concentrates more and more around \\(0\\) and thus \\(x+Y\\) concentrates more and more around \\(x\\). For this reason we expect \\(\\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) \\approx f(x)\\) for small \\(h\\). To get a more quantitative version of this argument, we consider the Taylor approximation of \\(f\\) around the point \\(x\\): \\[\\begin{equation*} f(x + y) \\approx f(x) + y f&#39;(x) + \\frac{y^2}{2} f&#39;&#39;(x). \\end{equation*}\\] Substituting this into equation (2.5) we find \\[\\begin{align*} \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) &amp;\\approx \\mathbb{E}\\Bigl( f(x) + Y f&#39;(x) + \\frac{Y^2}{2} f&#39;&#39;(x) \\Bigr) \\\\ &amp;= f(x) + \\mathbb{E}(Y) f&#39;(x) + \\frac12 \\mathbb{E}(Y^2) f&#39;&#39;(x) \\\\ &amp;= f(x) + \\frac12 h^2 \\mu_2(K) f&#39;&#39;(x) \\end{align*}\\] for small \\(h\\). Considering the bias again, this gives \\[\\begin{equation} \\mathop{\\mathrm{bias}}\\bigl( \\hat f_h(x) \\bigr) = \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) - f(x) \\approx \\frac{\\mu_2(K) f&#39;&#39;(x)}{2} h^2 \\tag{2.6} \\end{equation}\\] which shows that the bias of the estimator decreases quadratically as \\(h\\) gets smaller. 2.4.3 Variance We now turn to computing the variance of the estimator. Recall from the bias analysis above that \\[\\begin{equation} \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) = \\mathbb{E}\\bigl( K_h(x - X_i) \\bigr) \\tag{2.7} \\end{equation}\\] for all \\(i \\in \\{1, \\ldots, n\\}\\). We will use similar arguments to derive the variance. We use the formula \\[\\begin{equation*} \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) = \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) - \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2 \\end{equation*}\\] and consider the two terms separately. 2.4.3.1 Second Moment For the second moment term in the definition of the variance we get \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) &amp;= \\mathbb{E}\\Bigl( \\frac{1}{n} \\sum_{i=1}^n K_h(x - X_i) \\frac{1}{n} \\sum_{j=1}^n K_h(x - X_j) \\Bigr) \\\\ &amp;= \\frac{1}{n^2} \\mathbb{E}\\Bigl( \\sum_{i,j=1}^n K_h(x - X_i) K_h(x - X_j) \\Bigr) \\\\ &amp;= \\frac{1}{n^2} \\sum_{i,j=1}^n \\mathbb{E}\\Bigl( K_h(x - X_i) K_h(x - X_j) \\Bigr) \\end{align*}\\] Since the \\(X_i\\) are independent, the values of \\(i\\) and \\(j\\) in this sum do not matter. For the \\(n\\) terms where \\(i=j\\) we can assume that both indices equal 1, and for the \\(n(n-1)\\) terms where \\(i\\neq j\\) we can assume \\(i=1\\) and \\(j=2\\), without changing the value of the expectation. So we get \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) &amp;= \\frac{1}{n^2} \\Bigl( n \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\Bigr) + n(n-1) \\mathbb{E}\\bigl( K_h(x - X_1) K_h(x - X_2) \\bigr) \\Bigr) \\\\ &amp;= \\frac{1}{n^2} \\Bigl( n \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\Bigr) + n(n-1) \\mathbb{E}\\bigl( K_h(x - X_1) \\bigr) \\mathbb{E}\\bigl( K_h(x - X_2) \\bigr) \\Bigr) \\\\ &amp;= \\frac{1}{n^2} \\Bigl( n \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\Bigr) + n(n-1) \\mathbb{E}\\bigl( K_h(x - X_1) \\bigr)^2 \\Bigr) \\\\ &amp;= \\frac{1}{n} \\mathbb{E}\\Bigl( K_h(x - X_1)^2 \\Bigr) + \\frac{n-1}{n} \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2, \\end{align*}\\] where we used equation (2.7) for the last term. Using this equation, we can write the variance as \\[\\begin{align*} \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) &amp;= \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) - \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2 \\\\ &amp;= \\frac{1}{n} \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\bigr) + \\Bigl(\\frac{n-1}{n} - 1\\Bigr) \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2. \\end{align*}\\] Since \\((n-1)/n - 1 = -1/n\\) we get \\[\\begin{equation} \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) = \\frac{1}{n} \\Bigl( \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\bigr) - \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2 \\Bigr). \\tag{2.8} \\end{equation}\\] 2.4.3.2 Approximating \\(\\mathbb{E}\\bigl( K_h(x - X_1)^2 \\bigr)\\) Similar to the bias analysis, we will use Taylor expansion of \\(f\\) around the point \\(x\\) to understand the behaviour of the variance for small \\(h\\). Some more care is needed here, because this time the result also depends on the sample size \\(n\\) and we will consider the joint limit of \\(n \\to \\infty\\) and \\(h\\to 0\\). For the first term in equation (2.8) we find \\[\\begin{align*} \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\bigr) &amp;= \\int K_h(x - y)^2 f(y) \\,dy \\\\ &amp;= \\int K_h(y - x)^2 f(y) \\,dy \\\\ &amp;= \\int \\frac{1}{h^2} K\\Bigl( \\frac{y - x}{h} \\Bigr)^2 f(y) \\,dy. \\end{align*}\\] Now we use the substitution \\(z = (y - x) / h\\). This gives \\[\\begin{align*} \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\bigr) &amp;= \\int \\frac{1}{h^2} K(z)^2 f(x + hz) \\,h \\,dz \\end{align*}\\] Finally, we use Taylor approximation to get \\[\\begin{align*} \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\bigr) &amp;\\approx \\int \\frac{1}{h} K(z)^2 \\Bigl( f(x) + hz\\,f&#39;(x) + \\frac12 h^2 z^2 \\,f&#39;&#39;(x) \\Bigr) \\,dz \\\\ &amp;= \\frac{1}{h} f(x) \\int K(z)^2 \\,dz + f&#39;(x) \\int z K(z)^2 \\,dz + \\frac12 h f&#39;&#39;(x) \\int z^2 K(z)^2 \\,dz \\\\ &amp;= \\frac{1}{h} f(x) \\int K(z)^2 \\,dz + \\frac12 h f&#39;&#39;(x) \\int z^2 K(z)^2 \\,dz \\end{align*}\\] where the first-order term disappears since \\(z \\mapsto z K(z)^2\\) is an antisymmetric function. Using the definition of the roughness of a kernel, \\(R(K) = \\int K(x)^2 \\,dx\\), this gives the result \\[\\begin{equation} \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\bigr) \\approx \\frac{1}{h} f(x) R(K) + \\frac12 h f&#39;&#39;(x) \\int z^2 K(z)^2 \\,dz \\tag{2.9} \\end{equation}\\] for small \\(h\\). 2.4.3.3 The Variance for Small Bandwidth Here we consider the term \\(\\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2\\) in the formula for the variance. From the bias analysis above we know \\[\\begin{equation*} \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) \\approx f(x) + \\frac12 h^2 \\mu_2(K) f&#39;&#39;(x) \\end{equation*}\\] and thus we get \\[\\begin{equation} \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2 \\approx f(x)^2 + h^2 \\mu_2(K) f(x) f&#39;&#39;(x) + \\frac14 h^4 \\mu_2(K)^2 f&#39;&#39;(x)^2 \\tag{2.10} \\end{equation}\\] for small \\(h\\). Substituting equations (2.9) and (2.10) into equation (2.8) we finally find \\[\\begin{equation*} \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) = \\frac{1}{n} \\Bigl( \\frac{1}{h} f(x) R(K) - f(x)^2 + \\cdots \\Bigr), \\end{equation*}\\] where all the omitted terms go to zero as \\(h \\downarrow 0\\). Omitting one more term and keeping only the leading term we find \\[\\begin{equation} \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) \\approx \\frac{1}{nh} f(x) R(K) \\tag{2.11} \\end{equation}\\] as \\(h\\downarrow 0\\). 2.4.4 Result Recall that the mean squared error can be decomposed as \\[\\begin{equation*} \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) = \\mathbb{E}\\Bigl( \\bigl( \\hat f_h(x) - f(x) \\bigr)^2 \\Bigr) = \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) + \\mathop{\\mathrm{bias}}\\bigl( \\hat f_h(x) \\bigr)^2. \\end{equation*}\\] Substituting equations (2.6) and (2.11) into the formula for the MSE, we obtain the main two terms in theorem 2.1: \\[\\begin{equation*} \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) \\approx \\frac{1}{nh} f(x) R(K) + \\frac14 \\mu_2(K)^2 f&#39;&#39;(x)^2 h^4. \\end{equation*}\\] The additional error terms \\(o(1/nh)\\) and \\(o(h^4)\\) in the theorem statement account for the higher-order terms neglected in the Taylor approximations used to derive the bias and variance. These two main terms have opposing behaviour: the variance term \\(\\frac{1}{nh} f(x) R(K)\\) increases as \\(h \\downarrow 0\\), whilst the squared bias term \\(\\frac14 \\mu_2(K)^2 f&#39;&#39;(x)^2 h^4\\) decreases as \\(h \\downarrow 0\\). We will need to balance these two effects to find the optimal value of \\(h\\). 2.5 Optimal Bandwidth for Pointwise MSE The two terms on the right-hand side of (2.4) are balanced in that the first term decreases for large \\(h\\) while the second term decreases for small \\(h\\). By taking derivatives with respect to \\(h\\), we can find the optimal value of \\(h\\). Ignoring the higher order terms, we get \\[\\begin{equation*} \\frac{\\partial}{\\partial h} \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) = -\\frac{1}{nh^2} f(x) R(K) + \\mu_2(K)^2 f&#39;&#39;(x)^2 h^3 \\end{equation*}\\] and thus the derivative equals zero, if \\[\\begin{equation*} \\frac{1}{nh^2} f(x) R(K) = \\mu_2(K)^2 f&#39;&#39;(x)^2 h^3 \\end{equation*}\\] or, equivalently, \\[\\begin{equation*} h = h_\\mathrm{opt} := \\Bigl( \\frac{f(x) R(K)}{n \\mu_2(K)^2 f&#39;&#39;(x)^2} \\Bigr)^{1/5}. \\end{equation*}\\] It is easy to check that this \\(h\\) corresponds to the minimum of the MSE. This shows how the optimal bandwidth depends both on the kernel and on the target density \\(f\\). In practice, this formula is hard to use, since \\(f&#39;&#39;\\) is unknown. (We don’t even know \\(f\\)!) Substituting the optimal value of \\(h\\) back into equation (2.4), we get \\[\\begin{align*} \\mathop{\\mathrm{MSE}}\\nolimits_\\mathrm{opt} &amp;= \\frac{1}{n} f(x) R(K) \\Bigl( \\frac{n \\mu_2(K)^2 f&#39;&#39;(x)^2}{f(x) R(K)} \\Bigr)^{1/5} + \\frac14 \\mu_2(K)^2 f&#39;&#39;(x)^2 \\Bigl( \\frac{f(x) R(K)}{n \\mu_2(K)^2 f&#39;&#39;(x)^2} \\Bigr)^{4/5} \\\\ &amp;= \\frac54 \\; \\frac{1}{n^{4/5}} \\; \\Bigl( R(K)^2 \\mu_2(K) \\Bigr)^{2/5} \\; \\Bigl( f(x)^2 |f&#39;&#39;(x)| \\Bigr)^{2/5}. \\end{align*}\\] This expression clearly shows the contribution of \\(n\\), \\(K\\) and \\(f\\): If the bandwidth is chosen optimally, as \\(n\\) increases the bandwidth \\(h\\) decreases proportionally to \\(1/n^{1/5}\\) and the MSE decreases proportionally to \\(1 / n^{4/5}\\). For comparison, in a Monte Carlo estimate for an expectation, the MSE decreases proportionally to \\(1/n\\). The error in kernel density estimation decreases slightly slower than for Monte Carlo estimates. The error is proportional to \\(\\bigl( R(K)^2 \\mu_2(K) \\bigr)^{2/5}\\). Thus we should use kernels where the value \\(R(K)^2 \\mu_2(K)\\) is small. The error is proportional to \\(\\bigl( f(x)^2 |f&#39;&#39;(x)| \\bigr)^{2/5}\\). We cannot influence this term, but we can see that \\(x\\) where \\(f\\) is large or has high curvature have higher estimation error. 2.6 Integrated Error Equation (2.4) gives the mean squared error when trying to estimate the density \\(f(x)\\) at a fixed point \\(x\\). Usually we are interested in estimating the function \\(f\\) rather than individual points \\(f(x)\\). In this case, we consider the integrated mean squared error (IMSE): \\[\\begin{equation*} \\mathrm{IMSE}\\bigl( \\hat f_h \\bigr) := \\int_{-\\infty}^\\infty \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) \\,dx. \\end{equation*}\\] Using our result from above we find \\[\\begin{align*} \\mathrm{IMSE}\\bigl( \\hat f_h \\bigr) &amp;\\approx \\int \\Bigl( \\frac{1}{nh} f(x) R(K) + \\frac14 \\mu_2(K)^2 f&#39;&#39;(x)^2 h^4 \\Bigr) \\,dx \\\\ &amp;= \\frac{1}{nh} R(K) \\int f(x) \\,dx + \\frac{h^4}{4} \\mu_2(K)^2 \\int f&#39;&#39;(x)^2 \\,dx \\\\ &amp;= \\frac{1}{nh} R(K) + \\frac{1}{4} \\mu_2(K)^2 R(f&#39;&#39;) h^4, \\end{align*}\\] where we (mis-)use the definition of roughness as an abbreviation to express the integral over \\(f&#39;&#39;\\). As in the pointwise case above, we can use differentiation to find the optimal value of \\(h\\). Here we get \\[\\begin{equation*} h_\\mathrm{opt} = \\Bigl( \\frac{R(K)}{n \\mu_2(K)^2 R(f&#39;&#39;)} \\Bigr)^{1/5}. \\end{equation*}\\] and the corresponding error is \\[\\begin{equation} \\mathrm{IMSE}_\\mathrm{opt} = \\frac54 \\; \\frac{1}{n^{4/5}} \\; \\Bigl( R(K)^2 \\mu_2(K) \\Bigr)^{2/5} \\; R(f&#39;&#39;)^{1/5}. \\tag{2.12} \\end{equation}\\] Thus, in order to minimise the error we still need to choose \\(h \\propto n^{-1/5}\\) and we should choose a kernel \\(K\\) which minimises the value \\(R(K)^2 \\mu_2(K)\\). Summary We introduced a statistical model for density estimation where data are i.i.d. samples from the unknown density \\(f\\). Kernel moments \\(\\mu_k(K) = \\int x^k K(x) dx\\) characterise kernel properties and scale with bandwidth: \\(\\mu_k(K_h) = h^k \\mu_k(K)\\). For small bandwidth \\(h\\), the bias is approximately \\(\\frac{\\mu_2(K) f&#39;&#39;(x)}{2} h^2\\), decreasing quadratically as \\(h \\to 0\\). The variance for small \\(h\\) is approximately \\(\\frac{f(x) R(K)}{nh}\\), where \\(R(K) = \\int K(x)^2 dx\\) is the kernel roughness, and increases as \\(h \\to 0\\). The mean squared error (MSE) combines both bias and variance. The integrated mean squared error (IMSE) integrates the MSE over all points and provides a global measure of estimation quality. "],["X03-practice.html", "Section 3 Kernel Density Estimation in Practice 3.1 Choice of Kernel 3.2 Bandwidth Selection 3.3 Higher Dimensions", " Section 3 Kernel Density Estimation in Practice In this section we conclude our discussion of kernel density estimation by considering practical aspects of implementing the method. We discuss how to choose a kernel, how to select an appropriate bandwidth in practice, and how the method extends to higher dimensions. 3.1 Choice of Kernel The integrated error in equation (2.12) is proportional to \\(\\bigl( R(K)^2 \\mu_2(K) \\bigr)^{2/5}\\), and none of the remaining terms in the equation depends on the choice of the kernel. Thus, we can minimise the error by choosing a kernel which has minimal \\(R(K)^2 \\mu_2(K)\\). For a given kernel, it is easy to work out the value of \\(R(K)^2 \\mu_2(K)\\). Example 3.1 For the uniform kernel we have \\[\\begin{equation*} K(x) = \\begin{cases} 1/2 &amp; \\mbox{if $-1 \\leq x \\leq 1$} \\\\ 0 &amp; \\mbox{otherwise.} \\end{cases} \\end{equation*}\\] From this we find \\[\\begin{equation*} R(K) = \\int_{-\\infty}^\\infty K(x)^2 \\,dx = \\int_{-1}^1 \\frac14 \\,dx = \\frac12 \\end{equation*}\\] and \\[\\begin{equation*} \\mu_2(K) = \\int_{-\\infty}^\\infty x^2 K(x) \\,dx = \\int_{-1}^1 \\frac12 x^2 \\,dx = \\frac16 \\Bigl. x^3 \\Bigr|_{x=-1}^1 = \\frac16 \\bigl( 1 - (-1) \\bigr) = \\frac13. \\end{equation*}\\] Thus, for the uniform kernel we have \\[\\begin{equation*} R(K)^2 \\mu_2(K) = \\Bigl( \\frac12 \\Bigr)^2 \\frac13 = \\frac{1}{12} \\approx 0.083333. \\end{equation*}\\] Calculations similar to the ones in the example give the following values: kernel \\(\\mu_2(K)\\) \\(R(K)\\) \\(R(K)^2 \\mu_2(K)\\) Uniform \\(\\displaystyle\\frac13\\) \\(\\displaystyle\\frac12\\) \\(0.08333\\) Triangular \\(\\displaystyle\\frac16\\) \\(\\displaystyle\\frac23\\) \\(0.07407\\) Epanechnikov \\(\\displaystyle\\frac15\\) \\(\\displaystyle\\frac35\\) \\(0.07200\\) Gaussian \\(1\\) \\(\\displaystyle\\frac{1}{2\\sqrt{\\pi}}\\) \\(0.07958\\) The best value in the table is obtained for the Epanechnikov kernel, with \\(R(K)^2 \\mu_2(K) = 9/125 = 0.072\\). One can show that this value is indeed optimal amongst all kernels. Since the difference in error for the kernels listed above is only a few percent, any of these kernels would be a reasonable choice. 3.2 Bandwidth Selection Our formulas for the optimal bandwidth contain the terms \\(f(x)^2 |f&#39;&#39;(x)|\\) for fixed \\(x\\) and \\(R(f&#39;&#39;)\\) for the integrated error. Since \\(f\\) is unknown, neither of these quantities are available and instead different rules of thumb are used in the literature. Here we present one possible choice of bandwidth estimator. Suppose that \\(f\\) is a normal density, with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then we have \\[\\begin{equation*} f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\bigl( - (x-\\mu)^2 / 2\\sigma^2 \\bigr). \\end{equation*}\\] Taking derivatives we get \\[\\begin{equation*} f&#39;(x) = - \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\frac{x-\\mu}{\\sigma^2} \\exp\\bigl( - (x-\\mu)^2 / 2\\sigma^2 \\bigr) \\end{equation*}\\] and \\[\\begin{equation*} f&#39;&#39;(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\Bigl( \\frac{(x-\\mu)^2}{\\sigma^4} - \\frac{1}{\\sigma^2} \\Bigr) \\exp\\bigl( - (x-\\mu)^2 / 2\\sigma^2 \\bigr) \\end{equation*}\\] Patiently integrating the square of this function gives \\[\\begin{equation*} R(f&#39;&#39;) = \\int_{-\\infty}^\\infty f&#39;&#39;(x)^2 \\,dx = \\cdots = \\frac{3}{8\\sigma^5\\sqrt{\\pi}}. \\end{equation*}\\] This can be used as a simple “plug-in rule” with \\(\\sigma\\) estimated by the sample standard deviation. We now demonstrate how this rule of thumb could be used in R to obtain a kernel density estimate for the snowfall data. We will use the Epanechnikov kernel. For compatibility with the kernels built into R, we rescale this kernel, so that \\(\\mu_2(K) = 1\\), i.e. we consider \\(K_{\\sqrt{5}}\\) in place of \\(K\\). An easy calculation shows that the roughness is then \\(R(K) = 3/(5\\sqrt{5})\\). # downloaded from https://teaching.seehuhn.de/data/buffalo/ x &lt;- read.csv(&quot;data/buffalo.csv&quot;) snowfall &lt;- x$snowfall n &lt;- length(snowfall) # Roughness of the Epanechnikov kernel, after rescaling with h = sqrt(5) # so that the second moment becomes mu_2 = 1: R.K &lt;- 3 / (5 * sqrt(5)) # Rule of thumb: R.fpp &lt;- 3 / (8 * sd(snowfall)^5 * sqrt(pi)) # formula for the optimal h my.bw &lt;- (R.K / (n * 1^2 * R.fpp))^0.2 my.bw ## [1] 11.58548 R has a variety of different builtin methods to estimate bandwidths. See help(\"bw.nrd0\") for R’s built-in bandwidth selectors. For comparison to our result, we list here the bandwidths suggested by some of R’s algorithms: data.frame( name = c(&quot;nrd0&quot;, &quot;nrd&quot;, &quot;SJ&quot;), bw = c(bw.nrd0(snowfall), bw.nrd(snowfall), bw.SJ(snowfall))) ## name bw ## 1 nrd0 9.724206 ## 2 nrd 11.452953 ## 3 SJ 11.903840 All of these values seem close the value we obtained manually. Using our bandwidth estimate, we get the following estimated density. plot(density(snowfall, bw = my.bw, kernel = &quot;epanechnikov&quot;), main = NA) In practice one would just use one of the built-in methods, for example using bw=\"SJ\" instead of estimating the bandwidth manually. 3.3 Higher Dimensions So far we have only considered the one-dimensional case, where the samples \\(x_i\\) are real numbers. In this subsection we will sketch how these methods will need to be adjusted for the multivariate case of \\(x_i = (x_{i,1}, \\ldots, x_{i,p}) \\in \\mathbb{R}^p\\). In this setup, a kernel is a function \\(K\\colon \\mathbb{R}^p\\to \\mathbb{R}\\) (generalizing Definition 1.1) such that \\(\\int \\cdots \\int K(x) \\,dx_p \\cdots dx_1 = 1\\), \\(K(x) = K(-x)\\) and \\(K(x) \\geq 0\\) for all \\(x\\in \\mathbb{R}^p\\), where the integral in the first condition is now over all \\(p\\) coordinates. Example 3.2 If \\(K_1, \\ldots, K_p\\) are one-dimensional kernels, then the product \\[\\begin{equation*} K(x_1, \\ldots, x_p) := K_1(x_1) \\cdots K_p(x_p) \\end{equation*}\\] is a kernel in \\(p\\) dimensions. If we use the product of \\(p\\) Gaussian kernels, we get \\[\\begin{align*} K(x) &amp;= \\prod_{i=1}^p \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigl(-x_i^2/2\\bigr) \\\\ &amp;= \\frac{1}{(2\\pi)^{p/2}} \\exp\\Bigl(-\\frac12 (x_1^2 + \\cdots + x_p^2) \\Bigr). \\end{align*}\\] There are different possibilities for rescaling these kernels: If all coordinates live on “comparable scales” (e.g., if they are measured in the same units), the formula \\[ K_h(x) = \\frac{1}{h^p} K(x/h) \\] for all \\(x\\in\\mathbb{R}^p\\) can be used, where \\(h&gt;0\\) is a bandwidth parameter as before. The scaling by \\(1/h^p\\) is required to ensure that the integral of \\(K_h\\) equals \\(1\\), so that \\(K_h\\) is a kernel again. If different scaling is desirable for different components, the formula \\[\\begin{equation*} K_h(x) = \\frac{1}{h_1 \\cdots h_p} K(x_1/h_1, \\ldots, x_p/h_p) \\end{equation*}\\] for all \\(x\\in\\mathbb{R}^p\\) can be used, where \\(h = (h_1, \\ldots, h_p)\\) is a vector of bandwidth parameters. A more general version would be to use a symmetric, positive definite bandwidth matrix \\(H \\in \\mathbb{R}^{p\\times p}\\). In this case the required scaling is \\[\\begin{equation*} K_H(x) = \\frac{1}{\\mathrm{det}(H)} K\\bigl( H^{-1} x \\bigr) \\end{equation*}\\] for all \\(x\\in\\mathbb{R}^p\\). For all of these choices, the kernel density estimator is given by \\[\\begin{equation*} \\hat f_h(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i) \\end{equation*}\\] (using \\(K_H\\) for the third option) for all \\(x\\in\\mathbb{R}^p\\). Bandwidth selection in the multivariate case is a difficult problem and we will not discuss this here. In the previous sections we have used kernel methods to estimate probability densities from observed data. In the following sections we will see a different application of kernel methods: using them for regression, where we estimate a function \\(m\\) from noisy observations. "],["P01.html", "Problem Sheet 1", " Problem Sheet 1 .fold-btn { float: right; margin: -12px 0 0 0; } This problem sheet is for self-study only. It is not assessed. 1. Consider the following function: \\[\\begin{equation*} K(x) = \\begin{cases} \\frac23 (1 - |x|^3) &amp; \\mbox{if $|x|\\leq 1$, and} \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\end{equation*}\\] Show that this function integrates to 1 over its domain. We have \\[\\begin{align*} \\int_{-\\infty}^{\\infty} K(x) , dx &amp;= \\frac23 \\int_{-1}^1 (1 - |x|^3) , dx \\\\ &amp;= \\frac43 \\int_0^1 (1 - x^3) , dx \\\\ &amp;= \\frac43 \\left[ x - \\frac{x^4}{4} \\right]_0^1 \\\\ &amp;= \\frac43 \\left( 1 - \\frac{1}{4} \\right) \\\\ &amp;= 1. \\end{align*}\\] Show that \\(K\\) satisfies the conditions of a kernel. We have already seen that \\(K\\) integrates to 1 over its domain. Since \\(|x| = |-x|\\) for all \\(x\\in\\mathbb{R}\\), the function \\(K\\) is symmetric. Finally, since \\(|x|^3 \\leq 1\\) for all \\(x\\in\\mathbb{R}\\) with \\(|x|\\leq 1\\), we have \\(K(x) \\geq 0\\) for all \\(x\\in\\mathbb{R}\\). Compute the moments \\(\\mu_0(K)\\), \\(\\mu_1(K)\\) and \\(\\mu_2(K)\\) of \\(K\\). The \\(k\\)th moment of \\(K\\) is given by \\[\\begin{equation*} \\mu_k(K) = \\int_{-\\infty}^\\infty x^k K(x) \\,dx = \\frac23 \\int_{-1}^1 x^k \\bigl( 1 - |x|^3 \\bigr) \\,dx. \\end{equation*}\\] For \\(k = 0\\), we know \\(\\mu_0(K) = 1\\), from part a. For \\(k = 1\\), we have \\(\\mu_1(K) = 0\\), since \\(K\\) is symmetric. For \\(k = 2\\), we find \\[\\begin{align*} \\mu_2(K) &amp;= \\frac23 \\int_{-1}^1 x^2 (1 - |x|^3) \\,dx \\\\ &amp;= \\frac43 \\int_0^1 x^2 (1 - x^3) \\,dx \\\\ &amp;= \\frac43 \\left[ \\frac{x^3}{3} - \\frac{x^6}{6} \\right]_0^1 \\\\ &amp;= \\frac43 \\left( \\frac13 - \\frac16 \\right) \\\\ &amp;= \\frac29. \\end{align*}\\] 2. Consider a normal density with mean μ and variance σ², given by: \\[\\begin{equation*} f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right). \\end{equation*}\\] Calculate f’(x) and f’’(x) for this density. Using the chain rule: \\[\\begin{align*} f&#39;(x) &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\cdot \\left(-\\frac{x-\\mu}{\\sigma^2}\\right) \\\\ &amp;= -f(x)\\cdot\\frac{x-\\mu}{\\sigma^2} \\end{align*}\\] Taking derivatives again, using the product rule: \\[\\begin{align*} f&#39;&#39;(x) &amp;= -f&#39;(x)\\cdot\\frac{x-\\mu}{\\sigma^2} - f(x)\\cdot\\frac{1}{\\sigma^2} \\\\ &amp;= f(x)\\cdot\\frac{(x-\\mu)^2}{\\sigma^4} - f(x)\\cdot\\frac{1}{\\sigma^2} \\\\ &amp;= f(x)\\cdot\\left(\\frac{(x-\\mu)^2}{\\sigma^4} - \\frac{1}{\\sigma^2}\\right) \\end{align*}\\] Using the formula \\[\\begin{equation*} \\mathop{\\mathrm{bias}}(\\hat f_h(x)) \\approx \\frac{\\mu_2(K)}{2} f&#39;&#39;(x) h^2, \\end{equation*}\\] show that for fixed \\(K\\) and \\(h\\), the bias satisfies the following proportionality: \\[\\begin{equation*} \\mathop{\\mathrm{bias}}(\\hat f_h(x)) \\propto \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\left(\\frac{(x-\\mu)^2}{\\sigma^4} - \\frac{1}{\\sigma^2}\\right). \\end{equation*}\\] Substituting our expression for \\(f&#39;&#39;(x)\\) into the bias formula we get \\[\\begin{align*} \\mathop{\\mathrm{bias}}(\\hat f_h(x)) &amp;\\approx \\frac{\\mu_2(K)}{2} f&#39;&#39;(x) h^2 \\\\ &amp;= \\frac{\\mu_2(K)}{2} \\cdot f(x)\\cdot\\left(\\frac{(x-\\mu)^2}{\\sigma^4} - \\frac{1}{\\sigma^2}\\right) h^2 \\\\ &amp;= \\frac{\\mu_2(K)h^2}{2\\sqrt{2\\pi\\sigma^2}} \\cdot \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\left(\\frac{(x-\\mu)^2}{\\sigma^4} - \\frac{1}{\\sigma^2}\\right). \\end{align*}\\] For which values of \\(x\\) is the bias positive, negative, or zero? Why does this make sense intuitively? The exponential term is always positive, so the sign depends on the sign of the term \\((x-\\mu)^2 / \\sigma^4 - 1/\\sigma^2\\). This equals zero when \\((x-\\mu)^2 = \\sigma^2\\), or when \\(x = \\mu \\pm \\sigma\\). The bias is negative when \\(|x-\\mu| &lt; \\sigma\\), is positive when \\(|x-\\mu| &gt; \\sigma\\). This makes sense because the positive bandwidth \\(h\\) smoothes the density, so the bias is negative when the density is concave (near the maxiumum) and positive when it is convex (towards the tails). 3. Consider the variance of a kernel density estimate \\(\\hat f_h(x)\\) based on a sample \\(X_1, \\ldots, X_n\\) with common density \\(f\\): \\[\\begin{equation*} \\mathop{\\mathrm{Var}}(\\hat f_h(x)) = \\frac{1}{n^2} \\sum_{i,j=1}^n \\mathbb{E}\\Bigl( K_h(x - X_i)K_h(x - X_j) \\Bigr) \\end{equation*}\\] Let \\(n=3\\). Write out all terms in the sum explicitly and identify which terms involve the only one random variable \\(X_i\\) and which involve more than one random variable. For \\(n=3\\), expanding the double sum gives 9 terms: Terms with same random variable (\\(i=j\\)): \\[\\begin{align*} &amp; \\mathbb{E}(K_h(x - X_1)K_h(x - X_1)) \\\\ &amp; \\mathbb{E}(K_h(x - X_2)K_h(x - X_2)) \\\\ &amp; \\mathbb{E}(K_h(x - X_3)K_h(x - X_3)) \\end{align*}\\] Terms with different random variables (\\(i\\neq j\\)): \\[\\begin{align*} &amp; \\mathbb{E}(K_h(x - X_1)K_h(x - X_2)) \\\\ &amp; \\mathbb{E}(K_h(x - X_1)K_h(x - X_3)) \\\\ &amp; \\mathbb{E}(K_h(x - X_2)K_h(x - X_1)) \\\\ &amp; \\mathbb{E}(K_h(x - X_2)K_h(x - X_3)) \\\\ &amp; \\mathbb{E}(K_h(x - X_3)K_h(x - X_1)) \\\\ &amp; \\mathbb{E}(K_h(x - X_3)K_h(x - X_2)) \\end{align*}\\] For general \\(n\\), how many terms in the sum have \\(i=j\\) and how many have \\(i\\neq j\\)? When \\(i=j\\), we are choosing the same index from \\(\\{1,\\ldots,n\\}\\) once, giving \\(n\\) terms. When \\(i\\neq j\\), we are choosing two different indices from \\(\\{1,\\ldots,n\\}\\), giving \\(n(n-1)\\) terms. The total number of terms is thus \\(n + n(n-1) = n^2\\), as expected. Using these counts and the independence of the \\(X_i\\), show that: \\[\\begin{equation*} \\mathop{\\mathrm{Var}}(\\hat f_h(x)) = \\frac{1}{n} \\mathbb{E}\\Bigl( K_h(x - X_1)^2 \\Bigr) + \\frac{n-1}{n} \\mathbb{E}\\Bigl( \\hat f_h(x) \\Bigr)^2. \\end{equation*}\\] Using part b, we can split the sum into two parts: \\[\\begin{align*} \\mathop{\\mathrm{Var}}(\\hat f_h(x)) &amp;= \\frac{1}{n^2} \\Bigl( n \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\bigr) + n(n-1) \\mathbb{E}\\bigl( K_h(x - X_1)K_h(x - X_2) \\bigr) \\Bigr) \\\\ &amp;= \\frac{1}{n} \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\bigr) + \\frac{n-1}{n} \\mathbb{E}\\bigl( K_h(x - X_1) \\bigr) \\, \\mathbb{E}\\bigl( K_h(x - X_2) \\bigr), \\end{align*}\\] where we used the independence of the \\(X_i\\) in the last step. Since \\(\\mathbb{E}\\bigl( K_h(x - X_1) \\bigr) = \\mathbb{E}\\bigl( K_h(x - X_2) \\bigr) = \\mathbb{E}(\\hat f_h(x))\\), this gives the required result. 4. Consider the following data: x &lt;- c(89.6, 82.5, 70.9, 83.8, 92.4, 86.5, 77.3, 89.2, 93.1, 84.7, 78.5, 88.3, 85.6, 90.4, 76.8) Determine the sample standard deviation of these data. sigma &lt;- sd(x) sigma ## [1] 6.410126 The R ouputs shows that the standard deviation is \\(6.410126\\). Using the “plug-in rule” from section 3.2, what bandwidth would you choose for a kernel density estimate of these data using the triangular kernel? The plug-in rule assumes that the density is normal, to get \\[\\begin{equation*} R(f&#39;&#39;) = \\frac{3}{8\\sigma^5\\sqrt{\\pi}}. \\end{equation*}\\] Substituting the sample standard deviation into this formula gives R.fpp &lt;- 3 / (8 * sigma^5 * sqrt(pi)) R.fpp ## [1] 1.954895e-05 We also need the sample size, and the roughness and second moment of the kernel: n &lt;- length(x) R.K &lt;- 2/3 mu2.K &lt;- 1/6 Using these quantities and the formula from section 2.6 gives the result: h &lt;- (R.K / (n * mu2.K^2 * R.fpp))^(1/5) h ## [1] 9.607254 "],["X04-smoothing.html", "Section 4 Kernel Smoothing 4.1 The Nadaraya-Watson Estimator 4.2 Estimation Error", " Section 4 Kernel Smoothing We now consider the statistical model \\[\\begin{equation*} Y_i = m(x_i) + \\varepsilon_i, \\end{equation*}\\] where \\(m\\colon \\mathbb{R}\\to \\mathbb{R}\\) is a smooth function and \\(\\varepsilon_i\\) are independent random variables with \\(\\mathbb{E}(\\varepsilon_i) = 0\\). We are given data \\((x_i, y_i)\\) for \\(i\\in \\{1, \\ldots, n\\}\\) and our aim is to estimate the function \\(m\\). The task of estimating the function \\(m\\) from data is called smoothing. 4.1 The Nadaraya-Watson Estimator Since we have \\[\\begin{equation*} \\mathbb{E}(Y_i) = \\mathbb{E}\\bigl( m(x_i) + \\varepsilon_i \\bigr) = m(x_i) + \\mathbb{E}( \\varepsilon_i ) = m(x_i), \\end{equation*}\\] we could attempt to use a Monte-Carlo approach where we estimate the expectation \\(\\mathbb{E}(Y_i)\\) using an average of many \\(Y\\) values. This approach is not feasible in practice, since typically we will only have a single observation \\(y_i\\) corresponding to a given \\(x_i\\). The idea of the Nadaraya-Watson Estimator is to average the \\(y_i\\) corresponding to nearby \\(x_i\\) instead. A weighted average is used, which gives less weight to further away values. This leads to the following definition. Definition 4.1 The Nadaraya-Watson Estimator for \\(m(x)\\) is given by \\[\\begin{equation*} \\hat m_h(x) = \\frac{\\sum_{i=1}^n K_h(x - x_i) y_i}{\\sum_{j=1}^n K_h(x - x_j)}, \\end{equation*}\\] where \\(K_h\\) is a kernel scaled to bandwidth \\(h\\) as in definition 1.2. The problem of finding \\(m\\) using kernel functions is called kernel smoothing or kernel regression. In this context, the bandwidth \\(h\\) is also called the smoothing parameter. The Nadaraya-Watson Estimator is not the only method for kernel smoothing. We will learn about different methods in the next sections. Using the shorthand \\[\\begin{equation*} w_i(x) := \\frac{K_h(x - x_i)}{\\sum_{j=1}^n K_h(x - x_j)} \\end{equation*}\\] we can write the Nadaraya-Watson Estimator as \\(\\hat m_h(x) = \\sum_{i=1}^n w_i(x) y_i\\) and since \\[\\begin{align*} \\sum_{i=1}^n w_i(x) &amp;= \\sum_{i=1}^n \\frac{K_h(x - x_i)}{\\sum_{j=1}^n K_h(x - x_j)} \\\\ &amp;= \\frac{\\sum_{i=1}^n K_h(x - x_i)}{\\sum_{j=1}^n K_h(x - x_j)} \\\\ &amp;= 1, \\end{align*}\\] this is indeed a weighted average. Example 4.1 The faithful dataset built into R contains 272 observations of waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park. We can use the ksmooth() function to compute Nadaraya-Watson estimate for the waiting time after an eruption of a given length. Here we use a Gaussian kernel with bandwidth 1. x &lt;- faithful$eruptions y &lt;- faithful$waiting plot(x, y, cex = .5, xlab = &quot;eruption time [mins]&quot;, ylab = &quot;time to next eruption [mins]&quot;) lines(ksmooth(x, y, kernel = &quot;normal&quot;, bandwidth = 1, n.points = 1000), col = &quot;red&quot;) The estimate \\(\\hat m_h\\) (red line) smoothly connects the two clusters visible in the scatter plot. For kernels with bounded support, e.g. for the Epanechnikov kernel, the denominator \\(\\sum_{j=1}^n K_h(x - x_j)\\) will equal zero for \\(x\\) which are too far away from all of the \\(x_i\\). For these \\(x\\), the weights \\(w_i\\) and thus also the estimate \\(\\hat m_h(x)\\) are undefined. This problem can easily be seen in practice, when the bandwidth is chosen too small. Example 4.2 To illustrate the problem of the estimate becoming undefined far away from the data points, we redo the previous estimate using a uniform kernel: plot(x, y, cex = .5, xlab = &quot;eruption time [mins]&quot;, ylab = &quot;time to next eruption [mins]&quot;) lines(ksmooth(x, y, kernel = &quot;box&quot;, bandwidth = 1, n.points = 1000), col = &quot;red&quot;) lines(ksmooth(x, y, kernel = &quot;box&quot;, bandwidth = 0.1, n.points = 1000), col = &quot;blue&quot;) For \\(h = 1\\) (red line) we get a line \\(\\hat m_h\\) which is less smooth than the estimate using the Gaussian kernel, but is otherwise looks similar to the previous estimate. In contrast, if we reduce the bandwidth to \\(h = 0.1\\) (blue line), gaps start to appear in the plot of \\(\\hat m_h\\) where the spacing of the data points is too large. 4.2 Estimation Error Here we will discuss how fast the estimation error decreases in the limit of \\(n\\to \\infty\\), i.e. for the case when we have a large dataset to use for the estimate. As before, we will find that we need to decrease the bandwidth \\(h\\) as \\(n\\) increases. To allow for \\(n\\) to change, we will introduce a statistical model also for the inputs \\(x_i\\). (This is different from what we did in the level 3 part of the module for linear regression.) Here we will consider the following model: \\(X_1, \\ldots, X_n\\) are independent and identically distributed with density \\(f\\). \\(\\eta_1, \\ldots \\eta_n\\) are independent, with \\(\\mathbb{E}(\\eta_i) = 0\\) and \\(\\mathop{\\mathrm{Var}}(\\eta_i) = 1\\). \\(\\varepsilon_i = s(X_i) \\eta_i\\) for all \\(i \\in \\{1, \\ldots, n\\}\\), where \\(s\\colon \\mathbb{R}\\to (0, \\infty)\\) is a smooth function. \\(Y_i = m(X_i) + \\varepsilon_i\\) where \\(m\\colon \\mathbb{R}\\to \\mathbb{R}\\) is a smooth function. While this extended model allows us to increase \\(n\\), it also creates a practical problem: the estimator \\[\\begin{equation*} \\hat m_h(x) = \\frac{\\sum_{i=1}^n K_h(x - X_i) Y_i}{\\sum_{j=1}^n K_h(x - X_j)}, \\end{equation*}\\] now has random terms both in the numerator and in the denominator. This will make it more challenging to determine the behaviour of \\(\\mathbb{E}\\bigl( \\hat m_h(x) \\bigr)\\) and \\(\\mathop{\\mathrm{Var}}\\bigl( \\hat m_h(x) \\bigr)\\) as \\(n \\to \\infty\\) and \\(h \\downarrow 0\\). We can write \\(\\hat m_h(x)\\) as \\[\\begin{equation} \\hat m_h(x) = \\frac{\\frac1n \\sum_{i=1}^n K_h(x - X_i) Y_i}{\\frac1n \\sum_{j=1}^n K_h(x - X_j)} = \\frac{\\frac1n \\sum_{i=1}^n K_h(x - X_i) Y_i}{\\hat f_h(x)} = \\frac{\\hat r_h(x)}{\\hat f_h(x)} \\tag{4.1} \\end{equation}\\] where \\(\\hat f_h(x)\\) is the kernel density estimator from Section 1 and \\[\\begin{equation*} \\hat r_h(x) := \\frac1n \\sum_{i=1}^n K_h(x - X_i) Y_i. \\end{equation*}\\] We will consider the numerator and denominator of equation (4.1) separately. Denominator From equations (2.6) and (2.11) we know that \\[\\begin{equation*} \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) \\approx f(x) + \\frac{\\mu_2(K) f&#39;&#39;(x)}{2} h^2 \\end{equation*}\\] and \\[\\begin{equation*} \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) \\approx \\frac{1}{nh} f(x) R(K) \\end{equation*}\\] as \\(h \\downarrow 0\\). Numerator We start by considering the numerator \\(\\hat r_h(x)\\). The arguments used here will be very similar to the arguments used in Section 2.4.3 on the variance of kernel density estimates. The expectation of \\(\\hat r_h(x)\\) is \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat r_h(x) \\bigr) &amp;= \\mathbb{E}\\Bigl( \\frac1n \\sum_{i=1}^n K_h(x - X_i) Y_i \\Bigr) \\\\ &amp;= \\frac1n \\sum_{i=1}^n \\mathbb{E}\\bigl( K_h(x - X_i) Y_i \\bigr) \\\\ &amp;= \\mathbb{E}\\bigl( K_h(x - X) Y \\bigr) \\\\ &amp;= \\mathbb{E}\\Bigl( K_h(x - X) (m(X) + s(X)\\eta) \\Bigr). \\end{align*}\\] We use integrals to average over the randomness in \\(X\\) and \\(\\eta\\), denoting the density of \\(\\eta\\) by \\(\\varphi\\): \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat r_h(x) \\bigr) &amp;= \\int \\int K_h(x - \\xi) \\bigl( m(\\xi) + s(\\xi) e \\bigr) \\, \\varphi(e) \\, de \\, f(\\xi) \\, d\\xi \\\\ &amp;= \\int K_h(x - \\xi) \\Bigl( m(\\xi) + s(\\xi) \\int e \\, \\varphi(e) \\, de \\Bigr) \\, f(\\xi) \\, d\\xi \\\\ &amp;= \\int K_h(x - \\xi) m(\\xi) f(\\xi) \\, d\\xi, \\end{align*}\\] since \\[\\begin{equation*} \\int e \\, \\varphi(e) \\, de = \\mathbb{E}(\\eta) = 0. \\end{equation*}\\] Writing \\[\\begin{equation*} r(x) := m(x) f(x) \\end{equation*}\\] as an abbreviation, we finally get \\[\\begin{equation*} \\mathbb{E}\\bigl( \\hat r_h(x) \\bigr) = \\int K_h(x - \\xi) r(\\xi) \\, d\\xi. \\end{equation*}\\] We now formalise an argument which we already used earlier. Lemma 4.1 Let \\(g\\colon \\mathbb{R}\\to \\mathbb{R}\\) be two times continuously differentiable and let \\(K\\) be a kernel function. Then we have \\(\\displaystyle\\int K_h(x - \\xi) g(\\xi) \\, d\\xi = g(x) + \\frac12 \\mu_2(K) g&#39;&#39;(x) h^2 + o(h^2)\\) as \\(h \\downarrow 0\\), and \\(\\displaystyle\\int K_h(x - \\xi)^2 g(\\xi) \\, d\\xi = \\frac1h R(K) g(x) + o(1/h)\\) as \\(h \\downarrow 0\\). Proof. The first statement is proved using substitution and Taylor expansion of \\(r\\) around \\(x\\) as shown in the derivation of equation (2.6). The second statement is proved similarly, as shown in the derivation of equation (2.9). Using the first part of lemma 4.1 for \\(g = r\\) we get \\[\\begin{equation*} \\mathbb{E}\\bigl( \\hat r_h(x) \\bigr) = r(x) + \\frac12 \\mu_2(K) r&#39;&#39;(x) h^2 + o(h^2). \\end{equation*}\\] For the variance of \\(\\hat r_h(x)\\) we get \\[\\begin{align*} \\mathop{\\mathrm{Var}}\\bigl( \\hat r_h(x) \\bigr) &amp;= \\mathop{\\mathrm{Var}}\\Bigl( \\frac1n \\sum_{i=1}^n K_h(x - X_i) Y_i \\Bigr) \\\\ &amp;= \\frac{1}{n^2} \\sum_{i=1}^n \\mathop{\\mathrm{Var}}\\bigl( K_h(x - X_i) Y_i \\bigr) \\\\ &amp;= \\frac1n \\mathop{\\mathrm{Var}}\\bigl( K_h(x - X) Y \\bigr) \\\\ &amp;= \\frac1n \\Bigl( \\mathbb{E}\\bigl( K_h(x - X)^2 Y^2 \\bigr) - \\mathbb{E}\\bigl( K_h(x - X) Y \\bigr)^2 \\Bigr). \\end{align*}\\] We have already seen that \\[\\begin{equation*} \\mathbb{E}\\bigl( K_h(x - X) Y \\bigr) = \\mathbb{E}\\bigl( \\hat r_h(x) \\bigr) = r(x) + \\frac12 \\mu_2(K) r&#39;&#39;(x) h^2 + o(h^2) \\end{equation*}\\] and thus \\[\\begin{equation*} \\mathbb{E}\\bigl( K_h(x - X) Y \\bigr)^2 = r(x)^2 + O(h^2). \\end{equation*}\\] Using the second part of lemma 4.1 one can show that \\[\\begin{align*} \\mathbb{E}\\bigl( K_h(x - X)^2 Y^2 \\bigr) &amp;= \\int \\int K_h(x - \\xi)^2 \\bigl( m(\\xi) + s(\\xi)e \\bigr)^2 \\,\\varphi(e)\\,de \\,f(\\xi)\\,d\\xi \\\\ &amp;= \\int K_h(x - \\xi)^2 \\bigl( m(\\xi)^2 + s(\\xi)^2 \\bigr) f(\\xi) \\,d\\xi \\\\ &amp;= \\frac1h R(K) \\bigl( m(x)^2 + s(x)^2 \\bigr) f(x) + o(1/h). \\end{align*}\\] Combining these equations we find \\[\\begin{equation*} \\mathop{\\mathrm{Var}}\\bigl( \\hat r_h(x) \\bigr) \\approx \\frac{1}{nh} R(K) \\bigl( m(x)^2 + s(x)^2 \\bigr) f(x) + \\frac1n r(x)^2 \\end{equation*}\\] as \\(n\\to\\infty\\), \\(h\\downarrow 0\\) and \\(nh\\to\\infty\\). Mean Squared Error To turn our results about \\(\\hat r_h\\) and our previous results about \\(\\hat f\\) into an error estimate for \\[\\begin{equation*} \\hat m_h(x) = \\frac{\\hat r_h(x)}{\\hat f_h(x)}, \\end{equation*}\\] we consider Taylor expansion of the function \\(g(y) = 1/y\\): \\[\\begin{align*} g(y + h) &amp;= g(y) + g&#39;(y) h + o(h) \\\\ &amp;= \\frac1y - \\frac{1}{y^2} h + o(h). \\end{align*}\\] Using this approximation we get \\[\\begin{align*} \\hat m_h(x) &amp;= \\hat r_h(x) g\\bigl( \\hat f_h(x) \\bigr) \\\\ &amp;= \\hat r_h(x) g\\bigl( f(x) + \\hat f_h(x) - f(x) \\bigr) \\\\ &amp;\\approx \\hat r_h(x) \\Bigl( \\frac{1}{f(x)} - \\frac{1}{f(x)^2} (\\hat f_h(x) - f(x)) \\Bigr) \\\\ &amp;= \\frac{\\hat r_h(x)}{f(x)} - \\frac{\\hat r_h(x) \\bigl(\\hat f_h(x) - f(x) \\bigr)}{f(x)^2}. \\end{align*}\\] With the help of this trick, we have achieved that now all random terms are in the denominator and thus we can take expectations easily: \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat m_h(x) \\bigr) &amp;= \\frac{\\mathbb{E}\\bigl( \\hat r_h(x) \\bigr)}{f(x)} - \\frac{\\mathbb{E}\\Bigl( \\hat r_h(x) \\bigl(\\hat f_h(x) - f(x) \\bigr) \\Bigr)}{f(x)^2} \\\\ &amp;\\approx \\frac{\\mathbb{E}\\bigl( \\hat r_h(x) \\bigr)}{f(x)} - \\frac{\\mathbb{E}\\bigl( \\hat r_h(x) \\bigr) \\mathbb{E}\\bigl( \\hat f_h(x) - f(x) \\bigr)}{f(x)^2}. \\end{align*}\\] Substituting in our previous results we get \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat m_h(x) \\bigr) &amp;\\approx \\frac{r(x) + \\frac12 \\mu_2(K) r&#39;&#39;(x) h^2 + o(h^2)}{f(x)} - \\frac{\\bigl( r(x) + \\frac12 \\mu_2(K) r&#39;&#39;(x) h^2 + o(h^2) \\bigr) \\frac12 \\mu_2(K) f&#39;&#39;(x) h^2}{f(x)^2} \\\\ &amp;= \\frac{r(x)}{f(x)} + \\frac12 \\frac{\\mu_2(K) r&#39;&#39;(x)}{f(x)} h^2 - \\frac12 \\frac{\\mu_2(K) r(x) f&#39;&#39;(x)}{f(x)^2} h^2 + o(h^2) \\end{align*}\\] Using \\(r(x) = f(x) m(x)\\) we find the derivative \\(r&#39;(x) = f&#39;(x) m(x) + f(x) m&#39;(x)\\) as well as the second derivative \\(r&#39;&#39;(x) = f&#39;&#39;(x) m(x) + 2 f&#39;(x) m&#39;(x) + f(x) m&#39;&#39;(x)\\). This gives \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat m_h(x) \\bigr) &amp;= m(x) + \\frac12 \\frac{\\mu_2(K) r&#39;&#39;(x)}{f(x)} h^2 - \\frac12 \\frac{\\mu_2(K) m(x) f&#39;&#39;(x)}{f(x)} h^2 + o(h^2) \\\\ &amp;= m(x) + \\frac12 \\frac{\\mu_2(K) \\bigl(2 f&#39;(x) m&#39;(x) + f(x) m&#39;&#39;(x)\\bigr)}{f(x)} h^2 + o(h^2) \\\\ &amp;= m(x) + \\mu_2(K) \\Bigl( \\frac{f&#39;(x)}{f(x)} m&#39;(x) + \\frac12 m&#39;&#39;(x) \\Bigr) h^2 + o(h^2) \\end{align*}\\] and \\[\\begin{equation*} \\mathop{\\mathrm{bias}}\\bigl( \\hat m_h(x) \\bigr) = \\mu_2(K) \\Bigl( \\frac{f&#39;(x)}{f(x)} m&#39;(x) + \\frac12 m&#39;&#39;(x) \\Bigr) h^2 + o(h^2). \\end{equation*}\\] A similar calculation gives the approximate variance as \\[\\begin{equation*} \\mathop{\\mathrm{Var}}\\bigl( \\hat m_h(x) \\bigr) = \\frac{1}{nh} \\frac{\\sigma^2(x) R(K)}{f(x)} + o\\Bigl( \\frac{1}{nh} \\Bigr). \\end{equation*}\\] So finally we have \\[\\begin{align*} \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat m_h(x) \\bigr) &amp;= \\frac{h^4 \\mu_2(K)^2}{4} \\Bigl(m&#39;&#39;(x) +\\frac{2m&#39;(x) f&#39;(x)}{f(x)} \\Bigr)^2 \\\\ &amp;\\hskip1cm + \\frac{1}{nh} \\frac{\\sigma^2(x) R(K)}{f(x)} + o\\Bigl( \\frac{1}{nh} \\Bigr) + o(h^4). \\end{align*}\\] Notes: A more careful calculation will need to take into account that \\(\\hat m_h(x)\\) may be undefined. All expectations and variances are conditional on \\(\\hat f(x) \\neq 0\\). One can show that \\(P\\bigl( \\hat f(x) \\neq 0 \\bigr) \\to 1\\) as \\(n\\to\\infty\\) for all \\(x\\in\\mathbb{R}\\) with \\(f(x) &gt; 0\\), so this is not a problem. The MSE is of order \\(O(n^{-4/5})\\) when we choose \\(h \\sim n^{-1/5}\\), as before. The formula for the variance shows that the regression curve is more stable in those areas where there are plenty of observations. The bias-squared is either dominated by the second derivative \\(m&#39;&#39;(x)\\) - when we are close to a local extremum of \\(m(x)\\) (turning point), or by the first derivative \\(m&#39;(x)\\) when we have few observations. This calculation is helpful in creating confidence intervals for the estimate \\(\\hat m_h(x)\\) in which \\(\\sigma^2(x)\\) can be estimated by \\[\\hat \\sigma^2(x) = \\sum w_i(x) \\bigl( y_i-\\hat m_h^{(i)}(x_i) \\bigr)^2,\\] where \\(\\hat m_h^{(i)}(x_i)\\) is the estimate of \\(m\\) at the point \\(x_i\\) using all of the data except for the observation at \\((x_i, y_i)\\). Summary The Nadaraya-Watson estimator provides a kernel-based method for nonparametric regression, estimating \\(m(x)\\) as a weighted average of nearby observations. For kernels with bounded support, the estimator may be undefined when data points are too sparse (bandwidth too small). Under a random design model, the estimator has bias of order \\(O(h^2)\\) and variance of order \\(O(1/(nh))\\). The optimal bandwidth is \\(h \\sim n^{-1/5}\\), giving mean squared error of order \\(O(n^{-4/5})\\). The bias depends on both \\(m&#39;&#39;(x)\\) and the ratio \\(f&#39;(x)/f(x)\\), reflecting curvature in the regression function and the density of observations. The variance is smaller in regions where observations are plentiful (large \\(f(x)\\)) and increases with the local noise level \\(\\sigma^2(x)\\). "],["X05-locpoly.html", "Section 5 Local Polynomial Regression 5.1 Linear Regression with Weights 5.2 Polynomial Regression 5.3 Polynomial Regression with Weights 5.4 Special Cases", " Section 5 Local Polynomial Regression Local polynomial regression is a generalisation of the Nadaraya-Watson estimator. The method combines the two ideas of linear regression with weights and polynomial regression. The aim is still to estimate the model mean \\(m \\colon\\mathbb{R}\\to \\mathbb{R}\\) from given data \\((x_1, y_1), \\ldots, (x_n, y_n)\\). 5.1 Linear Regression with Weights In the level 3 part of the module, we introduced the least squares method for linear regression. This method estimates the regression coefficients by minimising the residual sum of squares: \\[\\begin{equation*} r(\\beta) = \\sum_{i=1}^n \\varepsilon_i^2 = \\sum_{i=1}^n \\bigl( y_i - (X\\beta)_i \\bigr)^2. \\end{equation*}\\] Here we will extend this method to include weights for the observations. Given weights \\(w_1, \\ldots, w_n &gt; 0\\), the weighted least squares method minimises \\[\\begin{equation*} r_w(\\beta) = \\sum_{i=1}^n w_i \\varepsilon_i^2 = \\sum_{i=1}^n w_i \\bigl( y_i - (X\\beta)_i \\bigr)^2. \\end{equation*}\\] In matrix notation, this function can be written as \\[\\begin{align*} r_w(\\beta) = (y - X \\beta)^\\top W (y - X \\beta), \\end{align*}\\] where \\(W\\) is a diagonal matrix with the weights on the diagonal: \\[\\begin{equation} W = \\begin{pmatrix} w_1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; w_2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; w_3 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; w_n \\end{pmatrix}. \\tag{5.1} \\end{equation}\\] Similar to lemma 2.1 in the level 3 notes, we can take derivatives to find the minimum of \\(r_w\\). The result is \\[\\begin{equation*} \\hat\\beta = (X^\\top W X)^{-1} X^\\top W y. \\end{equation*}\\] Since \\(W\\) appears once in the inverse and once before the \\(y\\), we can multiply \\(W\\) by any number without changing the result. Thus we don’t need to “normalise” the weights \\(w_i\\) to sum to one. As before, the fitted value for inputs \\((\\tilde x_1, \\ldots, \\tilde x_p) \\in \\mathbb{R}^p\\) is given by \\[\\begin{equation*} \\hat\\beta_0 + \\hat\\beta_1 \\tilde x_1 + \\cdots + \\hat\\beta_p \\tilde x_p = \\tilde x^\\top \\hat\\beta, \\end{equation*}\\] where \\(\\tilde x = (1, \\tilde x_1, \\ldots, \\tilde x_p)\\). 5.2 Polynomial Regression In these notes we only consider the case of \\(p=1\\). In this case we can easily fit a polynomial of degree \\(p\\) to the data by using \\(x, x^2, \\ldots, x^p\\) as the input variables. The corresponding model is \\[\\begin{equation*} y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_p x^p + \\varepsilon. \\end{equation*}\\] This leads to the design matrix \\[\\begin{equation*} X = \\begin{pmatrix} 1 &amp; x_1 &amp; x_1^2 &amp; \\cdots &amp; x_1^p \\\\ 1 &amp; x_2 &amp; x_2^2 &amp; \\cdots &amp; x_2^p \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_n &amp; x_n^2 &amp; \\cdots &amp; x_n^p \\end{pmatrix}. \\end{equation*}\\] Example 5.1 To illustrate polynomial regression, we fit a third-order polynomial to a simple, simulated dataset. Since the operator ^ has a special meaning inside lm(), we have to use the function I() (which disables the special meaning of + and ^ for its arguments) when computing the inputs \\(x^2\\) and \\(x^3\\). set.seed(20211102) n &lt;- 40 x &lt;- seq(0, 10, length.out = n) y &lt;- cos(x) + rnorm(n, sd = 0.5) m &lt;- lm(y ~ x + I(x^2) + I(x^3)) plot(x, y) lines(x, fitted(m)) The resulting regression line seems like a reasonable fit for the data. We note that a third-order polynomial grows very quickly to \\(\\pm\\infty\\) as \\(|x|\\) increases. Thus, the fitted model cannot be used for extrapolating beyond the range of the data. When the regression is set up in this way, the design matrix often suffers from collinearity. To check for this, we can consider the condition number \\(\\kappa\\). For the example above we get the following value: kappa(m, exact = TRUE) ## [1] 1849.947 This is a large value, indicating collinearity. To improve the setup of the problem we can use the model \\[\\begin{equation*} y = \\beta_0 + \\beta_1 (x - \\tilde x) + \\beta_2 (x - \\tilde x)^2 + \\cdots + \\beta_p (x - \\tilde x)^p + \\varepsilon, \\end{equation*}\\] where \\(\\tilde x\\) is inside the interval of \\(x\\) values. This leads to the design matrix \\[\\begin{equation} X = \\begin{pmatrix} 1 &amp; (x_1-\\tilde x) &amp; (x_1-\\tilde x)^2 &amp; \\cdots &amp; (x_1-\\tilde x)^p \\\\ 1 &amp; (x_2-\\tilde x) &amp; (x_2-\\tilde x)^2 &amp; \\cdots &amp; (x_2-\\tilde x)^p \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; (x_n-\\tilde x) &amp; (x_n-\\tilde x)^2 &amp; \\cdots &amp; (x_n-\\tilde x)^p \\end{pmatrix}. \\tag{5.2} \\end{equation}\\] Example 5.2 Continuing from the previous example, we can see that writing the model as in (5.2) greatly improves the condition number: x.tilde &lt;- 5 m2 &lt;- lm(y ~ I(x-x.tilde) + I((x-x.tilde)^2) + I((x-x.tilde)^3)) kappa(m2, exact = TRUE) ## [1] 76.59629 While there is still collinearity, the condition number is now much smaller. Polynomials of higher degree take very large values as \\(|x|\\) increases and often make poor global models. Instead, these polynomials are best used for local interpolation of data. 5.3 Polynomial Regression with Weights The idea of local polynomial regression is to combine polynomial regression with weights which give more weight to close by observations: to get an estimate at a point \\(\\tilde x \\in \\mathbb{R}\\), we define \\[\\begin{equation*} w_i := K_h(\\tilde x - x_i), \\end{equation*}\\] where \\(K_h\\) is a scaled kernel function as before. Using the diagonal matrix \\(W\\) from (5.1) for the weights and the matrix \\(X\\) from (5.2) as the design matrix, we can fit a polynomial of degree \\(p\\) to the data which fits the data near \\(\\tilde x\\) well. The regression coefficients are again estimated as \\[\\begin{equation*} \\hat\\beta = (X^\\top W X)^{-1} X^\\top W y \\end{equation*}\\] and the model mean near \\(\\tilde x\\) is given by \\[\\begin{equation*} \\hat m_h(x; \\tilde x) = \\hat\\beta_0 + \\hat\\beta_1 (x - \\tilde x) + \\hat\\beta_2 (x - \\tilde x)^2 + \\cdots + \\hat\\beta_p (x - \\tilde x)^p, \\end{equation*}\\] where the coefficients \\(\\hat\\beta\\) depend on \\(\\tilde x\\). The model mean at \\(x = \\tilde x\\) simplifies to \\[\\begin{align*} \\hat m_h(\\tilde x) &amp;= \\hat m_h(\\tilde x; \\tilde x) \\\\ &amp;= \\hat\\beta_0 + \\hat\\beta_1 (\\tilde x - \\tilde x) + \\hat\\beta_2 (\\tilde x - \\tilde x)^2 + \\cdots + \\hat\\beta_p (\\tilde x - \\tilde x)^p \\\\ &amp;= \\hat\\beta_0. \\end{align*}\\] Using matrix notation, we can write this as \\[\\begin{align*} \\hat m_h(\\tilde x) &amp;= \\hat\\beta_0 \\\\ &amp;= e_0^\\top \\hat\\beta \\\\ &amp;= e_0^\\top (X^\\top W X)^{-1} X^\\top W y, \\end{align*}\\] where \\(e_0 = (1, 0, \\ldots, 0) \\in \\mathbb{R}^{p+1}\\). Since both \\(X\\) and \\(W\\) depend on \\(\\tilde x\\), we need to evaluate \\((X^\\top W X)^{-1} X^\\top W y\\) separately for each \\(\\tilde x\\) where an estimate of \\(\\hat m_h\\) is needed. To get a regression line, this needs to be done over a grid of \\(\\tilde x\\) values. Thus, this method can be computationally expensive. 5.4 Special Cases Here we discuss the special cases of \\(p=0\\), \\(p=1\\), and \\(p=2\\). \\(p = 0\\) For \\(p=0\\), the polynomial consists only of the constant term \\(\\beta_0\\). In this case, the design matrix \\(X\\) simplifies to \\(X = (1, \\ldots, 1) \\in \\mathbb{R}^{n\\times 1}\\). Thus we have \\[\\begin{align*} X^\\top W X &amp;= (1, \\ldots, 1)^\\top W (1, \\ldots, 1) \\\\ &amp;= \\sum_{i=1}^n w_i \\\\ &amp;= \\sum_{i=1}^n K_h(\\tilde x - x_i). \\end{align*}\\] Similarly, we have \\[\\begin{align*} X^\\top W y &amp;= (1, \\ldots, 1)^\\top W y \\\\ &amp;= \\sum_{i=1}^n w_i y_i \\\\ &amp;= \\sum_{i=1}^n K_h(\\tilde x - x_i) y_i. \\end{align*}\\] Thus we find \\[\\begin{align*} \\hat m_h(\\tilde x) &amp;= (X^\\top W X)^{-1} X^\\top W y \\\\ &amp;= \\frac{\\sum_{i=1}^n K_h(\\tilde x - x_i) y_i}{\\sum_{i=1}^n K_h(\\tilde x - x_i)}. \\end{align*}\\] This is the same formula as in definition 4.1: for \\(p=0\\) the local polynomial regression estimator is the same as the Nadaraya-Watson estimator. \\(p = 1\\) For \\(p=1\\) the polynomial is a straight line, allowing to model the value as well as the slope of the mean line. The resulting estimator is called the local linear estimator. This sometimes gives a better fit than the Nadaraya-Watson estimator, for example at the boundaries of the domain. Example 5.3 We can use the R function locpoly from the KernSmooth package to compute locally polynomial regression estimates. Here we plot the estimate for \\(p=1\\) (blue line) together with the Nadaraya-Watson estimator (red line), for a simple, simulated dataset. Unfortunately, the function locpoly() has an interpretation of the bandwidth which is different from what ksmooth() uses. Experimentally I found that bandwidth = 0.3 for ksmooth() corresponds to bandwidth = 0.11 for locpoly(): the output of ksmooth(..., bandwidth = 0.3) and of locpoly(.., degree = 0, bandwidth = 0.11) is nearly identical. set.seed(20211103) n &lt;- 200 x &lt;- seq(0, 1, length.out = n) y &lt;- x + rnorm(n, sd = 0.05) plot(x, y, cex = .5) m1 &lt;- ksmooth(x, y, kernel = &quot;normal&quot;, bandwidth = 0.3) lines(m1, col = &quot;red&quot;) library(KernSmooth) ## KernSmooth 2.23 loaded ## Copyright M. P. Wand 1997-2009 m2 &lt;- locpoly(x, y, degree = 1, bandwidth = 0.11) lines(m2, col = &quot;blue&quot;) Near the boundaries the Nadaraya-Watson estimator is biased, because on the left-hand boundary all nearby samples correspond to larger values of the mean line, and similarly on the right-hand boundary all nearby samples correspond to smaller values of the mean line. In contrast, the local polynomial estimate retains its slope right up to the boundary. \\(p = 2\\) For \\(p=2\\) the local polynomials are parabolas. This sometimes allows us to reduce bias near peaks. Example 5.4 We compare the Nadaraya-Watson estimator to locally polynomial regression with \\(p=2\\), using a simulated dataset which has a peak in the middle of the domain. We choose the same bandwidths as in the previous example. set.seed(20211103) n &lt;- 200 x &lt;- seq(-1, 1, length.out = n) y &lt;- 1/(x^2 + 0.05) + rnorm(n, sd = 1) plot(x, y, cex = .5) m1 &lt;- ksmooth(x, y, kernel = &quot;normal&quot;, bandwidth = 0.3, n.points = 100) lines(m1, col = &quot;red&quot;) library(KernSmooth) m2 &lt;- locpoly(x, y, degree = 2, bandwidth = 0.11) lines(m2, col = &quot;blue&quot;) We can see that the Nadaraya-Watson estimator (red line) is biased near the peak, because all nearby samples correspond to smaller values of the mean line. In contrast, the local polynomial estimate (blue line) has much lower bias. Summary Regression with weights can be used to fit models to the data near a given point. A simple application of linear regression can fit polynomials as well as straight lines. The local polynomial regression estimator is a generalization of the Nadaraya-Watson estimator. "],["X06-nearest.html", "Section 6 k-Nearest Neighbour Regression 6.1 Definition of the Estimator 6.2 Properties 6.3 Numerical Experiment 6.4 Variants of the Method", " Section 6 k-Nearest Neighbour Regression In the previous sections we used a fixed bandwidth \\(h\\) to determine the scale on which “closeness” of existing samples to a new input \\(x\\) was measured. While this approach generally works well, problems can appear in regions where samples are sparse (e.g. in example 4.2). This problem can be addressed by choosing \\(h\\) adaptively, using larger bandwidths where samples are sparse and smaller bandwidths in regions where there are many samples. The \\(k\\)-nearest neighbour method is one of several methods which implements this idea. 6.1 Definition of the Estimator Definition 6.1 For \\(k \\in \\{1, \\ldots, n\\}\\), the k-nearest neighbour, or \\(k\\)-NN estimate for the model mean \\(m(x)\\) is given by \\[\\begin{equation} \\hat m_k(x) := \\frac1k \\sum_{i\\in J_k(x)} y_i, \\tag{6.1} \\end{equation}\\] where \\[\\begin{equation*} J_k(x) := \\bigl\\{ i \\bigm| \\mbox{$x_i$ is one of the $k$ nearest observations to $x$} \\bigr\\}. \\end{equation*}\\] The \\(k\\)-NN estimate \\(\\hat m_k(x)\\) is the average of the \\(k\\) responses where the inputs are closest to \\(x\\). We can interpret equation (6.1) as a weighted average \\[\\begin{equation*} \\hat m_k(x) = \\sum_{i=1}^n w_i(x) y_i, \\end{equation*}\\] where the weights are given by \\[\\begin{equation*} w_i(x) = \\begin{cases} \\frac1k, &amp; \\mbox{if $i \\in J_k(x)$, and} \\\\ 0 &amp; \\mbox{otherwise.} \\end{cases} \\end{equation*}\\] If several \\(x_i\\) have the same distance to \\(x\\), some tie-breaking rule must be used to decide which indices to include in the set \\(J_k(x)\\). This case is so unlikely that the choice of rule is not important. One could, for example, pick one of the tied neighbours at random. The method can be used both for the one-dimensional case \\(x\\in\\mathbb{R}\\), and for vector-valued inputs \\(x\\in\\mathbb{R}^p\\). For the one-dimensional case, it is advantageous to sort the data in order of increasing \\(x_i\\). In this case, the position of \\(x\\) in the list of the \\(x_i\\) can be found using a binary search, and the nearest neighbours can be identified by search to the left and right of this position. For \\(p &gt; 1\\) the method becomes computationally very expensive, since the data needs to be sorted afresh for every new input \\(x\\). Advanced data structures like “cover trees” can be used to speed up the process of finding the nearest neighbours. 6.2 Properties The parameter \\(k\\) controls the “smoothness” of the estimate. In the extreme case \\(k = n\\), we have \\(J_n(x) = \\{1, \\ldots, n\\}\\) and \\[\\begin{equation*} \\hat m_k(x) = \\frac1n \\sum_{i=1}^n y_i \\end{equation*}\\] for all \\(x\\), i.e. for this case \\(\\hat m_k\\) is constant. The other extreme is the case of \\(k=1\\), where \\(\\hat m_k(x)\\) always equals the value of the closest \\(x_i\\) and has jumps at the mid-points between the data points. In the next section we will learn how \\(k\\) can be chosen using cross-validation. Independent of the value of \\(k\\), the function \\(\\hat m_k\\) is always a step function, with jumps at points \\(x\\) where two points have equal distance from \\(x\\). 6.3 Numerical Experiment In R, an implementation of the \\(k\\)-NN method can be found in the FNN package. This package implements not only \\(k\\)-NN regression, but also \\(k\\)-NN classification, and it implements sophisticated algorithms to speed up the search for the nearest neighbours in higher-dimensional spaces. The function to perform \\(k\\)-NN regression is knn.reg(). Example 6.1 Here we compute a \\(k\\)-NN estimate for the mean of the faithful dataset, which we have already encountered in examples 4.1 and 4.2. We start by storing the data in the variables x and y: x &lt;- faithful$eruptions y &lt;- faithful$waiting Now we use knn.reg() to compute the \\(k\\)-NN estimate on a grid of values. The help page for knn.reg() explains that we need to convert the input to either a matrix or a data frame; here we use data frames. The grid of input values where we want to estimate the \\(k\\)-NN estimate is passed in via the optional argument test = .... Here we use the arbitrarily chosen value \\(k = 50\\). library(FNN) x.test &lt;- seq(1.5, 5, length.out = 500) m &lt;- knn.reg(data.frame(x=x), y = y, test = data.frame(x=x.test), k = 50) plot(x, y, cex=.5) lines(x.test, m$pred, col = &quot;blue&quot;) The estimated mean curve looks reasonable. 6.4 Variants of the Method For one-dimensional inputs and even \\(k\\), the symmetric k-NN estimate averages the responses corresponding to the \\(k/2\\) nearest neighbours smaller than \\(x\\) and the \\(k/2\\) nearest neighbours larger than \\(x\\). To obtain a continuous estimate, we can define a “local bandwidth” \\(h(x)\\) as \\[\\begin{equation*} h(x) = c \\max\\bigl\\{ |x - x_i| \\bigm| i \\in J_k(x) \\bigr\\} \\end{equation*}\\] for some constant \\(c\\), and then use the Nadaraya-Watson estimator with this bandwidth: \\[\\begin{equation*} \\tilde m(x) = \\frac{\\sum_{i=1}^n K_{h(x)}(x - x_i) y_i}{\\sum_{j=1}^n K_{h(x)}(x - x_j)}, \\end{equation*}\\] where \\(K\\) is a kernel function as before. If we use \\(c = 1\\) together with the uniform kernel \\[\\begin{equation*} K(x) = \\begin{cases} 1/2 &amp; \\mbox{if $-1 \\leq x \\leq 1$} \\\\ 0 &amp; \\mbox{otherwise,} \\end{cases} \\end{equation*}\\] this method coincides with the \\(k\\)-NN estimator. "],["X07-splines.html", "Section 7 Spline Smoothing 7.1 Smoothing Splines 7.2 Cubic Splines 7.3 Degrees of Freedom 7.4 Smoothing Splines in R 7.5 Comparison with Kernel Methods", " Section 7 Spline Smoothing The previous sections showed how kernel methods and \\(k\\)-nearest neighbour methods estimate the regression function \\(m\\colon \\mathbb{R}\\to \\mathbb{R}\\) from data \\((x_1, y_1), \\ldots, (x_n, y_n)\\). In this section we introduce an alternative approach based on spline smoothing. Instead of local averaging, we fit a smooth function globally to the entire dataset, using a penalty term to control the smoothness. 7.1 Smoothing Splines Spline smoothing finds a function \\(m\\) that balances two competing goals: Fit the data well: minimise the residual sum of squares \\(\\sum_{i=1}^n \\bigl( y_i - m(x_i) \\bigr)^2\\). Remain smooth: avoid excessive curvature. We measure the curvature of a function \\(m\\) using the integral of the squared second derivative: \\[\\begin{equation*} \\int_{-\\infty}^\\infty \\bigl( m&#39;&#39;(x) \\bigr)^2 \\,dx. \\end{equation*}\\] A function with high curvature produces a large integral; a nearly linear function produces a small integral. A linear function \\(m(x) = a + bx\\) has \\(m&#39;&#39;(x) = 0\\), giving an integral of zero. Definition 7.1 The smoothing spline estimate for the regression function \\(m\\) is the function \\(\\hat m_\\lambda\\) which minimises \\[\\begin{equation} I(m) = \\sum_{i=1}^n \\bigl( y_i - m(x_i) \\bigr)^2 + \\lambda \\int_{-\\infty}^\\infty \\bigl( m&#39;&#39;(x) \\bigr)^2 \\,dx \\tag{7.1} \\end{equation}\\] over all twice differentiable functions \\(m\\colon \\mathbb{R}\\to \\mathbb{R}\\). The parameter \\(\\lambda \\geq 0\\) is called the smoothing parameter. The smoothing parameter \\(\\lambda\\) controls the trade-off between fitting the data and smoothness: For \\(\\lambda = 0\\), only the residual sum of squares matters, and the solution is the interpolating function which passes through all data points. For \\(\\lambda \\to \\infty\\), only the smoothness matters, and the solution is the linear regression line (which has zero curvature). For intermediate values of \\(\\lambda\\), the solution balances fit and smoothness. This is similar to ridge regression (see section 16 of the level 3 notes), where a penalty term \\(\\lambda \\|\\beta\\|^2\\) is added to the residual sum of squares to control the size of the coefficients. 7.2 Cubic Splines Before we can state the solution to the optimization problem in definition 7.1, we need to introduce the concept of a cubic spline. Definition 7.2 A cubic spline with knots \\(\\kappa_1 &lt; \\kappa_2 &lt; \\cdots &lt; \\kappa_k\\) is a function \\(s\\colon \\mathbb{R}\\to \\mathbb{R}\\) such that \\(s\\) is a cubic polynomial on each interval \\((-\\infty, \\kappa_1)\\), \\((\\kappa_1, \\kappa_2)\\), …, \\((\\kappa_{k-1}, \\kappa_k)\\), and \\((\\kappa_k, \\infty)\\), and \\(s\\) is twice continuously differentiable, i.e. \\(s\\), \\(s&#39;\\) and \\(s&#39;&#39;\\) are all continuous. A natural cubic spline is a cubic spline which is linear on the intervals \\((-\\infty, \\kappa_1)\\) and \\((\\kappa_k, \\infty)\\). The points \\(\\kappa_1, \\ldots, \\kappa_k\\) are called knots. At each knot, the function \\(s\\) transitions from one cubic polynomial to another, but the transition is smooth because \\(s\\), \\(s&#39;\\) and \\(s&#39;&#39;\\) are all continuous. Example 7.1 Consider the knots at \\(\\kappa_1 = 0\\), \\(\\kappa_2 = 1\\), and \\(\\kappa_3 = 2\\). We will show that the function defined by On \\((-\\infty, 0)\\): \\(s(x) = \\frac{3}{2}x\\) On \\((0, 1)\\): \\(s(x) = \\frac{3}{2}x - \\frac{1}{2}x^3\\) On \\((1, 2)\\): \\(s(x) = \\frac{3}{2}(2-x) - \\frac{1}{2}(2-x)^3\\) On \\((2, \\infty)\\): \\(s(x) = \\frac{3}{2}(2-x)\\) (linear) is a natural cubic spline, by checking values and continuity at the knots. For \\(\\kappa_1 = 0\\) we have: \\(s(0^-) = 0\\) and \\(s(0^+) = 0\\) \\(s&#39;(0^-) = \\frac{3}{2}\\) and \\(s&#39;(0^+) = \\frac{3}{2}\\) \\(s&#39;&#39;(0^-) = 0\\) and \\(s&#39;&#39;(0^+) = 0\\) For the knot \\(\\kappa_2 = 1\\) we get: \\(s(1^-) = \\frac{3}{2} - \\frac{1}{2} = 1\\) and \\(s(1^+) = \\frac{3}{2} - \\frac{1}{2} = 1\\) \\(s&#39;(1^-) = \\frac{3}{2} - \\frac{3}{2} = 0\\) and \\(s&#39;(1^+) = -\\frac{3}{2} + \\frac{3}{2} = 0\\) \\(s&#39;&#39;(1^-) = -3\\) and \\(s&#39;&#39;(1^+) = -3\\) Finally, for the knot \\(\\kappa_3 = 2\\) we find: \\(s(2^-) = 0\\) and \\(s(2^+) = 0\\) \\(s&#39;(2^-) = -\\frac{3}{2}\\) and \\(s&#39;(2^+) = -\\frac{3}{2}\\) \\(s&#39;&#39;(2^-) = 0\\) and \\(s&#39;&#39;(2^+) = 0\\) Thus, \\(s\\) is indeed a natural cubic spline. The following theorem shows that the solution to the smoothing spline problem is always a natural cubic spline. Theorem 7.1 The solution \\(\\hat m_\\lambda\\) to the optimization problem in definition 7.1 is a natural cubic spline with knots at the data points \\(x_1, \\ldots, x_n\\). Proof. We sketch the main idea of the proof, showing that the solution must be a cubic spline. The “calculus of variations” shows that among all twice-differentiable functions \\(f\\colon [x_i, x_{i+1}] \\to \\mathbb{R}\\) with given values and derivatives at the endpoints, the cubic polynomial minimises \\(\\int_{x_i}^{x_{i+1}} [f&#39;&#39;(x)]^2 dx\\). Equality holds only if \\(\\hat m_\\lambda\\) is already cubic on \\([x_i, x_{i+1}]\\). Now suppose \\(\\hat m_\\lambda\\) is the solution but \\(\\hat m_\\lambda\\) is not a cubic polynomial on some interval \\([x_i, x_{i+1}]\\). We will show that this leads to a contradiction. Let \\(p\\) be the unique cubic polynomial on \\([x_i, x_{i+1}]\\) which matches \\(\\hat m_\\lambda\\) in value and first derivative at both endpoints: \\(p(x_i) = \\hat m_\\lambda(x_i)\\), \\(p&#39;(x_i) = \\hat m_\\lambda&#39;(x_i)\\), \\(p(x_{i+1}) = \\hat m_\\lambda(x_{i+1})\\), and \\(p&#39;(x_{i+1}) = \\hat m_\\lambda&#39;(x_{i+1})\\). Now consider the function \\(\\tilde m\\) which equals \\(\\hat m_\\lambda\\) outside \\([x_i, x_{i+1}]\\) and equals \\(p\\) on \\([x_i, x_{i+1}]\\). Since \\(\\tilde m\\) agrees with \\(\\hat m_\\lambda\\) at all data points, both functions produce the same residual sum of squares. However, \\(\\tilde m\\) has a smaller penalty term: \\[\\begin{equation*} \\int_{-\\infty}^\\infty \\bigl( \\tilde m&#39;&#39;(x) \\bigr)^2 \\,dx &lt; \\int_{-\\infty}^\\infty \\bigl( \\hat m_\\lambda&#39;&#39;(x) \\bigr)^2 \\,dx, \\end{equation*}\\] by the property of cubic polynomials mentioned above. This contradicts the assumption that \\(\\hat m_\\lambda\\) minimises equation (7.1). Therefore, \\(\\hat m_\\lambda\\) must be a cubic polynomial on each interval \\([x_i, x_{i+1}]\\). A similar argument shows that \\(\\hat m_\\lambda\\) must be twice continuously differentiable at the knots, completing the proof that \\(\\hat m_\\lambda\\) is a cubic spline. The theorem shows that the smoothing spline is a natural cubic spline with \\(n\\) knots at the data points \\(x_1, \\ldots, x_n\\). Such a spline consists of: A linear function on \\((-\\infty, x_1)\\): 2 parameters Cubic polynomials on each of the \\(n-1\\) intervals between consecutive knots: \\(4(n-1)\\) parameters A linear function on \\((x_n, \\infty)\\): 2 parameters This gives \\(2 + 4(n-1) + 2 = 4n\\) parameters initially. However, at each of the \\(n\\) knots, we require continuity of the function, its first derivative, and its second derivative. This imposes \\(3n\\) constraints, leaving \\(4n - 3n = n\\) free parameters. With \\(n\\) free parameters and \\(n\\) data points \\((x_1, y_1), \\ldots, (x_n, y_n)\\), we could make the spline pass through all data points exactly—a perfect interpolating fit. This corresponds to \\(\\lambda = 0\\). For \\(\\lambda &gt; 0\\), the penalty term in equation (7.1) forces the spline to be smoother, in exchange for imperfect fit. 7.3 Degrees of Freedom As with linear regression, we can write the smoothing spline estimate in matrix form. Let \\(y = (y_1, \\ldots, y_n)^\\top\\) be the vector of responses, and let \\(\\hat y = (\\hat m_\\lambda(x_1), \\ldots, \\hat m_\\lambda(x_n))^\\top\\) be the vector of fitted values. Then we can write \\[\\begin{equation} \\hat y = S_\\lambda y, \\tag{7.2} \\end{equation}\\] where \\(S_\\lambda \\in \\mathbb{R}^{n\\times n}\\) is the smoother matrix. This is analogous to the hat matrix \\(H = X(X^\\top X)^{-1}X^\\top\\) in linear regression (see section 2 of the level 3 notes). The smoother matrix \\(S_\\lambda\\) depends on the smoothing parameter \\(\\lambda\\) and on the data points \\(x_1, \\ldots, x_n\\), but not on the responses \\(y_1, \\ldots, y_n\\). We omit the explicit formula for \\(S_\\lambda\\), but note that efficient algorithms exist for computing it. Definition 7.3 The effective degrees of freedom of the smoothing spline estimate is \\[\\begin{equation*} \\mathop{\\mathrm{df}}\\nolimits(\\lambda) = \\mathop{\\mathrm{tr}}\\nolimits(S_\\lambda), \\end{equation*}\\] where \\(\\mathop{\\mathrm{tr}}\\nolimits\\) denotes the trace of a matrix (the sum of the diagonal elements). The effective degrees of freedom measure the complexity of the fitted model: For \\(\\lambda = 0\\), the smoother matrix is \\(S_0 = I\\) (the identity matrix), and we have \\(\\mathop{\\mathrm{df}}\\nolimits(0) = n\\). This corresponds to interpolation, where we use all \\(n\\) data points. For \\(\\lambda \\to \\infty\\), the smoother matrix converges to the hat matrix of linear regression, and we have \\(\\mathop{\\mathrm{df}}\\nolimits(\\infty) = 2\\). This corresponds to fitting a straight line. For intermediate values of \\(\\lambda\\), we have \\(2 &lt; \\mathop{\\mathrm{df}}\\nolimits(\\lambda) &lt; n\\). The effective degrees of freedom provide an alternative way to specify the amount of smoothing: choose a target value for \\(\\mathop{\\mathrm{df}}\\nolimits(\\lambda)\\) and find the corresponding \\(\\lambda\\). 7.4 Smoothing Splines in R R computes smoothing splines using the built-in function smooth.spline(), which takes the following arguments: x and y: the data points. spar: the smoothing parameter. This is related to \\(\\lambda\\) but uses a different scale: specifically, \\(\\lambda = r \\cdot 256^{3 \\cdot \\text{spar} - 1}\\) where \\(r\\) is a data-dependent scaling factor. If not specified, the function chooses a default value. df: the target degrees of freedom. If specified, the function finds the value of spar which gives the desired degrees of freedom. The return value is an object which contains the fitted spline. The most important components are: $x and $y: the fitted spline evaluated at the data points (or at a grid of points if you set the optional argument all.knots = FALSE). $df: the effective degrees of freedom of the fitted spline. $lambda: the smoothing parameter \\(\\lambda\\). Example 7.2 We illustrate smoothing splines using the faithful dataset, which contains waiting times between eruptions and eruption durations for the Old Faithful geyser. We fit smoothing splines with different degrees of freedom and compare them to kernel methods. # Load the data data(faithful) x &lt;- faithful$waiting y &lt;- faithful$eruptions # Sort the data for plotting ord &lt;- order(x) x &lt;- x[ord] y &lt;- y[ord] # Fit smoothing splines with different df par(mfrow = c(1, 2)) # Left panel: varying degrees of freedom plot(x, y, main = &quot;Smoothing Splines with Different df&quot;, xlab = &quot;Waiting time (min)&quot;, ylab = &quot;Duration (min)&quot;, pch = 16, col = &quot;grey70&quot;, cex = 0.7) # Fit splines with df = 3, 6, and 15 fit3 &lt;- smooth.spline(x, y, df = 3) fit6 &lt;- smooth.spline(x, y, df = 6) fit15 &lt;- smooth.spline(x, y, df = 15) lines(fit3, col = &quot;blue&quot;, lwd = 2) lines(fit6, col = &quot;red&quot;, lwd = 2) lines(fit15, col = &quot;darkgreen&quot;, lwd = 2) legend(&quot;bottomright&quot;, legend = c(&quot;df = 3&quot;, &quot;df = 6&quot;, &quot;df = 15&quot;), col = c(&quot;blue&quot;, &quot;red&quot;, &quot;darkgreen&quot;), lwd = 2, bty = &quot;n&quot;) # Right panel: comparison with kernel smoother plot(x, y, main = &quot;Spline vs. Kernel Smoother&quot;, xlab = &quot;Waiting time (min)&quot;, ylab = &quot;Duration (min)&quot;, pch = 16, col = &quot;grey70&quot;, cex = 0.7) # Smoothing spline with df = 15 lines(fit15, col = &quot;darkgreen&quot;, lwd = 2) # Nadaraya-Watson kernel smoother with automatic bandwidth h &lt;- bw.nrd(x) fit_nw &lt;- ksmooth(x, y, kernel = &quot;normal&quot;, bandwidth = h) lines(fit_nw, col = &quot;blue&quot;, lwd = 2) legend(&quot;bottomright&quot;, legend = c(&quot;Spline (df = 15)&quot;, paste(&quot;Nadaraya-Watson (h =&quot;, round(h, 1), &quot;)&quot;)), col = c(&quot;darkgreen&quot;, &quot;blue&quot;), lwd = 2, bty = &quot;n&quot;) The left panel shows how degrees of freedom control smoothness: \\(\\mathop{\\mathrm{df}}\\nolimits= 3\\) gives an almost linear fit, while \\(\\mathop{\\mathrm{df}}\\nolimits= 15\\) follows the data closely. The right panel compares the smoothing spline (\\(\\mathop{\\mathrm{df}}\\nolimits= 15\\)) with the Nadaraya-Watson estimator (using the normal reference rule for bandwidth selection). Both methods produce similar estimates, though the spline has better boundary behaviour. 7.5 Comparison with Kernel Methods Both smoothing splines and kernel methods estimate the regression function \\(m\\) from data, but they differ in strengths and weaknesses. Advantages of smoothing splines: Global method: The spline fits the entire dataset at once, which gives better results at the boundaries of the data range. Automatic smoothness: The solution is always twice continuously differentiable. Efficient computation: Solving a system of linear equations computes the spline, avoiding local fits at many points. Advantages of kernel methods: Local adaptation: Kernel methods can adapt to local features of the data, for example by using different bandwidths in different regions (as in \\(k\\)-NN methods). Geometric interpretation: Kernel methods use weighted averages, which are easier to understand than the penalized least squares formulation of splines. Robustness: Robust kernels or trimmed extreme values make kernel methods resistant to outliers. In practice, the choice between smoothing splines and kernel methods often depends on the specific application and the properties of the data. Example 7.3 We compare smoothing splines with Nadaraya-Watson and local linear regression using simulated data where the true regression function is known. This allows us to assess the performance of each method, particularly near the boundaries. # Generate simulated data set.seed(123) n &lt;- 100 x &lt;- sort(runif(n, 0, 4)) m_true &lt;- function(t) sin(2*t) + 2*exp(-16*(t-2)^2) y &lt;- m_true(x) + rnorm(n, 0, 0.3) # Grid for evaluation x_grid &lt;- seq(0, 4, length = 200) y_true &lt;- m_true(x_grid) # Nadaraya-Watson estimator nw_estimate &lt;- function(x_new, x, y, h) { K &lt;- dnorm((x_new - x) / h) sum(K * y) / sum(K) } # Local linear estimator local_linear &lt;- function(x_new, x, y, h) { w &lt;- dnorm((x - x_new) / h) X &lt;- cbind(1, x - x_new) beta &lt;- solve(t(X) %*% diag(w) %*% X) %*% t(X) %*% diag(w) %*% y beta[1] } # Choose bandwidth using rule of thumb h &lt;- 0.3 # Compute estimates y_nw &lt;- sapply(x_grid, nw_estimate, x = x, y = y, h = h) y_ll &lt;- sapply(x_grid, local_linear, x = x, y = y, h = h) # Smoothing spline fit_spline &lt;- smooth.spline(x, y, cv = TRUE) y_spline &lt;- predict(fit_spline, x_grid)$y # Plotting par(mfrow = c(1, 2)) # Left panel: all three methods plot(x, y, main = &quot;Comparison of Smoothing Methods&quot;, xlab = &quot;x&quot;, ylab = &quot;y&quot;, pch = 16, col = &quot;grey70&quot;, cex = 0.7) lines(x_grid, y_true, col = &quot;black&quot;, lwd = 2, lty = 2) lines(x_grid, y_nw, col = &quot;blue&quot;, lwd = 2) lines(x_grid, y_ll, col = &quot;darkgreen&quot;, lwd = 2) lines(x_grid, y_spline, col = &quot;red&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;True function&quot;, &quot;Nadaraya-Watson&quot;, &quot;Local linear&quot;, paste(&quot;Spline (df =&quot;, round(fit_spline$df, 1), &quot;)&quot;)), col = c(&quot;black&quot;, &quot;blue&quot;, &quot;darkgreen&quot;, &quot;red&quot;), lty = c(2, 1, 1, 1), lwd = 2, bty = &quot;n&quot;, cex = 0.8) # Right panel: zoom on boundary region plot(x, y, main = &quot;Boundary Behaviour (x &lt; 0.5)&quot;, xlim = c(0, 0.5), ylim = c(0, 2), xlab = &quot;x&quot;, ylab = &quot;y&quot;, pch = 16, col = &quot;grey70&quot;, cex = 0.7) # Only plot in boundary region idx &lt;- x_grid &lt;= 0.5 lines(x_grid[idx], y_true[idx], col = &quot;black&quot;, lwd = 2, lty = 2) lines(x_grid[idx], y_nw[idx], col = &quot;blue&quot;, lwd = 2) lines(x_grid[idx], y_ll[idx], col = &quot;darkgreen&quot;, lwd = 2) lines(x_grid[idx], y_spline[idx], col = &quot;red&quot;, lwd = 2) # Add vertical line at x = h to show bandwidth abline(v = h, col = &quot;grey&quot;, lty = 3) text(h, 0.2, &quot;h&quot;, pos = 4, col = &quot;grey&quot;) The comparison reveals several key differences: Boundary effects: The Nadaraya-Watson estimator (blue) shows clear bias near \\(x = 0\\), underestimating the function. The local linear estimator (green) performs better at the boundary but still shows bias. The smoothing spline (red) handles the boundary well through its global nature. Peak capture: All methods struggle with the sharp peak at \\(x = 2\\), but the smoothing spline adapts well despite using a global smoothing parameter. Smoothness: The spline produces the smoothest estimate, constrained to be twice differentiable everywhere. The kernel methods show more local variation. The choice of method depends on the specific requirements—boundary behaviour, local adaptation, or global smoothness. Summary Smoothing splines balance fit and smoothness using a penalty term. The solution is a natural cubic spline with knots at the data points. The smoothing parameter \\(\\lambda\\) controls the trade-off between fit and smoothness. Effective degrees of freedom measure model complexity. Smoothing splines are a global alternative to local kernel methods. "],["X08-xval.html", "Section 8 Cross-validation 8.1 Regression 8.2 Kernel Density Estimation", " Section 8 Cross-validation In this section we will discuss methods for chosing the bandwidth \\(h\\) for kernel-based methods, and the size \\(k\\) of the neighbourhood used in \\(k\\)-nearest neighbour regression. The methods we will use here are based on cross-validation. The idea of cross-validation is to measure goodness of fit by comparing each sample \\(y_i\\) to a fitted value \\(\\tilde y_i\\), where the fitted values \\(\\tilde y_i\\) are computed from a subset of the data which excludes \\(x_i\\). This way, a method cannot achive a misleadingly good fit by overfitting the data. There are different ways to implement this idea: In k-fold cross-validation, the data is partitioned into \\(k\\) subsamples of approximately equal size. Only \\(k\\) models are fitted, each one leaving out the data from one subsample. Then for every \\(i\\) there is exactly one model which does not use \\((x_i, y_i)\\), and to compute the fitted value for \\((x_i, y_i)\\), we use this model. Since fewer data are used to fit each model, this method gives less accurate results than leave-one-out cross-validation. For this method to work, it is important that the subsamples are independent of each other. In leave-one-out cross-validation, a separate model is fitted for each \\(i \\in \\{1, \\ldots, n\\}\\), leaving out just the sample \\((x_i, y_i)\\) for this model. This is the special case of \\(k\\)-fold cross validation where \\(k = n\\). Since \\(n\\) models need to be fitted to the data, for large \\(n\\) the method can be computationally expensive. 8.1 Regression In linear regression, we find the regression line by minimising the residual sum of squares. One could be tempted to try the same approach here and to find the “best” \\(h\\) by minimising \\[\\begin{equation*} r(h) := \\sum_{i=1}^n \\bigl( y_i - \\hat m_h(x_i) \\bigr)^2. \\end{equation*}\\] Unfortunately, this approach does not work: for \\(h \\downarrow 0\\) we have \\(\\hat m_h(x_i) \\to y_i\\) and thus \\(r(h) \\to 0\\). For this reason, minimising \\(r(h)\\) always finds \\(h=0\\) as the optimal value of \\(h\\). To solve the problem we use leave-one-out cross validation and minimise \\[\\begin{equation*} r_\\mathrm{LOO}(h) := \\sum_{i=1}^n \\bigl( y_i - \\hat m^{(i)}_h(x_i) \\bigr)^2 \\end{equation*}\\] instead, where \\(m^{(i)}\\) is the kernel regression estimate computed without using sample \\(i\\): \\[\\begin{equation*} \\hat m_h^{(i)}(x) = \\frac{\\sum_{j \\;\\mid\\; j\\neq i} K_h(x - x_j)y_j}{\\sum_{j \\;\\mid\\; j\\neq i}K_h(x - x_j)} \\end{equation*}\\] for the Nadaraya-Watson Estimator and similarly for local polynomial regression. A similar approach can be used to find the optimal \\(k\\) for a \\(k\\)-nearest neighbour estimate. 8.2 Kernel Density Estimation When using kernel density estimation in practice, we need to choose the bandwidth \\(h\\). One idea for finding a good \\(h\\) is by using maximum likelihood estimation: We could try to choose \\(h\\) to maximize the likelihood \\[\\begin{equation*} L(h;x_1, \\ldots, x_n) = \\prod_{i=1}^n \\hat f_h(x_i), \\end{equation*}\\] but this gives the solution \\(h=0\\). So, instead we maximize the leave-one-out estimate of the log likelihood, which is given by \\[\\begin{equation*} L_\\mathrm{LOO}(h;x_1, \\ldots, x_n) = \\prod_{i=1}^n \\hat f^{(i)}_h(x_i). \\end{equation*}\\] This technique is known as maximum likelihood cross-validation. When this method is used in practice, it is advantageous to minimise \\[\\begin{align*} \\mathcal{L}_\\mathrm{LOO}(h;x_1, \\ldots, x_n) &amp;:= \\log\\Bigl( L_\\mathrm{LOO}(h;x_1, \\ldots, x_n) \\Bigr) \\\\ &amp;= \\log\\Bigl( \\prod_{i=1}^n \\hat f^{(i)}_h(x_i) \\Bigr) \\\\ &amp;= \\sum_{i=1}^n \\log\\bigl( \\hat f^{(i)}_h(x_i) \\bigr) \\end{align*}\\] instead of \\(L_\\mathrm{LOO}\\), since the product in the definition of the likelihood can be strongly affected by numerical errors. An alternative method to find a good \\(h\\) considers the integrated mean squared error (IMSE) as a measure for the error. This is the same quantity we also used to derive our theoretical results: \\[\\begin{equation*} \\mathrm{IMSE}\\bigl( \\hat f_h \\bigr) = \\int_{-\\infty}^\\infty \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) \\,dx. \\end{equation*}\\] Unfortunately, the \\(h\\) which minimises this expression depends on properties of \\(f\\) and in section 3.2 we were only able to find a heuristic “plug-in” estimate to approximate the best \\(h\\). The following lemma shows how a variant of leave-one-out cross-validation can be used to estimate the optimal \\(h\\) from data. Lemma 8.1 Let \\(\\hat f_h\\) be the kernel density estimate with bandwidth \\(h\\) and let \\[\\begin{equation*} e(h) := \\int \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) dx - 2 \\int \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) f(x) dx. \\end{equation*}\\] Then the following statements hold. We have \\(\\mathrm{IMSE}\\bigl( \\hat f_h \\bigr) = e(h) + \\mathrm{const}\\), where \\(\\mathrm{const}\\) stands for a term which does not depend on \\(h\\). Let \\(f_h^{(i)}\\) be the kernel density estimate computed from the data with sample \\(i\\) omitted. Then \\[\\begin{equation*} \\mathrm{CV}(h) := \\int \\hat f_h(x)^2 dx - \\frac{2}{n}\\sum_{i=1}^n \\hat f_h^{(i)}(x_i) \\end{equation*}\\] is an (approximately) unbiased estimator for \\(e(h)\\). Proof. For the first statement can be shown by expanding the square in the definition of the (I)MSE: \\[\\begin{align*} \\mathrm{IMSE}\\bigl( \\hat f_h \\bigr) &amp;= \\int_{-\\infty}^\\infty \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) \\,dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\mathbb{E}\\Bigl( \\bigl( \\hat f_h(x) - f(x) \\bigr)^2 \\Bigr) \\,dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\mathbb{E}\\Bigl( \\hat f_h(x)^2 - 2 \\hat f_h(x) f(x) + f(x)^2 \\Bigr) \\,dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\mathbb{E}\\Bigl( \\hat f_h(x)^2 \\Bigr) \\,dx - 2 \\int_{-\\infty}^\\infty \\mathbb{E}\\Bigl( \\hat f_h(x) f(x) \\Bigr) \\,dx \\\\ &amp;\\hskip5cm + \\int_{-\\infty}^\\infty \\mathbb{E}\\Bigl( f(x)^2 \\Bigr) \\,dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\mathbb{E}\\Bigl( \\hat f_h(x)^2 \\Bigr) \\,dx - 2 \\int_{-\\infty}^\\infty \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) f(x) \\,dx \\\\ &amp;\\hskip5cm + \\int_{-\\infty}^\\infty f(x)^2 \\,dx, \\end{align*}\\] where we used the fact that \\(f(x)\\) is not random. Since the last term does not depend on \\(h\\), the first claim is proved. For the second statement we need to consider the kernel density estimates computed from random data \\(X_1, \\ldots, X_n \\sim f\\), i.i.d. We have to show that \\[\\begin{equation*} \\mathbb{E}\\bigl( \\mathrm{CV}(h) \\bigr) = e(h) = \\int \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) dx - 2 \\int \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) f(x) dx. \\end{equation*}\\] For the first term in the definition of \\(\\mathrm{CV}\\) we get \\[\\begin{equation*} \\mathbb{E}\\Bigl( \\int \\hat f_h(x)^2 dx \\Bigr) = \\int \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) dx, \\end{equation*}\\] where we used Fubini’s theorem to exchange the expectation with the integral. For the second term we have \\[\\begin{align*} \\mathbb{E}\\Bigl( \\frac{1}{n} \\sum_{i=1}^n \\hat f_h^{(i)}(X_i) \\Bigr) &amp;= \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}\\Bigl( \\hat f_h^{(i)}(X_i) \\Bigr) \\\\ &amp;= \\mathbb{E}\\Bigl( \\hat f_h^{(1)}(X_1) \\Bigr), \\end{align*}\\] since the \\(X_i\\) are independent and identically distributed. Since the estimator \\(f_h^{(1)}\\) is computed from \\(X_2, \\ldots, X_n\\), which are independent of \\(X_1\\), we can evaluate the expecation on the right-hand side by first computing \\(\\mathbb{E}\\Bigl( \\hat f_h^{(1)}(x) \\Bigr)\\) as a function of \\(x\\), then using \\(X_1\\) in place of \\(x\\), and computing the expecation over \\(X_1\\) afterwards. This gives \\[\\begin{align*} \\mathbb{E}\\Bigl( \\frac{1}{n} \\sum_{i=1}^n \\hat f_h^{(i)}(X_i) \\Bigr) &amp;= \\mathbb{E}\\Bigl( \\mathbb{E}\\bigl( \\hat f_h^{(1)}(X_1) \\bigm| X_1 \\bigr) \\Bigr) \\\\ &amp;= \\int \\mathbb{E}\\bigl( \\hat f_h^{(1)}(x) \\bigr) \\, f(x) \\, dx, \\end{align*}\\] since \\(X_1 \\sim f\\). Finally, since \\(\\hat f_h^{(1)}\\) and \\(\\hat f_h\\) only differ in the sample size, by using \\(n-1\\) and \\(n\\) samples respectively, we have \\[\\begin{equation*} \\mathbb{E}\\bigl( \\hat f_h^{(1)}(x) \\bigr) \\approx \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr). \\end{equation*}\\] Combining these equations completes the proof. Using the result from the lemma we see that we can choose \\(h\\) to minimise \\(\\mathrm{CV}(h)\\) in order to get a candidate for the bandwidth \\(h\\). This procedure is known as integrated squared error cross-validation. These two approaches differ slightly in the optimal \\(h\\), but the first one is easier to implement as there is no integration involved. Summary Cross-validation can be used to avoid overfitting the data when we choose \\(h\\). We have seen how to estimate \\(h\\) for kernel-based methods, and \\(k\\) for \\(k\\)-NN regression estimates. "],["X09-examples.html", "Section 9 Examples 9.1 Kernel Density Estimation 9.2 Kernel Regression 9.3 k-Nearest Neighbour Regression", " Section 9 Examples To conclude these notes, we give three examples where we use cross-validation to choose the tuning parameter in kernel density estimation, kernel regression, and k-nearest neighbour regression. 9.1 Kernel Density Estimation Here we show how to find a good bandwidth for Kernel Density Estimation, by using cross-validation. From lemma 8.1 we know that we can choose the \\(h\\) which minimises \\[\\begin{equation} \\mathrm{CV}(h) = \\int \\hat f_h(x)^2 dx - \\frac{2}{n}\\sum_{i=1}^n \\hat f_h^{(i)}(x_i) =: A - B. \\tag{9.1} \\end{equation}\\] We will consider the snow fall dataset again: # data from https://teaching.seehuhn.de/data/buffalo/ buffalo &lt;- read.csv(&quot;data/buffalo.csv&quot;) x &lt;- buffalo$snowfall n &lt;- length(x) In order to speed up the computation of the \\(f_h^{(i)}\\), we implement the kernel density estimate “by hand”. Thus, instead of using the built-in function density, we use the formula \\[\\begin{equation*} \\hat f_h(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i). \\end{equation*}\\] from definition 1.2. We use a number of “tricks” in the R code: For numerically computing the integral of \\(\\hat f_h^2\\) in term \\(A\\) we evaluate \\(\\hat f_h\\) on a grid of \\(x\\)-values, say \\(\\tilde x_1, \\ldots, \\tilde x_m\\). When computing \\(\\hat f_h(\\tilde x_j)\\) we need to compute all pair differences \\(\\tilde x_j - x_i\\). In R, this can efficiently be done using the command outer(x, x.tilde, \"-\"), which returns the pair differences as an \\(n \\times m\\) matrix. Here we use a Gaussian kernel, so that \\(K_h\\) can be evaluated using dnorm(..., sd = h) in R. This function can be applied to the matrix of pair differences; the result is a matrix K where row \\(i\\), column \\(j\\) stores the value \\(K_h(\\tilde x_j - x_i)\\). The kernel density estimate \\(\\hat f_h\\) now corresponds to the column means of the matrix K. In R, these can be efficiently computed using the command colMeans(). Term \\(A\\) in equation (9.1) can now be approximate by the sum of the \\(\\hat f_h(\\tilde x_j)\\), multiplied by the distance between the grid points: \\[\\begin{equation*} A = \\int \\hat f_h(x)^2 \\,dx \\approx \\sum_{j=1}^m \\hat f_h(\\tilde x_j)^2 \\, \\Delta \\tilde x. \\end{equation*}\\] To compute term \\(B\\) in equation (9.1), we can use the formula \\[\\begin{align*} \\sum_{j=1}^n \\hat f_h^{(j)}(x_j) &amp;= \\sum_{j=1}^n \\frac{1}{n-1} \\sum_{i\\neq j} K_h(x_j - x_i) \\\\ &amp;= \\frac{1}{n-1} \\sum_{i,j=1 \\atop i\\neq j}^n K_h(x_j - x_i). \\end{align*}\\] Here we can use outer() again, and then implement the condition \\(i\\neq j\\) by setting the matrix elements corresponding to \\(i=j\\) equal to 0 before taking the sum. Using these ideas, we can implement the function \\(\\mathrm{cv}(h)\\) in R as follows: cv.h &lt;- function(h) { x.min &lt;- min(x) - 3*h x.max &lt;- max(x) + 3*h m &lt;- 1000 dx &lt;- (x.max - x.min) / (m - 1) x.tilde &lt;- seq(x.min, x.max, length.out = m) K &lt;- dnorm(outer(x, x.tilde, &quot;-&quot;), sd = h) f.hat &lt;- colMeans(K) A &lt;- sum(f.hat^2 * dx) K &lt;- dnorm(outer(x, x, &quot;-&quot;), sd = h) diag(K) &lt;- 0 B &lt;- 2 * sum(K) / (n-1) / n return(A - B) } Finally, we evaluate the function cv.h() on a grid of \\(h\\)-values to find a good value of \\(h\\): h &lt;- 10^seq(-1, 3, length.out = 41) cv &lt;- numeric(length(h)) for (i in seq_along(h)) { cv[i] &lt;- cv.h(h[i]) } plot(h, cv, log=&quot;x&quot;, type = &quot;l&quot;) best.h &lt;- h[which.min(cv)] abline(v = best.h) The optimal bandwidth is \\(h = 12.59\\). The kernel density estimate using this \\(h\\) is shown in the following figure. x.min &lt;- min(x) - 3*best.h x.max &lt;- max(x) + 3*best.h m &lt;- 100 x.tilde &lt;- seq(x.min, x.max, length.out = m) K &lt;- dnorm(outer(x, x.tilde, &quot;-&quot;), sd = best.h) f.hat &lt;- colMeans(K) plot(x.tilde, f.hat, type = &quot;l&quot;, xlab = &quot;snowfall [in]&quot;, ylab = &quot;density&quot;) 9.2 Kernel Regression To illustrate cross-validation for the different smoothing methods, we use the faithful dataset again. x &lt;- faithful$eruptions y &lt;- faithful$waiting We compare the methods using the leave-one-out mean squared error \\[\\begin{equation*} r(h) = \\frac1n \\sum_{i=1}^n \\bigl( y_i - \\hat m^{(i)}(x_i) \\bigr)^2. \\end{equation*}\\] We start by considering the Nadaraya-Watson estimator. Here we have to compute \\[\\begin{equation*} \\hat m^{(i)}_h(x_i) = \\frac{\\sum_{j=1, j\\neq i}^n K_h(x_i - x_j) y_j}{\\sum_{j=1, j\\neq i}^n K_h(x_i - x_j)} \\end{equation*}\\] for all \\(i\\in\\{1, \\ldots, n\\}\\). To evaluate this expression in R, we use the same ideas as before: We use outer(x, x, \"-\") to compute all pair differences \\(x_i - x_j\\). We use dnorm(..., sd = h) to compute \\(K_h\\). We can obtain the leave-one-out estimate by setting the diagonal of \\(K\\) to zero. One new idea is needed to compute the products \\(K_h(x_i - x_j) y_j\\) in an efficient way: If we “multiply” a matrix K to a vector y using * (instead of using %*% for the usual matrix vector multiplication), the product is performed element-wise. If y has as many elements as K has rows, then the results is the matrix \\((k_{ij}y_i)_{i,j}\\), i.e. each row of K is multiplied with the corresponding element of y. Combining these ideas, we get the following function to compute the leave-one-out estimate for the mean squared error of the Nadaraya-Watson estimator: r.NW &lt;- function(h) { K &lt;- dnorm(outer(x, x, &quot;-&quot;), sd = h) # compute a leave-one-out estimate diag(K) &lt;- 0 m.hat &lt;- colSums(K*y) / colSums(K) mean((m.hat - y)^2) } We will also consider local linear smoothing, i.e. local polynomial smoothing where the degree \\(p\\) of the polynomials is \\(p=1\\). As we have seen in the section about Polynomial Regression with Weights, the local linear estimator can be computed as \\[\\begin{equation*} \\hat m_h(x) = e_0^\\top (X^\\top W X)^{-1} X^\\top W y, \\end{equation*}\\] where \\(X\\) and \\(W\\) are defined as in equations (5.2) and (5.1). Here we use the “linear” case (\\(p=1\\)) instead of the polynomial case (\\(p\\geq 1\\)). For this case it is easy to check that we have \\[\\begin{equation*} X^\\top W X = \\begin{pmatrix} \\sum_j K_h(x-x_j) &amp; \\sum_j K_h(x-x_j) x_j \\\\ \\sum_j K_h(x-x_j) x_j &amp; \\sum_j K_h(x-x_j) x_j^2 \\end{pmatrix} \\end{equation*}\\] and \\[\\begin{equation*} X^\\top W y = \\begin{pmatrix} \\sum_j K_h(x-x_j) y_j \\\\ \\sum_j K_h(x-x_j) x_j y_j \\end{pmatrix}. \\end{equation*}\\] Using the formula \\[\\begin{equation*} \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d &amp; -b \\\\ -c &amp; a \\end{pmatrix} \\end{equation*}\\] for the inverse of a general \\(2\\times 2\\)-matrix, we find \\[\\begin{equation*} \\hat m_h(x) = \\frac{T_1 T_2 - T_3 T_4}{B_1 B_2 - B_3^2}, \\end{equation*}\\] where \\[\\begin{align*} T_1 &amp;= \\sum_{j=1}^n K_h(x-x_j) y_j , \\\\ T_2 &amp;= \\sum_{j=1}^n K_h(x-x_j) x_j(x_j-x), \\\\ T_3 &amp;= \\sum_{j=1}^n K_h(x-x_j) x_j y_j , \\\\ T_4 &amp;= \\sum_{j=1}^n K_h(x-x_j) (x_j-x) , \\\\ B_1 &amp;= \\sum_{j=1}^n K_h(x-x_j) , \\\\ B_2 &amp;= \\sum_{j=1}^n K_h(x-x_j) x_j^2 , \\\\ B_3 &amp;= \\sum_{j=1}^n K_h(x-x_j) x_j . \\end{align*}\\] As before, for a leave-one-out estimate we need to compute these sums over all \\(j\\neq i\\). Since each of the seven terms listed above contains the term \\(K_h(x-x_j)\\) inside the sum, we can achieve this by setting the corresponing elements of the matrix K to zero. r.LL &lt;- function(h) { dx &lt;- outer(x, x, &quot;-&quot;) K &lt;- dnorm(dx, sd = h) # compute a leave-one-out estimate diag(K) &lt;- 0 T1 &lt;- colSums(y*K) T2 &lt;- colSums(x*dx*K) T3 &lt;- colSums(x*y*K) T4 &lt;- colSums(dx*K) B1 &lt;- colSums(K) B2 &lt;- colSums(x^2*K) B3 &lt;- colSums(x*K) m.hat &lt;- (T1*T2 - T3*T4) / (B1*B2 - B3^2) mean((m.hat - y)^2) } Now we evaluate the function r.NW() and r.LL() on a grid of \\(h\\)-values to find the optimal \\(h\\) for each method. h &lt;- 10^seq(-1.4, 0.1, length.out = 61) mse.nw &lt;- numeric(length(h)) mse.ll &lt;- numeric(length(h)) for (i in seq_along(h)) { mse.nw[i] &lt;- r.NW(h[i]) mse.ll[i] &lt;- r.LL(h[i]) } plot(h, mse.nw, log=&quot;x&quot;, type = &quot;l&quot;, ylim = range(mse.nw, mse.ll), ylab = &quot;leave-one-out MSE&quot;) lines(h, mse.ll, col=&quot;red&quot;) best.h.NW &lt;- h[which.min(mse.nw)] abline(v = best.h.NW) best.h.LL &lt;- h[which.min(mse.ll)] abline(v = best.h.LL, col=&quot;red&quot;) legend(&quot;topleft&quot;, legend = c(&quot;NW&quot;, &quot;LL&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lwd = 2) As expected, the optimal bandwidth for local linear regression is larger than for the Nadaraya Watson estimator. 9.3 k-Nearest Neighbour Regression To conclude this section we use leave-one-out cross-validation to determine the optimal \\(k\\) for \\(k\\)-nearest neighbour regression. Here it seems difficult to make any savings, so we resort to simply fitting \\(n\\) different models in the naive way. For this reason, the code in this section is much slower to run than the code in the previous sections. k &lt;- 10:70 mse.knn &lt;- numeric(length(k)) for (j in seq_along(k)) { y.pred &lt;- numeric(length(x)) for (i in seq_along(x)) { m &lt;- knn.reg(data.frame(x = x[-i]), y = y[-i], test = data.frame(x = x[i]), k = k[j]) y.pred[i] &lt;- m$pred } mse.knn[j] &lt;- mean((y - y.pred)^2) } plot(k, mse.knn, type = &quot;l&quot;, ylab = &quot;leave-one-out MSE&quot;) best.k &lt;- k[which.min(mse.knn)] abline(v = best.k) We note the that leave-one-out mean squared error for kNN is smaller than it is for Nadaraya-Watson or local linear regression, in the case of this dataset. Given the structure of the data, with different regions having very different densities of \\(x\\)-values, it makes sense that a method which choses the bandwidth “adaptively” performs better. To conclude, we show the optimal regression curves for the three smoothing methods together in one plot. x.tilde &lt;- seq(1.5, 5.5, length.out = 501) K &lt;- dnorm(outer(x, x.tilde, &quot;-&quot;), sd = best.h.NW) m.NW &lt;- colSums(K*y) / colSums(K) dx &lt;- outer(x, x.tilde, &quot;-&quot;) K &lt;- dnorm(dx, sd = best.h.LL) T1 &lt;- colSums(y*K) T2 &lt;- colSums(x*dx*K) T3 &lt;- colSums(x*y*K) T4 &lt;- colSums(dx*K) B1 &lt;- colSums(K) B2 &lt;- colSums(x^2*K) B3 &lt;- colSums(x*K) m.LL &lt;- (T1*T2 - T3*T4) / (B1*B2 - B3^2) m &lt;- knn.reg(data.frame(x), y = y[-i], test = data.frame(x=x.tilde), k = best.k) m.kNN &lt;- m$pred colours &lt;- c(&quot;#2C9CDA&quot;, &quot;#811631&quot;, &quot;#E0CA1D&quot;) plot(x, y, xlim = range(x.tilde), cex = .5, xlab = &quot;eruption time [mins]&quot;, ylab = &quot;time to next eruption [mins]&quot;) lines(x.tilde, m.NW, col = colours[1]) lines(x.tilde, m.LL, col = colours[2]) lines(x.tilde, m.kNN, col = colours[3]) legend(&quot;topleft&quot;, legend = c(&quot;NW&quot;, &quot;LL&quot;, &quot;kNN&quot;), col = colours, lwd = 2) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
