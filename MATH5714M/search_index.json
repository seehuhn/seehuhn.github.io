[["index.html", "MATH5714 Linear Regression, Robustness and Smoothing Preface", " MATH5714 Linear Regression, Robustness and Smoothing Jochen Voss University of Leeds, Semester 1, 2024/25 Preface The MATH5714M module includes all the material from MATH3714 (Linear Regression and Robustness), plus additional material on “Smoothing”. From MATH3714 (and from previous modules) we know how to fit a regression line through points \\((x_1, y_1), \\ldots, (x_n, y_n) \\in\\mathbb{R}^2\\). The underlying model there is described by the equation \\[\\begin{equation*} y_i = \\alpha + \\beta x_i + \\varepsilon_i \\end{equation*}\\] for all \\(i \\in \\{1, 2, \\ldots, n\\}\\), and the aim is to find values for the intercept \\(\\alpha\\) and the slope \\(\\beta\\) such that the residuals \\(\\varepsilon_i\\) are as small as possible. This procedure, linear regression, and its extensions are discussed in the level 3 component of the module. In the level 5 component of this module, we will discuss “smoothing” which is a technique which can be used when linear models are no longer appropriate for the data. An example of such a situation is illustrated in figure 0.1. The red line in this figure is obtained using one of the smoothing techniques which we will discuss in the level 5 part of the module. Figure 0.1: An illustration of a dataset where a linear (straight line) model is not appropriate. The data represents a series of measurements of head acceleration in a simulated motorcycle accident, used to test crash helmets (the mcycle dataset from the MASS R package). "],["X01-KDE.html", "Section 1 Kernel Density Estimation 1.1 Histograms 1.2 Kernel Density Estimation 1.3 Kernel Density Estimation in R", " Section 1 Kernel Density Estimation In this section we discuss the topic of “Kernel Density Estimation”. Here we suppose we are given data \\(x_1,\\ldots, x_n \\in\\mathbb{R}^d\\) from an unknown probability density, say \\(f\\). Our objective is to estimate the density \\(f\\). This section lays the foundations for many of the following topics. 1.1 Histograms 1.1.1 Probability Densities Before we consider how to estimate a density, let us just remember what a density is. A random variable \\(X \\in \\mathbb{R}\\) has density \\(f\\colon \\mathbb{R}\\to [0, \\infty)\\) if \\[\\begin{equation*} P\\bigl(X \\in [a,b]\\bigr) = \\int_a^b f(x) \\,dx \\end{equation*}\\] for all \\(a, b\\in\\mathbb{R}\\) with \\(a &lt; b\\). Densities are sometimes also called “probability densities” or even “probability density functions”. A density \\(f\\) is large in regions where \\(X\\) is very likely to take values, and small in regions where \\(X\\) is less likely to take values. If \\(f(x) = 0\\) for all \\(x\\) in a region, that means that \\(X\\) never takes values there. Graphically, the integral \\(\\int_a^b f(x) \\,dx\\) can be interpreted as the area under the graph of \\(f\\). This is illustrated in figure 1.1. Figure 1.1: An illustration of how the area under the graph of a density can be interpreted as a probability. A function \\(f\\) is the density of some random variable \\(X\\), if and only if \\(f \\geq 0\\) and \\[\\begin{equation*} \\int_{-\\infty}^\\infty f(x) \\,dx = 1. \\end{equation*}\\] 1.1.2 Histograms In the univariate case, i.e. for \\(d = 1\\), a commonly used technique to approximate the density of a sample is to plot a histogram. To illustrate this, we use the faithful dataset built in R, which contains waiting times between eruptions and the duration of the eruption for the Old Faithful geyser in the Yellowstone National Park. (You can type help(faithful) in R to learn more about this data set.) Here we focus on the waiting times only. A simple histogram for this dataset is shown in figure 1.2. x &lt;- faithful$waiting hist(x, probability = TRUE, main = NULL, xlab = &quot;time between eruptions [mins]&quot;) Figure 1.2: This figure shows how a histogram can be used to approximate a probability density. From the plot one can see that the density of the waiting times distribution seems to be bi-modal with peaks around 53 and 80 minutes. The histograms splits the range of the data into “buckets”, and for every bucket \\([a, b]\\) it shows a bar where the height is proportional the number of samples in the bucket. I am ignoring the case that a sample may fall exactly on the boundary between two buckets here; in reality all but one buckets need to be half-open intervals, for example \\([40, 45]\\), \\((45, 50]\\), …, \\((95, 100]\\). As we have seen, the area under the graph of the density \\(f\\) over the interval \\([a, b]\\) corresponds to the probability \\(P\\bigl(X \\in [a,b]\\bigr)\\). For the histogram to approximate the density, we need to scale the height \\(h_{a,b}\\) of the bucket \\([a, b]\\) so that the area in the histogram is also close to this probability. Since we don’t know the probability \\(P\\bigl(X \\in [a,b]\\bigr)\\) exactly, we have to approximate it as \\[\\begin{equation*} P\\bigl(X \\in [a,b]\\bigr) \\approx \\frac{n_{a,b}}{n}, \\end{equation*}\\] where \\(n_{a,b}\\) is the number of samples in the bucket \\([a,b]\\), and \\(n\\) is the total number of samples. So we need \\[\\begin{align*} (b-a) \\cdot h_{a,b} &amp;= \\mbox{area of the histogram bar} \\\\ &amp;\\approx \\mbox{area of the density} \\\\ &amp;= P\\bigl(X \\in [a,b]\\bigr) \\\\ &amp;\\approx \\frac{n_{a,b}}{n}. \\end{align*}\\] and thus we choose \\[\\begin{equation*} h_{a,b} = \\frac{1}{(b - a) n} n_{a,b}. \\end{equation*}\\] As expected, the height of the bar for the bucket \\([a,b]\\) is proportional to the number \\(n_{a,b}\\) of samples in the bucket. 1.1.3 Choice of Buckets Histograms are only meaningful if the buckets are chosen well. If the buckets are too large, not much of the structure of \\(f\\) can be resolved. If the buckets are too small, the estimate \\(P\\bigl(X \\in [a,b]\\bigr) \\approx n_{a,b}/n\\) will be poor and the histogram will no longer resemble the shape of \\(f\\). This is for example demonstrated in figure 1.3. set.seed(1) x &lt;- rnorm(50) hist(x, probability = TRUE, main = NULL, breaks = seq(-2.5, 2.5, length.out = 500)) Figure 1.3: This figure demonstrates the consequences of choosing a too small bucket size. Finally, even if reasonable bucket sizes are chosen, the result can depend quite strongly on the exact locations of these buckets. To illustrate this effect, we consider a dataset about the annual amount of snow falling in Buffalo, New York for different years. Figures 1.4 and 1.5 show the same data in two different ways, allowing to come to different conclusions about the distribution. # downloaded from https://teaching.seehuhn.de/data/buffalo/ x &lt;- read.csv(&quot;data/buffalo.csv&quot;) snowfall &lt;- x$snowfall hist(snowfall, probability = TRUE, breaks = seq(24.30, 208.22, length.out = 20), main = NULL, xlab = &quot;snowfall [in]&quot;) Figure 1.4: The annual amount of snowfall in Buffalo, New York, in inches. The histogram makes it plausible that there is one main peak in the distribution. hist(snowfall, probability = TRUE, breaks = seq(22.85, 204.92, length.out = 20), main = NULL, xlab = &quot;snowfall [in]&quot;) Figure 1.5: Continued from 1.4, this histogram shows the dataset in a way that three peaks are visible. As a further illustration of the effect of bucket width, the code in figure 1.6 shows how histograms with different bucket widths can be generated in R. Here we simply specify numeric values for the break argument to hist(), which R uses as the approximate number of buckets in the plot. par(mfrow = c(2,2)) for (breaks in c(80, 40, 20, 10)) { hist(snowfall, prob = TRUE, breaks = breaks, xlim = c(25,200), ylim = c(0, 0.03), xlab = paste(&quot;breaks =&quot;, breaks), main = NULL) } Figure 1.6: This figure illustrates how the bucket size in a histogram can be controlled in R. 1.2 Kernel Density Estimation Kernel density estimation allows to estimate the density \\(f\\) for given data while avoiding some of the disadvantages of histograms. Again, we suppose that we are given data \\(x_1, \\ldots, x_n \\in \\mathbb{R}\\) and that we want to estimate the density \\(f\\). 1.2.1 Motivation Similar to the argument in the previous subsection, for \\(x\\) in a “small” interval \\([a,b]\\) we can estimate \\(f(x)\\) as \\[\\begin{equation*} f(x) \\approx \\frac{1}{b-a} \\int_a^b f(y) \\,dy = \\frac{1}{b-a} P\\bigl( X\\in [a,b] \\bigr) \\approx \\frac{1}{b-a} \\frac{n_{a,b}}{n}, \\end{equation*}\\] where \\(n_{a,b}\\) denotes the number of samples in the interval \\([a, b]\\). This equation contains two approximation. The first one, \\(f(x) \\approx 1/(ba) \\int_a^b f(y) \\,dy\\), is more accurate if the interval is small, because then \\(f\\) will be nearly constant over the interval. The second approximation will be more accurate if the interval is large, because then the interval \\([a,b]\\) covers more samples and the estimate of the probability is based on more data. We will later discuss in detail how these two concerns can be optimally balanced. A mathematical “trick” to write more clearly how \\(n_{a,b}\\) depends on the data is to write the value as \\[\\begin{equation*} n_{a,b} = \\sum_{i=1}^n I_{[a,b]}(x_i), \\end{equation*}\\] where \\[\\begin{equation*} I_{[a,b]}(x) = \\begin{cases} 1 &amp; \\mbox{if $x \\in [a,b]$, and} \\\\ 0 &amp; \\mbox{otherwise.} \\end{cases} \\end{equation*}\\] The function \\(I_{[a,b]}\\) is called the indicator function of the set \\([a, b]\\). Using the indicator function notation, the estimate for \\(f(x)\\) can be written as \\[\\begin{align*} f(x) \\approx \\frac{1}{n(b-a)} n_{a,b} = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{b-a} I_{[a,b]}(x_i) \\end{align*}\\] whenever \\(x \\in [a,b]\\) and when \\(b-a\\) is “not too large and not too small”. For symmetry we choose the interval \\([a, b]\\) centred around \\(x\\), say \\([a, b] = [x - h, x + h]\\) where \\(h\\) can be chosen to control the width of the interval. In this case we have \\(b - a = x + h - x + h = 2h\\) and thus \\[\\begin{align*} f(x) &amp;\\approx \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2h} I_{[x-h,x+h]}(x_i) \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2h} I_{[-h,+h]}(x_i-x) \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2h} I_{[-1,+1]}\\Bigl(\\frac{x_i-x}{h}\\Bigr) \\end{align*}\\] for all \\(x \\in \\mathbb{R}\\). This is an example of a kernel density estimate. The function \\(K(x) = 1/2 \\, I_{[-1,+1]}(x)\\) on the right-hand side is called the kernel of the estimate, and the parameter \\(h\\) is called the bandwidth or the smoothing parameter. 1.2.2 Definition of a Kernel Density Estimator The general kernel density estimate is a generalisation of the idea from the previous subsection. We first define the class of functions which we use to replace the function \\(1/2 \\, I_{[-1,+1]}\\). Definition 1.1 A kernel is a function \\(K\\colon \\mathbb{R}\\to \\mathbb{R}\\) such that \\(\\int_{-\\infty}^\\infty K(x) \\,dx = 1\\), \\(K(x) = K(-x)\\) (i.e. \\(K\\) is symmetric) and \\(K(x) \\geq 0\\) (i.e. \\(K\\) is positive) for all \\(x\\in \\mathbb{R}\\). Of these three properties, the first one is the most important one. The second condition, symmetry, ensures that \\(K\\) is centred around \\(0\\) and the third definition, positivity, makes \\(K\\) a probability density. (While most authors list all three properties shown above, sometimes the third condition is omitted.) It is easy to check that \\(K(x) = 1/2 \\, I_{[-1,+1]}(x)\\) satisfies all three conditions of definition 1.1. This function \\(K\\) is sometimes called the “uniform kernel”, because it is the density of the uniform distribution \\(\\mathcal{U}[-1,+1]\\). Based on the concept of a kernel, we now can define what a Kernel Density Estimate is. Definition 1.2 For a kernel \\(K\\), bandwidth \\(h &gt; 0\\) and \\(x \\in \\mathbb{R}\\), the kernel density estimate for \\(f(x)\\) is given by \\[\\begin{equation*} \\hat f_h(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i), \\end{equation*}\\] where \\(K_h\\) is given by \\[\\begin{equation*} K_h(y) = \\frac{1}{h} K(y/h) \\end{equation*}\\] for all \\(y\\in \\mathbb{R}\\). For \\(K(x) = 1/2 \\, I_{[-1,+1]}(x)\\) this definition recovers the approximation we discussed in the previous section. In later sections we will see how the kernel \\(K\\) can be chosen for the estimator \\(\\hat f\\) to have “good” properties. As a simple example we note that if \\(K\\) is continuous, then the rescaled kernel \\(K_h\\) and thus also the estimate \\(f_h\\) are continuous functions. Similar to the bucket width in histograms, the bandwidth parameter \\(h\\) controls how smooth the density estimate \\(\\hat f_h\\) is, as a function of \\(x\\). 1.2.3 Kernels There are many different kernels in use. Some examples are listed below. A more exhautive list can, for example, be found on Wikipedia. 1.2.3.1 Uniform Kernel \\[\\begin{equation*} K(x) = \\begin{cases} 1/2 &amp; \\mbox{if $-1 \\leq x \\leq 1$} \\\\ 0 &amp; \\mbox{otherwise} \\end{cases} \\end{equation*}\\] 1.2.3.2 Triangular Kernel \\[\\begin{equation*} K(x) = \\begin{cases} 1-|x| &amp; \\mbox{if $-1 \\leq x \\leq 1$} \\\\ 0 &amp; \\mbox{otherwise} \\end{cases} \\end{equation*}\\] 1.2.3.3 Epanechnikov Kernel \\[\\begin{equation*} K(x) = \\begin{cases} \\frac34 (1-x^2) &amp; \\mbox{if $-1 \\leq x \\leq 1$} \\\\ 0 &amp; \\mbox{otherwise} \\end{cases} \\end{equation*}\\] 1.2.3.4 Gaussian Kernel \\[\\begin{equation*} K(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigl(-x^2/2\\bigr) \\end{equation*}\\] 1.3 Kernel Density Estimation in R Kernel density estimates can be computed in R using the built-in density() function. If x is a vector containing the data, then density(x) computes a basic kernel density estimate, using the Gaussian kernel. The function has a number of optional arguments, which can be used to control details of the estimate: bw = ... can be used to control the bandwidth \\(h\\). If no numeric value is given, a heuristic is used. Note that for some kernels, bw differs from our \\(h\\) by a constant factor. The value bw=1 always corresponds to the case where the probability distribution with density \\(K_h\\) has variance 1. kernel = ... can be used to choose the kernel. Choices incluse \"rectangular\" (the uniform kernel), \"triangular\", \"epanechnikov\" and \"gaussian\". The default value is \"gaussian\". Details about how to call density() can be found by using the command help(density) in R. The return value of density is an R object which contains information about the kernel density estimate. m &lt;- density(snowfall) str(m) ## List of 8 ## $ x : num [1:512] -4.17 -3.72 -3.26 -2.81 -2.35 ... ## $ y : num [1:512] 4.28e-06 4.94e-06 5.68e-06 6.50e-06 7.42e-06 ... ## $ bw : num 9.72 ## $ n : int 109 ## $ old.coords: logi FALSE ## $ call : language density.default(x = snowfall) ## $ data.name : chr &quot;snowfall&quot; ## $ has.na : logi FALSE ## - attr(*, &quot;class&quot;)= chr &quot;density&quot; The fields $x and $y contain the \\(x\\) and \\(y\\) coordinates, respectively, of points on the \\(x \\mapsto \\hat f_h(x)\\) curve, which approximates \\(f\\). The field $bw shows the numeric value for the bandwidth chosen by the heuristic. The returned object can also directly be used as an argument to plot() and lines(), to add the graph of \\(\\hat f_h\\) to a plot. The following code illustrates the use density() with different bandwidth bandwidth parameter. The result is shown in figure 1.7. par(mfrow = c(2,2)) for (bw in c(1, 2, 4, 8)) { plot(density(snowfall, bw = bw, kernel = &quot;triangular&quot;, n = 1000), xlim = c(25,200), ylim = c(0, 0.025), xlab = paste(&quot;bandwidth =&quot;, bw), main = NA) } Figure 1.7: This figure illustrates how the bandwidth of a kernel density estimate can be controlled in R. Summary Histograms can be scaled so that they approximate densities. Some care is needed when choosing buckets for a histogram. Kernel density estimates can be used to estimate densities from data. A variety of different kernel functions are commonly used. "],["I01-KDE.html", "Interlude: Kernel Density Estimation in R Direct Implementation Speed Improvements", " Interlude: Kernel Density Estimation in R In the previous section we have already seen how the function density() can be used compute kernel density estimates in R. In the current section we will compare different methods to compute these estimates “by hand”. For out experiments we will use the same “snowfall” dataset as in the previous section. # downloaded from https://teaching.seehuhn.de/data/buffalo/ x &lt;- read.csv(&quot;data/buffalo.csv&quot;) snowfall &lt;- x$snowfall Direct Implementation The kernel density estimate for given data \\(x_1, \\ldots, x_n \\in\\mathbb{R}\\) is \\[\\begin{equation*} \\hat f_h(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K\\Bigl( \\frac{x - x_i}{h} \\Bigr). \\end{equation*}\\] Our aim is to implement this formula in R. To compute the estimate we need to choose a kernel function. For our experiment we use the triangular kernel from section 1.2.3.2: K &lt;- function(x) pmax(1 - abs(x), 0) The use of pmax() here allows to evaluate our newly defined function K() for several \\(x\\)-values in parallel: K(c(-1, -0.5, 0, 0.5, 1)) ## [1] 0.0 0.5 1.0 0.5 0.0 With this in place, we can now easily compute the kernel density estimate. The triangular kernel used by the function density is scaled by a factor \\(1/\\sqrt{6}\\) compared to our formula, so here we use \\(h = 4\\sqrt{6}\\) to get output comparable to the density output for bw=4 (bottom left panel in figure 1.7). h &lt;- 4 * sqrt(6) # bandwidth x &lt;- 100 # the point at which to estimate f mean(1/h * K((x - snowfall) / h)) ## [1] 0.0108237 Typically we want to evaluate this function on a grid of \\(x\\)-values, for example to plot the estimated density \\(\\hat f\\) as a function. Since \\(x_1, \\ldots, x_n\\) is used to denote the input data, we need choose a different name for the \\(x\\)-values where we evaluate \\(\\hat f_h\\). Here we use the \\(\\tilde x_1, \\ldots \\tilde x_m\\) to denote these points: x.tilde &lt;- seq(25, 200, by = 1) f.hat &lt;- numeric(length(x.tilde)) # empty vector to store the results for (j in 1:length(x.tilde)) { f.hat[j] &lt;- mean(1/h * K((x.tilde[j] - snowfall) / h)) } Plotting f.hat as a function x.tilde (figure 1.8) shows that we get a similar result as we did in figure 1.7. plot(x.tilde, f.hat, type = &quot;l&quot;, xlab = &quot;snowfall&quot;, ylab = &quot;density&quot;, ylim = c(0, 0.025)) Figure 1.8: Manually computed kernel density estimate for the snowfall data, corresponding to bw=4 in density(). We will see in the next section that the code shown above is slower than the built-in function density(), but on the other hand using our own code we can control all details of the computation and there is no uncertainty about what exactly the code does. Speed Improvements In this sub-section we will see how we can speed up our implementation of the kernel density estimate. To allow for easier speed comparisons, we encapsulate the code from above into a function: KDE1 &lt;- function(x.tilde, x, K, h) { f.hat &lt;- numeric(length(x.tilde)) for (j in 1:length(x.tilde)) { f.hat[j] &lt;- mean(1/h * K((x.tilde[j] - x) / h)) } f.hat } A common source of inefficiency in R code is the use of loops, like the for-loop in the function KDE1(). The loop in our code is used to compute the differences \\(\\tilde x_j - x_i\\) for all \\(j\\). We have already avoided a second loop by using the fact that R interprets x.tilde[j] - x a vector operation, by using pmax() in our implementation of K, and by using mean() to evaluate the sum in the formula for \\(\\hat f_h\\). To also avoid the loop over j we can use the function outer(). This function takes two vectors as inputs and then applies a function to all pairs of elements from the two vectors. The result is a matrix where the rows correspond to the elements of the first vector and the columns to the elements of the second vector. The function to apply can either be an arithmetic operation like \"+\" or \"*\", or an R-function which takes two numbers as arguments. For example, the following code computes all pairwise differences between the elements of two vectors: x &lt;- c(1, 2, 3) y &lt;- c(1, 2) outer(x, y, &quot;-&quot;) ## [,1] [,2] ## [1,] 0 -1 ## [2,] 1 0 ## [3,] 2 1 Details about how to call outer() can be found by using the command help(outer) in R. We can use outer(x.tilde, x, \"-\") to compute all the \\(\\tilde x_j - x_i\\) at once. The result is a matrix with \\(m\\) rows and \\(n\\) columns, and our implementation of the function K() can be applied to this matrix to compute the kernel values for all values at once: KDE2 &lt;- function(x.tilde, x, K, h) { dx &lt;- outer(x.tilde, x, &quot;-&quot;) rowMeans(1/h * K(dx / h)) } We will see at the end of this section that the function KDE2() is significantly faster than KDE1(), and we can easily check that both functions return the same result: KDE1(c(50, 100, 150), snowfall, K, h) ## [1] 0.0078239091 0.0108236983 0.0007448277 KDE2(c(50, 100, 150), snowfall, K, h) ## [1] 0.0078239091 0.0108236983 0.0007448277 There is a another, minor optimisation we can make. Instead of dividing the \\(m\\times n\\) matrix elements of dx by h, we can achieve the same effect by dividing the vectors x and x.tilde, i.e. only \\(m+n\\) numbers, by \\(h\\). This reduces the number of operations required. Similarly, instead of multiplying the \\(m\\times n\\) kernel values by \\(1/h\\), we can divide the \\(m\\) row means by \\(h\\): KDE3 &lt;- function(x.tilde, x, K, h) { dx &lt;- outer(x.tilde / h, x / h, &quot;-&quot;) rowMeans(K(dx)) / h } Again, the new function KDE3() returns the same result as the original implementation: KDE3(c(50, 100, 150), snowfall, K, h) ## [1] 0.0078239091 0.0108236983 0.0007448277 To verify that our modifications really made the code faster, we measure the execution times using the R package microbenchmark: library(&quot;microbenchmark&quot;) microbenchmark( KDE1 = KDE1(x.tilde, snowfall, K, h), KDE2 = KDE2(x.tilde, snowfall, K, h), KDE3 = KDE3(x.tilde, snowfall, K, h), times = 1000) ## Unit: microseconds ## expr min lq mean median uq max neval ## KDE1 1412.778 1476.9020 1566.8028 1496.9510 1521.0795 6546.880 1000 ## KDE2 198.932 261.5390 333.3281 272.1375 281.9365 6038.070 1000 ## KDE3 179.703 225.6025 301.7422 233.5360 241.7770 8802.618 1000 The output summarises the execution times for 1000 runs of each of the three functions. The columns correspond to the fastest measured time, lower quartile, mean, median, upper quartile and slowest time, respectively. (Different runs take different times, due to the fact that the computer is busy with other tasks in the background.) We can see that the introduction of outer() in KDE2() made a big difference, and that the further optimisation in KDE3() improved speed further. Finally, we compare the speed of our implementation to the built-in function density(). To get comparable output we use the arguments from, to, and n to specify the same grid \\(\\tilde x\\) as we used above. KDE.builtin &lt;- function(x.tilde, x, K, h) { density(x, kernel = K, bw = h / sqrt(6), from = min(x.tilde), to = max(x.tilde), n = length(x.tilde))$y } microbenchmark(density = KDE.builtin(x.tilde, snowfall, &quot;triangular&quot;, h), times = 1000) ## Unit: microseconds ## expr min lq mean median uq max neval ## density 160.31 170.9905 189.1218 173.7375 177.5095 4533.534 1000 The result shows that density() is faster than KDE3(), but by a factor of less than 2. The reason is that density() uses a more efficient algorithm for computing an approximation to the kernel density estimate, based on Fast Fourier Transform. By comparing the output of KDE.builtin() we can see that the approximation used by density() gives nearly the same result as our exact implementation of the method: f.hat1 &lt;- KDE3(x.tilde, snowfall, K, h) f.hat2 &lt;- KDE.builtin(x.tilde, snowfall, &quot;triangular&quot;, h) max(abs(f.hat1 - f.hat2)) ## [1] 1.935582e-05 "],["X02-Bias.html", "Section 2 The Bias of Kernel Density Estimates 2.1 A Statistical Model 2.2 The Bias of the Estimate 2.3 Moments of Kernels 2.4 The Bias for Small Bandwidth", " Section 2 The Bias of Kernel Density Estimates In the previous section we introduced the kernel density estimate \\[\\begin{equation} \\hat f_h(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i) \\tag{2.1} \\end{equation}\\] for estimating the density \\(f\\), and we argued that \\(\\hat f_h(x) \\approx f(x)\\). The aim of the current section is to quantify the error of this approximation and to understand how this error depends on the true density \\(f\\) and on the bandwidth \\(h &gt; 0\\). 2.1 A Statistical Model As usual, we will make a statistical model for the data \\(x_1, \\ldots, x_n\\), and then use this model to analyse how well the estimator performs. The statistical model we will consider here is extremely simple: we model the \\(x_i\\) using random variables \\[\\begin{equation} X_1, \\ldots, X_n \\sim f, \\tag{2.2} \\end{equation}\\] which we assume to be independent and identically distributed (i.i.d.). Here, the notation \\(X \\sim f\\), where \\(f\\) is a probability density, simply denotes that the random variable \\(X\\) has density \\(f\\). It is important to not confuse \\(x\\) (the point where we are evaluating the densities during our analysis) with the data \\(x_i\\). A statistical model describes the data, so here we get random variables \\(X_1, \\ldots, x_n\\) to describe the behaviour of \\(x_1, \\ldots, x_n\\), but it does not descibe \\(x\\). The number \\(x\\) is not part of the data, so will never be modelled by a random variable. While the model is very simple, for example it is much simpler than the model we use in the level 3 part of the module for linear regression, the associated parameter estimation problem is more challenging. The only “parameter” in this model is the function \\(f \\colon\\mathbb{R}\\to \\mathbb{R}\\) instead of just a vector of numbers. The space of all possible density functions \\(f\\) is infinite dimensional, so this is a more challenging estimation problem then the one we consider, for example, for linear regression. Since \\(f\\) is not a “parameter” in the usual sense, sometimes this problem is called a “non-parametric” estimation problem. Our estimate for the density \\(f\\) is the function \\(\\hat f_h\\colon \\mathbb{R}\\to \\mathbb{R}\\), where \\(\\hat f_h(x)\\) is given by (2.1) for every \\(x \\in\\mathbb{R}\\). 2.2 The Bias of the Estimate As ususal, the bias of our estimate is the difference between what the estimator gives on average and the truth. For our estimation problem we get \\[\\begin{equation*} \\mathop{\\mathrm{bias}}\\bigl(\\hat f_h(x)\\bigr) = \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) - f(x). \\end{equation*}\\] The expectation on the right-hand side averages over the randomness in the data, by using \\(X_1, \\ldots, X_n\\) from the model in place of the data. Substituting in the definition of \\(\\hat f_h(x)\\) from equation (2.1) we find \\[\\begin{align*} \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) &amp;= \\mathbb{E}\\Bigl( \\frac{1}{n} \\sum_{i=1}^n K_h(x - X_i) \\Bigr) \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}\\bigl( K_h(x - X_i) \\bigr) \\end{align*}\\] and since the \\(X_i\\) are identically distributed, we can replace all \\(X_i\\) with \\(X_1\\) (or any other of them) to get \\[\\begin{align*} \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) &amp;= \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}\\bigl( K_h(x - X_1) \\bigr) \\\\ &amp;= \\frac{1}{n} n \\, \\mathbb{E}\\bigl( K_h(x - X_1) \\bigr) \\\\ &amp;= \\mathbb{E}\\bigl( K_h(x - X_1) \\bigr). \\end{align*}\\] Since the model assumes \\(X_1\\) (and all the other \\(X_i\\)) to have density \\(f\\), we can write this expectation as an integral to get \\[\\begin{align*} \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) &amp;= \\int_{-\\infty}^\\infty K_h(x - y) \\, f(y) \\, dy \\\\ &amp;= \\int_{-\\infty}^\\infty f(y) \\, K_h(y - x) \\, dy \\\\ &amp;= \\int_{-\\infty}^\\infty f(z+x) \\, K_h(z) \\, dz \\end{align*}\\] where we used the symmetry of \\(K_h\\) and the substitution \\(z = y - x\\). 2.3 Moments of Kernels To understand how the bias changes as \\(h\\) varies, we will need to consider properties of \\(K\\) and \\(K_h\\) in more detail. Definition 2.1 The \\(k\\)th moment of a kernel \\(K\\), for \\(k \\in \\mathbb{N}_0 = \\{0, 1, 2, \\ldots\\}\\), is given by \\[\\begin{equation*} \\mu_k(K) = \\int_{-\\infty}^\\infty x^k K(x) \\,dx. \\end{equation*}\\] The second moment \\(\\mu_2\\) is sometimes also called the variance of the kernel \\(K\\). Using the properties of \\(K\\), we find the following results: Since \\(x^0 = 1\\) for all \\(x\\in\\mathbb{R}\\), the \\(0\\)th moment is \\(\\mu_0(K) = \\int_{-\\infty}^\\infty K(x) \\,dx = 1\\) for every kernel \\(K\\). Since \\(K\\) is symmetric, the function \\(x \\mapsto x K(x)\\) is antisymmetric and we have \\[\\begin{equation*} \\mu_1(K) = \\int_{-\\infty}^\\infty x K(x) \\,dx = 0 \\end{equation*}\\] for every kernel \\(K\\). The moments of the rescaled kernel \\(K_h\\), given by \\[\\begin{equation*} K_h(x - y) = \\frac{1}{h} K\\Bigl( \\frac{x-y}{h} \\Bigr), \\end{equation*}\\] can be computed from the moments of \\(K\\). Lemma 2.1 Let \\(K\\) be a kernel, \\(k \\in \\mathbb{N}_0\\) and \\(h &gt; 0\\). Then \\[\\begin{equation*} \\mu_k(K_h) = h^k \\mu_k(K). \\end{equation*}\\] Proof. We have \\[\\begin{align*} \\mu_k(K_h) &amp;= \\int_{-\\infty}^\\infty x^k K_h(x) \\,dx \\\\ &amp;= \\int_{-\\infty}^\\infty x^k \\frac1h K\\Bigl(\\frac{x}{h}\\Bigr) \\,dx. \\end{align*}\\] Using the substitution \\(y = x/h\\) we find \\[\\begin{align*} \\mu_k(K_h) &amp;= \\int_{-\\infty}^\\infty (hy)^k \\frac1h K(y) \\, h \\,dy \\\\ &amp;= h^k \\int_{-\\infty}^\\infty y^k K(y) \\,dy \\\\ &amp;= h^k \\mu_k(K). \\end{align*}\\] This completes the proof. It is easy to check that if \\(K\\) is a kernel, then \\(K_h\\) is also a kernel which implies that \\(K_h\\) is a probability density. If \\(Y\\) is a random variable with density \\(K_h\\), written as \\(Y \\sim K_h\\) in short, then we find \\[\\begin{equation*} \\mathbb{E}(Y) = \\int y K_h(y) \\,dy = \\mu_1(K_h) = 0 \\end{equation*}\\] and \\[\\begin{equation} \\mathop{\\mathrm{Var}}(Y) = \\mathbb{E}(Y^2) = \\int y^2 K_h(y) \\,dy = \\mu_2(K_h) = h^2 \\, \\mu_2(K). \\tag{2.3} \\end{equation}\\] Thus, \\(Y\\) is centred and the variance of \\(Y\\) is proportional to \\(h^2\\). 2.4 The Bias for Small Bandwidth Considering again the formula \\[\\begin{equation*} \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) = \\int_{-\\infty}^\\infty f(x+y) \\, K_h(y) \\, dy, \\end{equation*}\\] we see that we can interpret this integral as an expectation with respect to a random variable \\(Y \\sim K_h\\): \\[\\begin{equation} \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) = \\mathbb{E}\\bigl( f(x+Y) \\bigr). \\tag{2.4} \\end{equation}\\] Equation (2.3) shows that for \\(h \\downarrow 0\\) the random variable concentrates more and more around \\(0\\) and thus \\(x+Y\\) concentrates more and more around \\(x\\). For this reason we expect \\(\\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) \\approx f(x)\\) for small \\(h\\). To get a more qualitative version of this argument, we consider the Taylor approximation of \\(f\\) around the point \\(x\\): \\[\\begin{equation*} f(x + y) \\approx f(x) + y f&#39;(x) + \\frac{y^2}{2} f&#39;&#39;(x). \\end{equation*}\\] Substituting this into equation (2.4) we find \\[\\begin{align*} \\mathbb{E}\\bigl(\\hat f_h(x)\\bigr) &amp;\\approx \\mathbb{E}\\Bigl( f(x) + Y f&#39;(x) + \\frac{Y^2}{2} f&#39;&#39;(x) \\Bigr) \\\\ &amp;= f(x) + \\mathbb{E}(Y) f&#39;(x) + \\frac12 \\mathbb{E}(Y^2) f&#39;&#39;(x) \\\\ &amp;= f(x) + \\frac12 h^2 \\mu_2(K) f&#39;&#39;(x) \\end{align*}\\] for small \\(h\\). Considering the bias again, this gives \\[\\begin{equation} \\mathop{\\mathrm{bias}}\\bigl( \\hat f_h(x) \\bigr) = \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) - f(x) \\approx \\frac{\\mu_2(K) f&#39;&#39;(x)}{2} h^2 \\tag{2.5} \\end{equation}\\] which shows that the bias of the estimator descreases quadratically as \\(h\\) gets smaller. In contrast, we will see in the next section that the variance of the estimator increases as \\(h \\downarrow 0\\). We will need to balance these two effects to find the optimal value of \\(h\\). Summary We have introduced a statistical model for density estimation. The bias for kernel density estimation can be written as an integral. We learned how the moments of a kernel are defined. The bias for small bandwidth depends on the second moment of the kernel and the second derivative of the density. "],["X03-Var.html", "Section 3 The Variance of Kernel Density Estimates 3.1 Variance 3.2 Mean Squared Error 3.3 Optimal Bandwidth", " Section 3 The Variance of Kernel Density Estimates In the previous section we considered the bias of the estimator \\[\\begin{equation*} \\hat f_h(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i). \\end{equation*}\\] We found \\[\\begin{equation} \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) = \\mathbb{E}\\bigl( K_h(x - X_i) \\bigr) \\tag{3.1} \\end{equation}\\] for all \\(i \\in \\{1, \\ldots, n\\}\\) (we used \\(i=1\\)), and we used this relation to compute the bias. In this section, we will use similar arguments to compute the variance and the mean squared error of the estimator. 3.1 Variance We use the formula \\[\\begin{equation*} \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) = \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) - \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2 \\end{equation*}\\] and consider the two terms separately. 3.1.1 Second Moment For the second moment term in the definition of the variance we get \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) &amp;= \\mathbb{E}\\Bigl( \\frac{1}{n} \\sum_{i=1}^n K_h(x - X_i) \\frac{1}{n} \\sum_{j=1}^n K_h(x - X_j) \\Bigr) \\\\ &amp;= \\frac{1}{n^2} \\mathbb{E}\\Bigl( \\sum_{i,j=1}^n K_h(x - X_i) K_h(x - X_j) \\Bigr) \\\\ &amp;= \\frac{1}{n^2} \\sum_{i,j=1}^n \\mathbb{E}\\Bigl( K_h(x - X_i) K_h(x - X_j) \\Bigr) \\end{align*}\\] Since the \\(X_i\\) are independent, the values of \\(i\\) and \\(j\\) in this sum do not matter. For the \\(n\\) terms where \\(i=j\\) we can assume that both indices equal 1, and for the \\(n(n-1)\\) terms where \\(i\\neq j\\) we can assume \\(i=1\\) and \\(j=2\\), without changing the value of the expectation. So we get \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) &amp;= \\frac{1}{n^2} \\Bigl( n \\mathbb{E}\\bigl( K_h(x - X_1)^2 ) + n(n-1) \\mathbb{E}\\bigl( K_h(x - X_1) K_h(x - X_2) \\bigr) \\Bigr) \\\\ &amp;= \\frac{1}{n^2} \\Bigl( n \\mathbb{E}\\bigl( K_h(x - X_1)^2 ) + n(n-1) \\mathbb{E}\\bigl( K_h(x - X_1) \\bigr) \\mathbb{E}\\bigl( K_h(x - X_2) \\bigr) \\Bigr) \\\\ &amp;= \\frac{1}{n^2} \\Bigl( n \\mathbb{E}\\bigl( K_h(x - X_1)^2 ) + n(n-1) \\mathbb{E}\\bigl( K_h(x - X_1) \\bigr)^2 \\Bigr) \\\\ &amp;= \\frac{1}{n} \\mathbb{E}\\Bigl( K_h(x - X_1)^2 \\Bigr) + \\frac{n-1}{n} \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2, \\end{align*}\\] where we used equation (3.1) for the last term. Using this equation, we can write the expectation as \\[\\begin{align*} \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) &amp;= \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) - \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2 \\\\ &amp;= \\frac{1}{n} \\mathbb{E}\\bigl( K_h(x - X_1)^2 ) + \\Bigl(\\frac{n-1}{n} - 1\\Bigr) \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2. \\end{align*}\\] Since \\((n-1)/n - 1 = -1/n\\) we get \\[\\begin{equation} \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) = \\frac{1}{n} \\Bigl( \\mathbb{E}\\bigl( K_h(x - X_1)^2 ) - \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2 \\Bigr). \\tag{3.2} \\end{equation}\\] 3.1.2 The Roughness of a Kernel Similar to what we did in the last section, we will use Taylor expansion of \\(f\\) around the point \\(x\\) to understand the behaviour of the variance for small \\(h\\). Some more care is needed here, because this time the result also depends on the sample size \\(n\\) and we will consider the joint limit of \\(n \\to \\infty\\) and \\(h\\to 0\\). For the first term in equation (3.2) we find \\[\\begin{align*} \\mathbb{E}\\bigl( K_h(x - X_1)^2 ) &amp;= \\int K_h(x - y)^2 f(y) \\,dy \\\\ &amp;= \\int K_h(y - x)^2 f(y) \\,dy \\\\ &amp;= \\int \\frac{1}{h^2} K\\Bigl( \\frac{y - x}{h} \\Bigr)^2 f(y) \\,dy. \\end{align*}\\] Now we use the substitution \\(z = (y - x) / h\\). This gives \\[\\begin{align*} \\mathbb{E}\\bigl( K_h(x - X_1)^2 ) &amp;= \\int \\frac{1}{h^2} K(z)^2 f(x + hz) \\,h \\,dz \\end{align*}\\] Finally, we use Taylor approximation to get \\[\\begin{align*} \\mathbb{E}\\bigl( K_h(x - X_1)^2 ) &amp;\\approx \\int \\frac{1}{h} K(z)^2 \\Bigl( f(x) + hz\\,f&#39;(x) + \\frac12 h^2 z^2 \\,f&#39;&#39;(x) \\Bigr) \\,dz \\\\ &amp;= \\frac{1}{h} f(x) \\int K(z)^2 \\,dz + f&#39;(x) \\int z K(z)^2 \\,dz + \\frac12 h f&#39;&#39;(x) \\int z^2 K(z)^2 \\,dz \\\\ &amp;= \\frac{1}{h} f(x) \\int K(z)^2 \\,dz + \\frac12 h f&#39;&#39;(x) \\int z^2 K(z)^2 \\,dz \\end{align*}\\] where the first-order term disappears since \\(z \\mapsto z K(z)^2\\) is an asymmetric function. As a shorthand we use the following definition. Definition 3.1 The roughness of a kernel \\(K\\) is given by \\[\\begin{equation*} R(K) := \\int_{-\\infty}^\\infty K(x)^2 \\,dx. \\end{equation*}\\] This gives the result \\[\\begin{equation} \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\bigr) \\approx \\frac{1}{h} f(x) R(K) + \\frac12 h f&#39;&#39;(x) \\int z^2 K(z)^2 \\,dz \\tag{3.3} \\end{equation}\\] for small \\(h\\). 3.1.3 The Variance for Small Bandwidth Here we consider the term \\(\\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2\\) in the formula for the variance. From the previous section we know \\[\\begin{equation*} \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) \\approx f(x) + \\frac12 h^2 \\mu_2(K) f&#39;&#39;(x) \\end{equation*}\\] and thus we get \\[\\begin{equation} \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr)^2 \\approx f(x)^2 + h^2 \\mu_2(K) f(x) f&#39;&#39;(x) + \\frac14 h^4 \\mu_2(K)^2 f&#39;&#39;(x)^2 \\tag{3.4} \\end{equation}\\] for small \\(h\\). Substituting (3.3) and (3.4) into equation (3.2) we finally find \\[\\begin{equation*} \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) = \\frac{1}{n} \\Bigl( \\frac{1}{h} f(x) R(K) - f(x)^2 + \\cdots \\Bigr), \\end{equation*}\\] where all the omitted terms go to zero as \\(h \\downarrow 0\\). Omitting one more term and keeping only the leading term we find \\[\\begin{equation} \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) \\approx \\frac{1}{nh} f(x) R(K) \\tag{3.5} \\end{equation}\\] as \\(h\\downarrow 0\\). 3.2 Mean Squared Error The Mean Squared Error of the estimator \\(\\hat f_h(x)\\) for \\(f(x)\\) is given by \\[\\begin{equation*} \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) = \\mathbb{E}\\Bigl( \\bigl( \\hat f_h(x) - f(x) \\bigr)^2 \\Bigr). \\end{equation*}\\] It is an easy exercise to show that this can equivalently be written as \\[\\begin{equation*} \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) = \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) + \\mathop{\\mathrm{bias}}\\bigl( \\hat f_h(x) \\bigr)^2. \\end{equation*}\\] Substituing equations (2.5) and (3.5) into the formula for the MSE, we get \\[\\begin{equation} \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) \\approx \\frac{1}{nh} f(x) R(K) + \\frac14 \\mu_2(K)^2 f&#39;&#39;(x)^2 h^4. \\tag{3.6} \\end{equation}\\] Some care is needed to make sure that the omitted terms from the Taylor approximations really don’t make a significant contribution in this formula for the MSE: The additional contributions from the variance have the form \\(e_1(h) / n\\), where the error term \\(e_1\\) does not depend on \\(n\\) and is negligible compared to \\(f(x) R(K) / h\\) as \\(h \\downarrow 0\\). Using little-o notation, This is sometimes denoted by \\(e_1(h) = o(1/h)\\), which indicates that \\(e_1(h) / (1/h) \\to 0\\) as \\(h \\downarrow 0\\). The additional terms from the squared bias, say \\(e_2(h)\\), do not depend on \\(n\\) and are negligible compared to \\(\\mu_2(K)^2 f&#39;&#39;(x)^2 h^4\\). We can write \\(e_2(h) = o(h^4)\\) as \\(n \\downarrow 0\\), to reflect this fact. We can summarise these results as \\[\\begin{equation*} \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) = \\frac{1}{nh} f(x) R(K) + \\frac14 \\mu_2(K)^2 f&#39;&#39;(x)^2 h^4 + o(1/nh) + o(h^4) \\end{equation*}\\] as \\(h \\downarrow 0\\), with the understanding that the constants in the definition of \\(o(h^4)\\) do not depend on \\(n\\) and that \\(o(1/nh)\\) really means “\\(o(1/h)\\), where the constants are proportional to \\(1/n\\).” 3.3 Optimal Bandwidth The two terms on the right-hand side of (3.6) are balanced in that the first term decreases for large \\(h\\) while the second term decreases for small \\(h\\). By taking derivatives with respect to \\(h\\), we can find the optimal value of \\(h\\). Ignoring the higher order terms, we get \\[\\begin{equation*} \\frac{\\partial}{\\partial h} \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) = -\\frac{1}{nh^2} f(x) R(K) + \\mu_2(K)^2 f&#39;&#39;(x)^2 h^3 \\end{equation*}\\] and thus the derivative equals zero, if \\[\\begin{equation*} \\frac{1}{nh^2} f(x) R(K) = \\mu_2(K)^2 f&#39;&#39;(x)^2 h^3 \\end{equation*}\\] or, equivalently, \\[\\begin{equation*} h = h_\\mathrm{opt} := \\Bigl( \\frac{f(x) R(K)}{n \\mu_2(K)^2 f&#39;&#39;(x)^2} \\Bigr)^{1/5}. \\end{equation*}\\] It is easy to check that this \\(h\\) corresponds to the minimum of the MSE. This shows how the optimal bandwidth depends both on the kernel and on the target density \\(f\\). In practice, this formula is hard to use, since \\(f&#39;&#39;\\) is unknown. (We don’t even know \\(f\\)!) Substituting the optimal value of \\(h\\) back into equation (3.6), we get \\[\\begin{align*} \\mathop{\\mathrm{MSE}}\\nolimits_\\mathrm{opt} &amp;= \\frac{1}{n} f(x) R(K) \\Bigl( \\frac{n \\mu_2(K)^2 f&#39;&#39;(x)^2}{f(x) R(K)} \\Bigr)^{1/5} + \\frac14 \\mu_2(K)^2 f&#39;&#39;(x)^2 \\Bigl( \\frac{f(x) R(K)}{n \\mu_2(K)^2 f&#39;&#39;(x)^2} \\Bigr)^{4/5} \\\\ &amp;= \\frac54 \\; \\frac{1}{n^{4/5}} \\; \\Bigl( R(K)^2 \\mu_2(K) \\Bigr)^{2/5} \\; \\Bigl( f(x)^2 |f&#39;&#39;(x)| \\Bigr)^{2/5}. \\end{align*}\\] This expression clearly shows the contribution of \\(n\\), \\(K\\) and \\(f\\): If the bandwidth is chosen optimally, as \\(n\\) increases the bandwidth \\(h\\) decreases proportionally to \\(1/n^{1/5}\\) and the MSE decreases proportionally to \\(1 / n^{4/5}\\). For comparison, in a Monte Carlo estimate for an expectation, the MSE decreases proportionally to \\(1/n\\). The error in kernel density estimation decreases slightly slower than for Monte Carlo estimates. The error is proportional to \\(\\bigl( R(K)^2 \\mu_2(K) \\bigr)^{2/5}\\). Thus we should use kernels where the value \\(R(K)^2 \\mu_2(K)\\) is small. The error is proportional to \\(f(x)^2 |f&#39;&#39;(x)|\\). We cannot influence this term, but we can see that \\(x\\) where \\(f\\) is large or has high curvature have higher estimation error. Summary We found the variance of the kernel density estimate. We studied the mean squared error for small \\(h\\). We derived a formula for the optimal value of the bandwidth \\(h\\). "],["X04-practice.html", "Section 4 Kernel Density Estimation in Practice 4.1 Integrated Error 4.2 Choice of Kernel 4.3 Bandwidth Selection 4.4 Higher Dimensions", " Section 4 Kernel Density Estimation in Practice In this section we conclude our discussion of kernel density estimation by considering different aspects which are important when using the method in practice. 4.1 Integrated Error From equation (3.6) we know \\[\\begin{equation*} \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) \\approx \\frac{1}{nh} f(x) R(K) + \\frac14 \\mu_2(K)^2 f&#39;&#39;(x)^2 h^4. \\end{equation*}\\] This gives the mean squared error when trying to estimate the density \\(f(x)\\) at a fixed point \\(x\\). Usually we are interested in estimating the function \\(f\\) rather than individual points \\(f(x)\\). In this case, we consider the integrated mean squared error (IMSE): \\[\\begin{equation*} \\mathrm{IMSE}\\bigl( \\hat f_h \\bigr) := \\int_{-\\infty}^\\infty \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) \\,dx. \\end{equation*}\\] Using our result from above we find \\[\\begin{align*} \\mathrm{IMSE}\\bigl( \\hat f_h \\bigr) &amp;\\approx \\int \\Bigl( \\frac{1}{nh} f(x) R(K) + \\frac14 \\mu_2(K)^2 f&#39;&#39;(x)^2 h^4 \\Bigr) \\,dx \\\\ &amp;= \\frac{1}{nh} R(K) \\int f(x) \\,dx + \\frac{h^4}{4} \\mu_2(K)^2 \\int f&#39;&#39;(x)^2 \\,dx \\\\ &amp;= \\frac{1}{nh} R(K) + \\frac{1}{4} \\mu_2(K)^2 R(f&#39;&#39;) h^4, \\end{align*}\\] where we (mis-)use the definition of roughness as an abbreviation to express the integral over \\(f&#39;&#39;\\). As before, we can use differentiation to find the optimal value of \\(h\\). Here we get \\[\\begin{equation*} h_\\mathrm{opt} = \\Bigl( \\frac{R(K)}{n \\mu_2(K)^2 R(f&#39;&#39;)} \\Bigr)^{1/5}. \\end{equation*}\\] and the corresponding error is \\[\\begin{equation} \\mathrm{IMSE}_\\mathrm{opt} = \\frac54 \\; \\frac{1}{n^{4/5}} \\; \\Bigl( R(K)^2 \\mu_2(K) \\Bigr)^{2/5} \\; R(f&#39;&#39;)^{1/5}. \\tag{4.1} \\end{equation}\\] Thus, in order to minimise the error we still need to choose \\(h \\propto n^{-1/5}\\) and we should choose a kernel \\(K\\) which minimises the value \\(R(K)^2 \\mu_2(K)\\). 4.2 Choice of Kernel The integrated error in equation (4.1) is proportional to \\(\\bigl( R(K)^2 \\mu_2(K) \\bigr)^{2/5}\\), and none of the remaining terms in the equation depends on the choice of the kernel. Thus, we can minimise the error by choosing a kernel which has minimal \\(R(K)^2 \\mu_2(K)\\). For a given kernel, it is easy to work out the value of \\(R(K)^2 \\mu_2(K)\\). Example 4.1 For the uniform kernel we have \\[\\begin{equation*} K(x) = \\begin{cases} 1/2 &amp; \\mbox{if $-1 \\leq x \\leq 1$} \\\\ 0 &amp; \\mbox{otherwise.} \\end{cases} \\end{equation*}\\] From this we find \\[\\begin{equation*} R(K) = \\int_{-\\infty}^\\infty K(x)^2 \\,dx = \\int_{-1}^1 \\frac14 \\,dx = \\frac12 \\end{equation*}\\] and \\[\\begin{equation*} \\mu_2(K) = \\int_{-\\infty}^\\infty x^2 K(x) \\,dx = \\int_{-1}^1 \\frac12 x^2 \\,dx = \\frac16 \\Bigl. x^3 \\Bigr|_{x=-1}^1 = \\frac16 \\bigl( 1 - (-1) \\bigr) = \\frac13. \\end{equation*}\\] Thus, for the uniform kernel we have \\[\\begin{equation*} R(K)^2 \\mu_2(K) = \\Bigl( \\frac12 \\Bigr)^2 \\frac13 = \\frac{1}{12} \\approx 0.083333. \\end{equation*}\\] Calculations similar to the ones in the example give the following values: kernel \\(\\mu_2(K)\\) \\(R(K)\\) \\(R(K)^2 \\mu_2(K)\\) Uniform \\(\\displaystyle\\frac13\\) \\(\\displaystyle\\frac12\\) \\(0.08333\\) Triangular \\(\\displaystyle\\frac16\\) \\(\\displaystyle\\frac23\\) \\(0.07407\\) Epanechnikov \\(\\displaystyle\\frac15\\) \\(\\displaystyle\\frac35\\) \\(0.07200\\) Gaussian \\(1\\) \\(\\displaystyle\\frac{1}{2\\sqrt{\\pi}}\\) \\(0.07958\\) The best value in the table is obtained for the Epanechnikov kernel, with \\(R(K)^2 \\mu_2(K) = 9/125 = 0.072\\). One can show that this value is indeed optimal amongst all kernels. Since the difference in error for the kernels listed above is only a few percent, any of these kernels would be a reasonable choice. 4.3 Bandwidth Selection Our formulas for the optimal bandwidth contain the terms \\(f(x)^2 |f&#39;&#39;(x)|\\) for fixed \\(x\\) and \\(R(f&#39;&#39;)\\) for the integrated error. Since \\(f\\) is unknown, neither of these quantities are available and instead different rules of thumb are used in the literature. Here we present one possible choice of bandwidth estimator. Suppose that \\(f\\) is a normal density, with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then we have \\[\\begin{equation*} f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\bigl( - (x-\\mu)^2 / 2\\sigma^2 \\bigr). \\end{equation*}\\] Taking derivatives we get \\[\\begin{equation*} f&#39;(x) = - \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\frac{x-\\mu}{\\sigma^2} \\exp\\bigl( - (x-\\mu)^2 / 2\\sigma^2 \\bigr) \\end{equation*}\\] and \\[\\begin{equation*} f&#39;&#39;(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\Bigl( \\frac{(x-\\mu)^2}{\\sigma^4} - \\frac{1}{\\sigma^2} \\Bigr) \\exp\\bigl( - (x-\\mu)^2 / 2\\sigma^2 \\bigr) \\end{equation*}\\] Patiently integrating the square of this function gives \\[\\begin{equation*} R(f&#39;&#39;) = \\int_{-\\infty}^\\infty f&#39;&#39;(x)^2 \\,dx = \\cdots = \\frac{3}{8\\sigma^5\\sqrt{\\pi}}. \\end{equation*}\\] This can be used as a simple “plug-in rule” with \\(\\sigma\\) estimated by the sample standard deviation. We now demonstrate how this rule of thumb could be used in R to obtain a kernel density estimate for the snowfall data. We will use the Epanechnikov kernel. For compatibility with the kernels built into R, we rescale this kernel, so that \\(\\mu_2(K) = 1\\), i.e. we consider \\(K_{\\sqrt{5}}\\) in place of \\(K\\). An easy calculation shows that the roughness is then \\(R(K) = 3 / (5*\\sqrt(5))\\). # downloaded from https://teaching.seehuhn.de/data/buffalo/ x &lt;- read.csv(&quot;data/buffalo.csv&quot;) snowfall &lt;- x$snowfall n &lt;- length(snowfall) # Roughness of the Epanechnikov kernel, after rescaling with h = sqrt(5) # so that the second moment becomes mu_2 = 1: R.K &lt;- 3 / (5 * sqrt(5)) # Rule of thumb: R.fpp &lt;- 3 / (8 * sd(snowfall)^5 * sqrt(pi)) # formula for the optimal h my.bw &lt;- (R.K / (n * 1^2 * R.fpp))^0.2 my.bw ## [1] 11.58548 R has a variety of different builtin methods to estimate bandwidths. See stats/bandwidth for a description. For comparison to our result, we list here the bandwidths suggested by some of R’s algorithms: data.frame( name = c(&quot;nrd0&quot;, &quot;nrd&quot;, &quot;SJ&quot;), bw = c(bw.nrd0(snowfall), bw.nrd(snowfall), bw.SJ(snowfall))) ## name bw ## 1 nrd0 9.724206 ## 2 nrd 11.452953 ## 3 SJ 11.903840 All of these value seem close the value we obtained manually. Using our bandwidth estimate, we get the following estimated density. plot(density(snowfall, bw = my.bw, kernel = &quot;epanechnikov&quot;), main = NA) In practice one would just use one of the built-in methods, for example using bw=\"SJ\" instead of estimating the bandwidth manually. 4.4 Higher Dimensions So far we have only considered the one-dimensional case, where the samples \\(x_i\\) are real numbers. In this subsection we will sketch how these methods will need to be adjusted for the multivariate case of \\(x_i = (x_{i,1}, \\ldots, x_{i,p}) \\in \\mathbb{R}^p\\). In this setup, a kernel is a function \\(K\\colon \\mathbb{R}^p\\to \\mathbb{R}\\) such that \\(\\int \\cdots \\int K(x) \\,dx_p \\cdots dx_1 = 1\\), \\(K(x) = K(-x)\\) and \\(K(x) \\geq 0\\) for all \\(x\\in \\mathbb{R}\\), where the integral in the first condition is now over all \\(p\\) coordinates. Example 4.2 If \\(K_1, \\ldots, K_p\\) are one-dimensional kernels, then the product \\[\\begin{equation*} K(x_1, \\ldots, x_p) := K_1(x_1) \\cdots K_p(x_p) \\end{equation*}\\] is a kernel in \\(p\\) dimensions. If we use the product of \\(p\\) Gaussian kernels, we get \\[\\begin{align*} K(x) &amp;= \\prod_{i=1}^p \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigl(-x_i^2/2\\bigr) \\\\ &amp;= \\frac{1}{(2\\pi)^{p/2}} \\exp\\Bigl(-\\frac12 (x_1^2 + \\cdots + x_p^2) \\Bigr). \\end{align*}\\] There are different possibilities for rescaling these kernels: If all coordinates live on “comparable scales” (e.g., if they are measured in the same units), the formula \\[ K_h(x) = \\frac{1}{h^p} K(x/h) \\] for all \\(x\\in\\mathbb{R}^p\\) can be used, where \\(h&gt;0\\) is a bandwidth parameter as before. The scaling by \\(1/h^p\\) is required to ensure that the integral of \\(K_h\\) equals \\(1\\), so that \\(K_h\\) is a kernel again. If different scaling is desirable for different components, the formula \\[\\begin{equation*} K_h(x) = \\frac{1}{h_1 \\cdots h_p} K(x_1/h_1, \\ldots, x_p/h_p) \\end{equation*}\\] for all \\(x\\in\\mathbb{R}^p\\) can be used, where \\(h = (h_1, \\ldots, h_p)\\) is a vector of bandwidth parameters. A more general version would be to use a symmetric, positive definite bandwidth matrix \\(H \\in \\mathbb{R}^{p\\times p}\\). In this case the required scaling is \\[\\begin{equation*} K_H(x) = \\frac{1}{\\mathrm{det}(H)} K\\bigl( H^{-1} x \\bigr) \\end{equation*}\\] for all \\(x\\in\\mathbb{R}^p\\). For all of these choices, the kernel density estimator is given by \\[\\begin{equation*} \\hat f_h(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i) \\end{equation*}\\] (using \\(K_H\\) for the third option) for all \\(x\\in\\mathbb{R}^p\\). Bandwidth selection in the multivariate case is a difficult problem and we will not discuss this here. "],["P01.html", "Problem Sheet 1", " Problem Sheet 1 .fold-btn { float: right; margin: -12px 0 0 0; } This problem sheet is for self-study only. It is not assessed. 1. Consider the following function: \\[\\begin{equation*} K(x) = \\begin{cases} \\frac23 (1 - |x|^3) &amp; \\mbox{if $|x|\\leq 1$, and} \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\end{equation*}\\] Show that this function integrates to 1 over its domain. We have \\[\\begin{align*} \\int_{-\\infty}^{\\infty} K(x) , dx &amp;= \\frac23 \\int_{-1}^1 (1 - |x|^3) , dx \\\\ &amp;= \\frac43 \\int_0^1 (1 - x^3) , dx \\\\ &amp;= \\frac43 \\left[ x - \\frac{x^4}{4} \\right]_0^1 \\\\ &amp;= \\frac43 \\left( 1 - \\frac{1}{4} \\right) \\\\ &amp;= 1. \\end{align*}\\] Show that \\(K\\) satisfies the conditions of a kernel. We have already seen that \\(K\\) integrates to 1 over its domain. Since \\(|x| = |-x|\\) for all \\(x\\in\\mathbb{R}\\), the function \\(K\\) is symmetric. Finally, since \\(|x|^3 \\leq 1\\) for all \\(x\\in\\mathbb{R}\\) with \\(|x|\\leq 1\\), we have \\(K(x) \\geq 0\\) for all \\(x\\in\\mathbb{R}\\). Compute the moments \\(\\mu_0(K)\\), \\(\\mu_1(K)\\) and \\(\\mu_2(K)\\) of \\(K\\). The \\(k\\)th moment of \\(K\\) is given by \\[\\begin{equation*} \\mu_k(K) = \\int_{-\\infty}^\\infty x^k K(x) \\,dx = \\frac23 \\int_{-1}^1 x^k \\bigl( 1 - |x|^3 \\bigr) \\,dx. \\end{equation*}\\] For \\(k = 0\\), we know \\(\\mu_0(K) = 1\\), from part a. For \\(k = 1\\), we have \\(\\mu_1(K) = 0\\), since \\(K\\) is symmetric. For \\(k = 2\\), we find \\[\\begin{align*} \\mu_2(K) &amp;= \\frac23 \\int_{-1}^1 x^2 (1 - |x|^3) \\,dx \\\\ &amp;= \\frac43 \\int_0^1 x^2 (1 - x^3) \\,dx \\\\ &amp;= \\frac43 \\left[ \\frac{x^3}{3} - \\frac{x^6}{6} \\right]_0^1 \\\\ &amp;= \\frac43 \\left( \\frac13 - \\frac16 \\right) \\\\ &amp;= \\frac29. \\end{align*}\\] 2. Consider a normal density with mean μ and variance σ², given by: \\[\\begin{equation*} f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right). \\end{equation*}\\] Calculate f’(x) and f’’(x) for this density. Using the chain rule: \\[\\begin{align*} f&#39;(x) &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\cdot \\left(-\\frac{x-\\mu}{\\sigma^2}\\right) \\\\ &amp;= -f(x)\\cdot\\frac{x-\\mu}{\\sigma^2} \\end{align*}\\] Taking derivatives again, using the product rule: \\[\\begin{align*} f&#39;&#39;(x) &amp;= -f&#39;(x)\\cdot\\frac{x-\\mu}{\\sigma^2} - f(x)\\cdot\\frac{1}{\\sigma^2} \\\\ &amp;= f(x)\\cdot\\frac{(x-\\mu)^2}{\\sigma^4} - f(x)\\cdot\\frac{1}{\\sigma^2} \\\\ &amp;= f(x)\\cdot\\left(\\frac{(x-\\mu)^2}{\\sigma^4} - \\frac{1}{\\sigma^2}\\right) \\end{align*}\\] Using the formula \\[\\begin{equation*} \\mathop{\\mathrm{bias}}(\\hat f_h(x)) \\approx \\frac{\\mu_2(K)}{2} f&#39;&#39;(x) h^2, \\end{equation*}\\] show that for fixed \\(K\\) and \\(h\\), the bias satisfies the following proportionality: \\[\\begin{equation*} \\mathop{\\mathrm{bias}}(\\hat f_h(x)) \\propto \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\left(\\frac{(x-\\mu)^2}{\\sigma^4} - \\frac{1}{\\sigma^2}\\right). \\end{equation*}\\] Substituting our expression for \\(f&#39;&#39;(x)\\) into the bias formula we get \\[\\begin{align*} \\mathop{\\mathrm{bias}}(\\hat f_h(x)) &amp;\\approx \\frac{\\mu_2(K)}{2} f&#39;&#39;(x) h^2 \\\\ &amp;= \\frac{\\mu_2(K)}{2} \\cdot f(x)\\cdot\\left(\\frac{(x-\\mu)^2}{\\sigma^4} - \\frac{1}{\\sigma^2}\\right) h^2 \\\\ &amp;= \\frac{\\mu_2(K)h^2}{2\\sqrt{2\\pi\\sigma^2}} \\cdot \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\left(\\frac{(x-\\mu)^2}{\\sigma^4} - \\frac{1}{\\sigma^2}\\right). \\end{align*}\\] For which values of \\(x\\) is the bias positive, negative, or zero? Why does this make sense intuitively? The exponential term is always positive, so the sign depends on the sign of the term \\((x-\\mu)^2 / \\sigma^4 - 1/\\sigma^2\\). This equals zero when \\((x-\\mu)^2 = \\sigma^2\\), or when \\(x = \\mu \\pm \\sigma\\). The bias is negative when \\(|x-\\mu| &lt; \\sigma\\), is positive when \\(|x-\\mu| &gt; \\sigma\\). This makes sense because the positive bandwidth \\(h\\) smoothes the density, so the bias is negative when the density is concave (near the maxiumum) and positive when it is convex (towards the tails). 3. Consider the variance of a kernel density estimate \\(\\hat f_h(x)\\) based on a sample \\(X_1, \\ldots, X_n\\) with common density \\(f\\): \\[\\begin{equation*} \\mathop{\\mathrm{Var}}(\\hat f_h(x)) = \\frac{1}{n^2} \\sum_{i,j=1}^n \\mathbb{E}\\Bigl( K_h(x - X_i)K_h(x - X_j) \\Bigr) \\end{equation*}\\] Let \\(n=3\\). Write out all terms in the sum explicitly and identify which terms involve the only one random variable \\(X_i\\) and which involve more than one random variable. For \\(n=3\\), expanding the double sum gives 9 terms: Terms with same random variable (\\(i=j\\)): \\[\\begin{align*} &amp; \\mathbb{E}(K_h(x - X_1)K_h(x - X_1)) \\\\ &amp; \\mathbb{E}(K_h(x - X_2)K_h(x - X_2)) \\\\ &amp; \\mathbb{E}(K_h(x - X_3)K_h(x - X_3)) \\end{align*}\\] Terms with different random variables (\\(i\\neq j\\)): \\[\\begin{align*} &amp; \\mathbb{E}(K_h(x - X_1)K_h(x - X_2)) \\\\ &amp; \\mathbb{E}(K_h(x - X_1)K_h(x - X_3)) \\\\ &amp; \\mathbb{E}(K_h(x - X_2)K_h(x - X_1)) \\\\ &amp; \\mathbb{E}(K_h(x - X_2)K_h(x - X_3)) \\\\ &amp; \\mathbb{E}(K_h(x - X_3)K_h(x - X_1)) \\\\ &amp; \\mathbb{E}(K_h(x - X_3)K_h(x - X_2)) \\end{align*}\\] For general \\(n\\), how many terms in the sum have \\(i=j\\) and how many have \\(i\\neq j\\)? When \\(i=j\\), we are choosing the same index from \\(\\{1,\\ldots,n\\}\\) once, giving \\(n\\) terms. When \\(i\\neq j\\), we are choosing two different indices from \\(\\{1,\\ldots,n\\}\\), giving \\(n(n-1)\\) terms. The total number of terms is thus \\(n + n(n-1) = n^2\\), as expected. Using these counts and the independence of the \\(X_i\\), show that: \\[\\begin{equation*} \\mathop{\\mathrm{Var}}(\\hat f_h(x)) = \\frac{1}{n} \\mathbb{E}\\Bigl( K_h(x - X_1)^2 \\Bigr) + \\frac{n-1}{n} \\mathbb{E}\\Bigl( \\hat f_h(x) \\Bigr)^2. \\end{equation*}\\] Using part b, we can split the sum into two parts: \\[\\begin{align*} \\mathop{\\mathrm{Var}}(\\hat f_h(x)) &amp;= \\frac{1}{n^2} \\Bigl( n \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\bigr) + n(n-1) \\mathbb{E}\\bigl( K_h(x - X_1)K_h(x - X_2) \\bigr) \\Bigr) \\\\ &amp;= \\frac{1}{n} \\mathbb{E}\\bigl( K_h(x - X_1)^2 \\bigr) + \\frac{n-1}{n} \\mathbb{E}\\bigl( K_h(x - X_1) \\bigr) \\, \\mathbb{E}\\bigl( K_h(x - X_2) \\bigr), \\end{align*}\\] where we used the independence of the \\(X_i\\) in the last step. Since \\(\\mathbb{E}\\bigl( K_h(x - X_1) \\bigr) = \\mathbb{E}\\bigl( K_h(x - X_2) \\bigr) = \\mathbb{E}(\\hat f_h(x))\\), this gives the required result. 4. Consider the following data: x &lt;- c(89.6, 82.5, 70.9, 83.8, 92.4, 86.5, 77.3, 89.2, 93.1, 84.7, 78.5, 88.3, 85.6, 90.4, 76.8) Determine the sample standard deviation of these data. sigma &lt;- sd(x) sigma ## [1] 6.410126 The R ouputs shows that the standard deviation is \\(6.410126\\). Using the “plug-in rule” from section 4.3, what bandwidth would you choose for a kernel density estimate of these data using the triangular kernel? The plug-in rule assumes that the density is normal, to get \\[\\begin{equation*} R(f&#39;&#39;) = \\frac{3}{8\\sigma^5\\sqrt{\\pi}}. \\end{equation*}\\] Substituting the sample standard deviation into this formula gives R.fpp &lt;- 3 / (8 * sigma^5 * sqrt(pi)) R.fpp ## [1] 1.954895e-05 We also need the sample size, and the roughness and second moment of the kernel: n &lt;- length(x) R.K &lt;- 2/3 mu2.K &lt;- 1/6 Using these quantities and the formula from section 4.1 gives the result: h &lt;- (R.K / (n * mu2.K^2 * R.fpp))^(1/5) h ## [1] 9.607254 "],["X05-smoothing.html", "Section 5 Kernel Smoothing 5.1 The Nadaraya-Watson Estimator 5.2 Estimation Error", " Section 5 Kernel Smoothing We now consider the statistical model \\[\\begin{equation*} Y_i = m(x_i) + \\varepsilon_i, \\end{equation*}\\] where \\(m\\colon \\mathbb{R}\\to \\mathbb{R}\\) is a smooth function and \\(\\varepsilon_i\\) are independent random variables with \\(\\mathbb{E}(\\varepsilon_i) = 0\\). We are given data \\((x_i, y_i)\\) for \\(i\\in \\{1, \\ldots, n\\}\\) and our aim is to estimate the function \\(m\\). The task of estimating the function \\(m\\) from data is called smoothing. 5.1 The Nadaraya-Watson Estimator Since we have \\[\\begin{equation*} \\mathbb{E}(Y_i) = \\mathbb{E}\\bigl( m(x_i) + \\varepsilon_i \\bigr) = m(x_i) + \\mathbb{E}( \\varepsilon_i ) = m(x_i), \\end{equation*}\\] we could attempt to use a Monte-Carlo approach where we estimate the expectation \\(\\mathbb{E}(Y_i)\\) using an average of many \\(Y\\) values. This approach is not feasible in practice, since typically we will only have a single observation \\(y_i\\) corresponding to a given \\(x_i\\). The idea of the Nadaraya-Watson Estimator is to average the \\(y_i\\) corresponding to nearby \\(x_i\\) instead. A weighted average is used, which gives less weight to further away values. This leads to the following definition. Definition 5.1 The Nadaraya-Watson Estimator for \\(m(x)\\) is given by \\[\\begin{equation*} \\hat m_h(x) = \\frac{\\sum_{i=1}^n K_h(x - x_i) y_i}{\\sum_{j=1}^n K_h(x - x_j)}, \\end{equation*}\\] where \\(K_h\\) is a kernel scaled to bandwidth \\(h\\) as in definition 1.2. The problem of finding \\(m\\) using kernel functions are called kernel smoothing or kernel regression. In this context, the bandwidth \\(h\\) is also called the smoothing parameter. The Nadaraya-Watson Estimator is not the only method for kernel smoothing. We will learn about different methods in the next sections. Using the shorthand \\[\\begin{equation*} w_i(x) := \\frac{K_h(x - x_i)}{\\sum_{j=1}^n K_h(x - x_j)} \\end{equation*}\\] we can write the Nadaraya-Watson Estimator as \\(\\hat m_h(x) = \\sum_{i=1}^n w_i(x) y_i\\) and since \\[\\begin{align*} \\sum_{i=1}^n w_i(x) &amp;= \\sum_{i=1}^n \\frac{K_h(x - x_i)}{\\sum_{j=1}^n K_h(x - x_j)} \\\\ &amp;= \\frac{\\sum_{i=1}^n K_h(x - x_i)}{\\sum_{j=1}^n K_h(x - x_j)} \\\\ &amp;= 1, \\end{align*}\\] this is indeed a weighted average. Example 5.1 The faithful dataset built into R contains 272 observations of waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park. We can use the ksmooth() function to compute Nadaraya-Watson estimate for the waiting time after an eruption of a given length. Here we use a Gaussian kernel with bandwidth 1. x &lt;- faithful$eruptions y &lt;- faithful$waiting plot(x, y, cex = .5, xlab = &quot;eruption time [mins]&quot;, ylab = &quot;time to next eruption [mins]&quot;) lines(ksmooth(x, y, kernel = &quot;normal&quot;, bandwidth = 1, n.points = 1000), col = &quot;red&quot;) The estimate \\(\\hat m_h\\) (red line) smoothly connects the two clusters visible in the scatter plot. For kernels with bounded support, e.g. for the Epanechnikov kernel, the denominator \\(\\sum_{j=1}^n K_h(x - x_j)\\) will equal zero for \\(x\\) which are too far away from all of the \\(x_i\\). For these \\(x\\), the weights \\(w_i\\) and thus also the estimate \\(\\hat m_h(x)\\) are undefined. This problem can easily be seen in practice, when the bandwidth is chosen too small. Example 5.2 To illustrate the problem of the estimate becoming undefined far away from the data points, we redo the previous estimate using a uniform kernel: plot(x, y, cex = .5, xlab = &quot;eruption time [mins]&quot;, ylab = &quot;time to next eruption [mins]&quot;) lines(ksmooth(x, y, kernel = &quot;box&quot;, bandwidth = 1, n.points = 1000), col = &quot;red&quot;) lines(ksmooth(x, y, kernel = &quot;box&quot;, bandwidth = 0.1, n.points = 1000), col = &quot;blue&quot;) For \\(h = 1\\) (red line) we get a line \\(\\hat m_h\\) which is less smooth than the estimate using the Gaussian kernel, but is otherwise looks similar to the previous estimate. In contrast, if we reduce the bandwidth to \\(h = 0.1\\) (blue line), gaps start to appear in the plot of \\(\\hat m_h\\) where the spacing of the data points is too large. 5.2 Estimation Error Here we will discuss how fast the estimation error decreases in the limit of \\(n\\to \\infty\\), i.e. for the case when we have a large dateset to use for the estimate. As before, we will find that we need to decrease the bandwidth \\(h\\) as \\(n\\) increases. To allow for \\(n\\) to change, we will introduce a statistical model also for the inputs \\(x_i\\). (This is different from what we did in the level 3 part of the module for linear regression.) Here we will consider the following model: \\(X_1, \\ldots, X_n\\) are independent and identically distributed with density \\(f\\). \\(\\eta_1, \\ldots \\eta_n\\) are independent, with \\(\\mathbb{E}(\\eta_i) = 0\\) and \\(\\mathop{\\mathrm{Var}}(\\eta_i) = 1\\). \\(\\varepsilon_i = s(X_i) \\eta_i\\) for all \\(i \\in \\{1, \\ldots, n\\}\\), where \\(s\\colon \\mathbb{R}\\to (0, \\infty)\\) is a smooth function. \\(Y_i = m(X_i) + \\varepsilon_i\\) where \\(m\\colon \\mathbb{R}\\to \\mathbb{R}\\) is a smooth function. While this extended model allows us to increase \\(n\\), it also creates a practical problem: the estimator \\[\\begin{equation*} \\hat m_h(x) = \\frac{\\sum_{i=1}^n K_h(x - X_i) Y_i}{\\sum_{j=1}^n K_h(x - X_j)}, \\end{equation*}\\] now has random terms both in the numerator and in the denominator. This will make it more challenging to determine the behaviour of \\(\\mathbb{E}\\bigl( \\hat m_h(x) \\bigr)\\) and \\(\\mathop{\\mathrm{Var}}\\bigl( \\hat m_h(x) \\bigr)\\) as \\(n \\to \\infty\\) and \\(h \\downarrow 0\\). We can write \\(\\hat m_h(x)\\) as \\[\\begin{equation} \\hat m_h(x) = \\frac{\\frac1n \\sum_{i=1}^n K_h(x - X_i) Y_i}{\\frac1n \\sum_{j=1}^n K_h(x - X_j)} = \\frac{\\frac1n \\sum_{i=1}^n K_h(x - X_i) Y_i}{\\hat f_h(x)} = \\frac{\\hat r_h(x)}{\\hat f_h(x)} \\tag{5.1} \\end{equation}\\] where \\(\\hat f_h(x)\\) is the kernel density estimator from Section 1 and \\[\\begin{equation*} \\hat r_h(x) := \\frac1n \\sum_{i=1}^n K_h(x - X_i) Y_i. \\end{equation*}\\] We will consider the numerator and denominator of equation (5.1) separately Denominator From equations (2.5) and (3.5) we know that \\[\\begin{equation*} \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) \\approx f(x) + \\frac{\\mu_2(K) f&#39;&#39;(x)}{2} h^2 \\end{equation*}\\] and \\[\\begin{equation*} \\mathop{\\mathrm{Var}}\\bigl( \\hat f_h(x) \\bigr) \\approx \\frac{1}{nh} f(x) R(K) \\end{equation*}\\] as \\(h \\downarrow 0\\). Numerator We start by considering the numerator \\(\\hat r_h(x)\\). The arguments used here will be very similar to the arguments used in the section about the variance of kernel density estimates. The expectation of \\(\\hat r_h(x)\\) is \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat r_h(x) \\bigr) &amp;= \\mathbb{E}\\Bigl( \\frac1n \\sum_{i=1}^n K_h(x - X_i) Y_i \\Bigr) \\\\ &amp;= \\frac1n \\sum_{i=1}^n \\mathbb{E}\\bigl( K_h(x - X_i) Y_i \\bigr) \\\\ &amp;= \\mathbb{E}\\bigl( K_h(x - X) Y \\bigr) \\\\ &amp;= \\mathbb{E}\\Bigl( K_h(x - X) (m(X) + \\sigma(X)\\eta) \\Bigr). \\end{align*}\\] We use integrals to average over the randomness in \\(X\\) and \\(\\eta\\), denoting the density of \\(\\eta\\) by \\(\\varphi\\): \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat r_h(x) \\bigr) &amp;= \\int \\int K_h(x - \\xi) \\bigl( m(\\xi) + \\sigma(\\xi) e \\bigr) \\, \\varphi(e) \\, de \\, f(\\xi) \\, d\\xi \\\\ &amp;= \\int K_h(x - \\xi) \\Bigl( m(\\xi) +\\sigma(\\xi) \\int e \\, \\varphi(e) \\, de \\Bigr) \\, f(\\xi) \\, d\\xi \\\\ &amp;= \\int K_h(x - \\xi) m(\\xi) f(\\xi) \\, d\\xi, \\end{align*}\\] since \\[\\begin{equation*} \\int e \\, \\varphi(e) \\, de = \\mathbb{E}(\\eta) = 0. \\end{equation*}\\] Writing \\[\\begin{equation*} r(x) := m(x) f(x) \\end{equation*}\\] as an abbreviation, we finally get \\[\\begin{equation*} \\mathbb{E}\\bigl( \\hat r_h(x) \\bigr) = \\int K_h(x - \\xi) r(\\xi) \\, d\\xi. \\end{equation*}\\] We now formalise an argument which we already used earlier. Lemma 5.1 Let \\(g\\colon \\mathbb{R}\\to \\mathbb{R}\\) be two times continuously differentiable and let \\(K\\) be a kernel function. Then we have \\(\\displaystyle\\int K_h(x - \\xi) g(\\xi) \\, d\\xi = g(x) + \\frac12 \\mu_2(K) g&#39;&#39;(x) h^2 + o(h^2)\\) as \\(h \\downarrow 0\\), and \\(\\displaystyle\\int K_h(x - \\xi)^2 g(\\xi) \\, d\\xi = \\frac1h R(K) g(x) + o(1/h)\\) as \\(h \\downarrow 0\\). Proof. The first statement is proved using substitution and Taylor expandion of \\(r\\) around \\(x\\) as shown in the derivation of equation (2.5). The second statement is proved similarly, as shown in the derivation of equation (3.3). Using the first part of lemma 5.1 for \\(g = r\\) we get \\[\\begin{equation*} \\mathbb{E}\\bigl( \\hat r_h(x) \\bigr) = r(x) + \\frac12 \\mu_2(K) r&#39;&#39;(x) h^2 + o(h^2). \\end{equation*}\\] For the variance of \\(\\hat r_h(x)\\) we get \\[\\begin{align*} \\mathop{\\mathrm{Var}}\\bigl( \\hat r_h(x) \\bigr) &amp;= \\mathop{\\mathrm{Var}}\\Bigl( \\frac1n \\sum_{i=1}^n K_h(x - X_i) Y_i \\Bigr) \\\\ &amp;= \\frac{1}{n^2} \\sum_{i=1}^n \\mathop{\\mathrm{Var}}\\bigl( K_h(x - X_i) Y_i \\bigr) \\\\ &amp;= \\frac1n \\mathop{\\mathrm{Var}}\\bigl( K_h(x - X) Y \\bigr) \\\\ &amp;= \\frac1n \\Bigl( \\mathbb{E}\\bigl( K_h(x - X)^2 Y^2 \\bigr) - \\mathbb{E}\\bigl( K_h(x - X) Y \\bigr)^2 \\Bigr). \\end{align*}\\] We have already seen that \\[\\begin{equation*} \\mathbb{E}\\bigl( K_h(x - X) Y \\bigr) = \\mathbb{E}\\bigl( \\hat r_h(x) \\bigr) = r(x) + \\frac12 \\mu_2(K) r&#39;&#39;(x) h^2 + o(h^2) \\end{equation*}\\] and thus \\[\\begin{equation*} \\mathbb{E}\\bigl( K_h(x - X) Y \\bigr)^2 = r(x)^2 + O(h^2). \\end{equation*}\\] Using the second part of lemma 5.1 one can show that \\[\\begin{align*} \\mathbb{E}\\bigl( K_h(x - X)^2 Y^2 \\bigr) &amp;= \\int \\int K_h(x - \\xi)^2 \\bigl( m(\\xi) + s(\\xi)e \\bigr)^2 \\,\\varphi(e)\\,de \\,f(\\xi)\\,d\\xi \\\\ &amp;= \\int K_h(x - \\xi)^2 \\bigl( m(\\xi)^2 + s(\\xi)^2 \\bigr) f(\\xi) \\,d\\xi \\\\ &amp;= \\frac1h R(K) \\bigl( m(x)^2 + s(x)^2 \\bigr) f(x) + o(1/h). \\end{align*}\\] Combining these equations we find \\[\\begin{equation*} \\mathop{\\mathrm{Var}}\\bigl( \\hat r_h(x) \\bigr) \\approx \\frac{1}{nh} R(K) \\bigl( m(x)^2 + s(x)^2 \\bigr) f(x) + \\frac1n r(x)^2 \\end{equation*}\\] as \\(n\\to\\infty\\), \\(h\\downarrow 0\\) and \\(nh\\to\\infty\\). Mean Squared Error To turn our results about \\(\\hat r_h\\) and our previous results about \\(\\hat f\\) into an error estimate for \\[\\begin{equation*} \\hat m_h(x) = \\frac{\\hat r_h(x)}{\\hat f_h(x)}, \\end{equation*}\\] we consider Taylor expansion of the function \\(g(y) = 1/y\\): \\[\\begin{align*} g(y + h) &amp;= g(y) + g&#39;(y) h + o(h) \\\\ &amp;= \\frac1y - \\frac{1}{y^2} h + o(h). \\end{align*}\\] Using this approximation we get \\[\\begin{align*} \\hat m_h(x) &amp;= \\hat r_h(x) g\\bigl( \\hat f_h(x) \\bigr) \\\\ &amp;= \\hat r_h(x) g\\bigl( f(x) + \\hat f_h(x) - f(x) \\bigr) \\\\ &amp;\\approx \\hat r_h(x) \\Bigl( \\frac{1}{f(x)} - \\frac{1}{f(x)^2} (\\hat f_h(x) - f(x)) \\Bigr) \\\\ &amp;= \\frac{\\hat r_h(x)}{f(x)} - \\frac{\\hat r_h(x) \\bigl(\\hat f_h(x) - f(x) \\bigr)}{f(x)^2}. \\end{align*}\\] With the help of this trick, we have achieved that now all random terms are in the denominator and thus we can take expectations easily: \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat m_h(x) \\bigr) &amp;= \\frac{\\mathbb{E}\\bigl( \\hat r_h(x) \\bigr)}{f(x)} - \\frac{\\mathbb{E}\\Bigl( \\hat r_h(x) \\bigl(\\hat f_h(x) - f(x) \\bigr) \\Bigr)}{f(x)^2} \\\\ &amp;\\approx \\frac{\\mathbb{E}\\bigl( \\hat r_h(x) \\bigr)}{f(x)} - \\frac{\\mathbb{E}\\bigl( \\hat r_h(x) \\bigr) \\mathbb{E}\\bigl( \\hat f_h(x) - f(x) \\bigr)}{f(x)^2}. \\end{align*}\\] Substituting in our previous results we get \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat m_h(x) \\bigr) &amp;\\approx \\frac{r(x) + \\frac12 \\mu_2(K) r&#39;&#39;(x) h^2 + o(h^2)}{f(x)} - \\frac{\\bigl( r(x) + \\frac12 \\mu_2(K) r&#39;&#39;(x) h^2 + o(h^2) \\bigr) \\frac12 \\mu_2(K) f&#39;&#39;(x) h^2}{f(x)^2} \\\\ &amp;= \\frac{r(x)}{f(x)} + \\frac12 \\frac{\\mu_2(K) r&#39;&#39;(x)}{f(x)} h^2 - \\frac12 \\frac{\\mu_2(x) r(x) f&#39;&#39;(x)}{f(x)^2} h^2 + o(h^2) \\end{align*}\\] Using \\(r(x) = f(x) m(x)\\) we find the derivative \\(r&#39;(x) = f&#39;(x) m(x) + f(x) m&#39;(x)\\) as well as the second derivative \\(r&#39;&#39;(x) = f&#39;&#39;(x) m(x) + 2 f&#39;(x) m&#39;(x) + f(x) m&#39;&#39;(x)\\). This gives \\[\\begin{align*} \\mathbb{E}\\bigl( \\hat m_h(x) \\bigr) &amp;= m(x) + \\frac12 \\frac{\\mu_2(K) r&#39;&#39;(x)}{f(x)} h^2 - \\frac12 \\frac{\\mu_2(x) m(x) f&#39;&#39;(x)}{f(x)} h^2 + o(h^2) \\\\ &amp;= m(x) + \\frac12 \\frac{\\mu_2(K) \\bigl(2 f&#39;(x) m&#39;(x) + f(x) m&#39;&#39;(x)\\bigr)}{f(x)} h^2 + o(h^2) \\\\ &amp;= m(x) + \\mu_2(K) \\Bigl( \\frac{f&#39;(x)}{f(x)} m&#39;(x) + \\frac12 m&#39;&#39;(x) \\Bigr) h^2 + o(h^2) \\end{align*}\\] and \\[\\begin{equation*} \\mathop{\\mathrm{bias}}\\bigl( \\hat m_h(x) \\bigr) = \\mu_2(K) \\Bigl( \\frac{f&#39;(x)}{f(x)} m&#39;(x) + \\frac12 m&#39;&#39;(x) \\Bigr) h^2 + o(h^2). \\end{equation*}\\] A similar calculation gives the approximate variance as \\[\\begin{equation*} \\mathop{\\mathrm{Var}}\\bigl( \\hat m_h(x) \\bigr) = \\frac{1}{nh} \\frac{\\sigma^2(x) R(K)}{f(x)} + o\\Bigl( \\frac{1}{nh} \\Bigr). \\end{equation*}\\] So finally we have \\[\\begin{align*} \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat m_h(x) \\bigr) &amp;= \\frac{h^4 \\mu_2(K)^2}{4} \\Bigl(m&#39;&#39;(x) +\\frac{2m&#39;(x) f&#39;(x)}{f(x)} \\Bigr)^2 \\\\ &amp;\\hskip1cm + \\frac{1}{nh} \\frac{\\sigma^2(x) R(K)}{f(x)} + o\\Bigl( \\frac{1}{nh} \\Bigr) + o(h^4). \\end{align*}\\] Notes: A more careful calculation will need to take into account that \\(\\hat m(x)\\) may be undefined. All expectations and variances are conditional on \\(\\hat f(x) \\neq 0\\). One can show that \\(P\\bigl( \\hat f(x) \\neq 0 \\bigr) \\to 1\\) as \\(n\\to\\infty\\) for all \\(x\\in\\mathbb{R}\\) with \\(f(x) &gt; 0\\), so this is not a problem. The MSE is of order \\(O(n^{-4/5})\\) when we choose \\(h \\sim n^{-1/5}\\), as before. The formula for the variance shows that the regression curve is more stable in those areas where there are plenty of observations. The bias-squared is either dominated by the second derivative \\(m&#39;&#39;(x)\\) - when we are close to a local extremum of \\(m(x)\\) (turning point), or by the first derivative \\(m&#39;(x)\\) when we have few observations. This calculation is helpful in creating confidence intervals for the estimate \\(\\hat m_h(x)\\) in which \\(\\sigma^2(x)\\) can be estimated by \\[\\hat \\sigma^2(x) = \\sum w_i(x) \\bigl( y_i-\\hat m_h^{(i)}(x_i) \\bigr)^2,\\] where \\(\\hat m_h^{(i)}(x_i)\\) is the estimate of \\(m\\) at the point \\(x_i\\) using all of the data except for the observation at \\((x_i, y_i)\\). Summary We have learned how the Nadaraya-Watson Estimator can be used for smoothing. We have considered the mean squared error of this estimator. "],["X06-locpoly.html", "Section 6 Local Polynomial Regression 6.1 Linear Regression with Weights 6.2 Polynomial Regression 6.3 Polynomial Regression with Weights 6.4 Special Cases", " Section 6 Local Polynomial Regression Local polynomial regression is a generalisation of the Nadaraya-Watson estimator. The method combines the two ideas of linear regression with weights and polynomial regression. The aim is still to estimate the model mean \\(m \\colon\\mathbb{R}\\to \\mathbb{R}\\) from given data \\((x_1, y_1), \\ldots, (x_n, y_n)\\). 6.1 Linear Regression with Weights In the level 3 part of the module, we introduced the least squares method for linear regression. This method estimartes the regression coefficients by minimising the residual sum of squares: \\[\\begin{equation*} r(\\beta) = \\sum_{i=1}^n \\varepsilon_i^2 = \\sum_{i=1}^n \\bigl( y_i - (X\\beta)_i \\bigr)^2. \\end{equation*}\\] Here we will extend this method to include weights for the observations. Given weights \\(w_1, \\ldots, w_n &gt; 0\\), the weighted least squares method minimises \\[\\begin{equation*} r_w(\\beta) = \\sum_{i=1}^n w_i \\varepsilon_i^2 = \\sum_{i=1}^n w_i \\bigl( y_i - (X\\beta)_i \\bigr)^2. \\end{equation*}\\] In matrix notation, this function can be written as \\[\\begin{align*} r_w(\\beta) = (y - X \\beta)^\\top W (y - X \\beta), \\end{align*}\\] where \\(W\\) is a diagonal matrix with the weights on the diagonal: \\[\\begin{equation} W = \\begin{pmatrix} w_1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; w_2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; w_3 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; w_n \\end{pmatrix}. \\tag{6.1} \\end{equation}\\] Similar to lemma 2.1 in the level 3 notes, we can take derivatives to find the minimum of \\(r_w\\). The result is \\[\\begin{equation*} \\hat\\beta = (X^\\top W X)^{-1} X^\\top W y. \\end{equation*}\\] Since \\(W\\) appears once in the inverse and once before the \\(y\\), we can multiply \\(W\\) by any number without changing the result. Thus we don’t need to “normalise” the weights \\(w_i\\) to sum to one. As before, the fitted value for inputs \\((\\tilde x_1, \\ldots, \\tilde x_p) \\in \\mathbb{R}^p\\) is given by \\[\\begin{equation*} \\hat\\beta_0 + \\hat\\beta_1 \\tilde x_1 + \\cdots + \\hat\\beta_p \\tilde x_p = \\tilde x^\\top \\hat\\beta, \\end{equation*}\\] where \\(\\tilde x = (1, \\tilde x_1, \\ldots, \\tilde x_n)\\). 6.2 Polynomial Regression In these notes we only consider the case of \\(p=1\\). In this case we can easily fit a polynomial of degree \\(p\\) to the data by using \\(x, x^2, \\ldots, x^p\\) as the input variables. The corresponding model is \\[\\begin{equation*} y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_p x^p + \\varepsilon. \\end{equation*}\\] This leads to the design matrix \\[\\begin{equation*} X = \\begin{pmatrix} 1 &amp; x_1 &amp; x_1^2 &amp; \\cdots &amp; x_1^p \\\\ 1 &amp; x_2 &amp; x_2^2 &amp; \\cdots &amp; x_2^p \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_n &amp; x_n^2 &amp; \\cdots &amp; x_n^p \\end{pmatrix}. \\end{equation*}\\] Example 6.1 To illustrate polynomial regression, we fit a third-order polynomial to a simple, simulated dataset. Since the operator ^ has a special meaning inside lm(), we have to use the function I() (which disables the special meaning of + and ^ for its arguments) when computing the inputs \\(x^2\\) and \\(x^3\\). set.seed(20211102) n &lt;- 40 x &lt;- seq(0, 10, length.out = n) y &lt;- cos(x) + rnorm(n, sd = 0.5) m &lt;- lm(y ~ x + I(x^2) + I(x^3)) plot(x, y) lines(x, fitted(m)) The resulting regression line seems like a resonable fit for the data. We note that a third-order polynomial grows very quickly to \\(\\pm\\infty\\) as \\(|x|\\) increases. Thus, the fitted model cannot be used for extrapolating beyond the range of the data. When the regression is set up in this way, the design matrix often suffers from collinearity. To check for this, we can consider the condition number \\(\\kappa\\). For the example above we get the following value: kappa(m, exact = TRUE) ## [1] 1849.947 This is a large value, indicating collinearity. To improve the setup of the problem we can use the model \\[\\begin{equation*} y = \\beta_0 + \\beta_1 (x - \\tilde x) + \\beta_2 (x - \\tilde x)^2 + \\cdots + \\beta_p (x - \\tilde x)^p + \\varepsilon, \\end{equation*}\\] where \\(\\tilde x\\) is inside the interval if \\(x\\) values. This leads to the design matrix \\[\\begin{equation} X = \\begin{pmatrix} 1 &amp; (x_1-\\tilde x) &amp; (x_1-\\tilde x)^2 &amp; \\cdots &amp; (x_1-\\tilde x)^p \\\\ 1 &amp; (x_2-\\tilde x) &amp; (x_2-\\tilde x)^2 &amp; \\cdots &amp; (x_2-\\tilde x)^p \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; (x_n-\\tilde x) &amp; (x_n-\\tilde x)^2 &amp; \\cdots &amp; (x_n-\\tilde x)^p \\end{pmatrix}. \\tag{6.2} \\end{equation}\\] Example 6.2 Continuing from the previous example, we can see that writing the model as in (6.2) greatly improves the condition number: x.tilde &lt;- 5 m2 &lt;- lm(y ~ I(x-x.tilde) + I((x-x.tilde)^2) + I((x-x.tilde)^3)) kappa(m2, exact = TRUE) ## [1] 76.59629 While there is still collinearity, the condition number is now much smaller. Polynomials of higher degree take very large values as \\(|x|\\) increases and often make poor global models. Instead, these polynomials are best used for local interpolation of data. 6.3 Polynomial Regression with Weights The idea of local polynomial regression is to combine polynomial regression with weights which give more weight to close by observations: to get an estimate at a point \\(\\tilde x \\in \\mathbb{R}\\), we define \\[\\begin{equation*} w_i := K_h(\\tilde x - x_i), \\end{equation*}\\] where \\(K_h\\) is a scaled kernel function as before. Using the diagonal matrix \\(W\\) from (6.1) for the weights and the matrix \\(X\\) from (6.2) as the design matrix, we can fit a polynomial of degree \\(p\\) to the data which fits the data near \\(\\tilde x\\) well. The regression coefficients are again estimated as \\[\\begin{equation*} \\hat\\beta = (X^\\top W X)^{-1} X^\\top W y \\end{equation*}\\] and the model mean near \\(\\tilde x\\) is given by \\[\\begin{equation*} \\hat m_h(x; \\tilde x) = \\hat\\beta_0 + \\hat\\beta_1 (x - \\tilde x) + \\hat\\beta_2 (x - \\tilde x)^2 + \\cdots + \\hat\\beta_p (x - \\tilde x)^p, \\end{equation*}\\] where the weights \\(\\hat\\beta\\) depend on \\(\\tilde x\\). The model mean at \\(x = \\tilde x\\) simplifies to \\[\\begin{align*} \\hat m_h(\\tilde x) &amp;= \\hat m_h(\\tilde x; \\tilde x) \\\\ &amp;= \\hat\\beta_0 + \\hat\\beta_1 (\\tilde x - \\tilde x) + \\hat\\beta_2 (\\tilde x - \\tilde x)^2 + \\cdots + \\hat\\beta_p (\\tilde x - \\tilde x)^p \\\\ &amp;= \\hat\\beta_0. \\end{align*}\\] Using matrix notation, we can write this as \\[\\begin{align*} \\hat m_h(\\tilde x) &amp;= \\hat\\beta_0 \\\\ &amp;= e_0^\\top \\hat\\beta \\\\ &amp;= e_0^\\top (X^\\top W X)^{-1} X^\\top W y, \\end{align*}\\] where \\(e_0 = (1, 0, \\ldots, 0) \\in \\mathbb{R}^{p+1}\\). Since both \\(X\\) and \\(W\\) depend on \\(\\tilde x\\), we need to evaluate \\((X^\\top W X)^{-1} X^\\top W y\\) separately for each \\(\\tilde x\\) where an estimate of \\(\\hat m_h\\) is needed. To get a regression line, this needs to be done over a grid of \\(\\tilde x\\) values. Thus, this method can be computationally expensive. 6.4 Special Cases Here we discuss the special cases of \\(p=0\\), \\(p=1\\), and \\(p=2\\). \\(p = 0\\) For \\(p=0\\), the polynomial consists only of the constant term \\(\\beta_0\\). In this case, the design matrix \\(X\\) simplifies to \\(X = (1, \\ldots, 1) \\in \\mathbb{R}^{n\\times 1}\\). Thus we have \\[\\begin{align*} X^\\top W X &amp;= (1, \\ldots, 1)^\\top W (1, \\ldots, 1) \\\\ &amp;= \\sum_{i=1}^n w_i \\\\ &amp;= \\sum_{i=1}^n K_h(\\tilde x - x_i). \\end{align*}\\] Similarly, we have \\[\\begin{align*} X^\\top W y &amp;= (1, \\ldots, 1)^\\top W y \\\\ &amp;= \\sum_{i=1}^n w_i y_i \\\\ &amp;= \\sum_{i=1}^n K_h(\\tilde x - x_i) y_i. \\end{align*}\\] Thus we find \\[\\begin{align*} \\hat m_h(\\tilde x) &amp;= (X^\\top W X)^{-1} X^\\top W y \\\\ &amp;= \\frac{\\sum_{i=1}^n K_h(\\tilde x - x_i) y_i}{\\sum_{i=1}^n K_h(\\tilde x - x_i)}. \\end{align*}\\] This is the same formula as in definition 5.1: for \\(p=0\\) the local polynomial regression estimator is the same as the Nadaraya-Watson estimator. \\(p = 1\\) For \\(p=1\\) the polynomial is a straight line, allowing to model the value as well as the slope of the mean line. The resulting estimator is called the local linear estimator. This sometimes gives a better fit than the Nadaraya-Watson estimator, for example at the boundaries of the domain. Example 6.3 We can use the R function locpoly from the KernSmooth package to compute locally polynomial regression estimates. Here we plot the estimate for \\(p=1\\) (blue line) together with the Nadaraya-Watson estimator (red line), for a simple, simulated dataset. Unfortunately, the function locpoly() has an interpretation of the bandwidth which is different from what ksmooth() uses. Experimentally I found that bandwidth = 0.3 for ksmooth() corresponds to bandwidth = 0.11 for locpoly(): the output of ksmooth(..., bandwidth = 0.3) and of locpoly(.., degree = 0, bandwidth = 0.11) is near identical. set.seed(20211103) n &lt;- 200 x &lt;- seq(0, 1, length.out = n) y &lt;- x + rnorm(n, sd = 0.05) plot(x, y, cex = .5) m1 &lt;- ksmooth(x, y, kernel = &quot;normal&quot;, bandwidth = 0.3) lines(m1, col = &quot;red&quot;) library(KernSmooth) ## KernSmooth 2.23 loaded ## Copyright M. P. Wand 1997-2009 m2 &lt;- locpoly(x, y, degree = 1, bandwidth = 0.11) lines(m2, col = &quot;blue&quot;) Near the boundaries the Nadaraya-Watson estimator is biased, because on the left-hand boundary all nearby samples correspond to larger values of the mean line, and similarly on the right-hand boundary all nearby samples correspond to smaller values of the mean line. In contrast, the local polynomial estimate retains its slope right up to the boundary. \\(p = 2\\) For \\(p=2\\) the local polynomials are parabolas. This allows sometimes to reduce bias near peaks. Example 6.4 We compare the Nadaraya-Watson estimator to locally polynomial regression with \\(p=2\\), using a simulated dataset which has a peak in the middle of the domain. We choose the same bandwidths as in the previous example. set.seed(20211103) n &lt;- 200 x &lt;- seq(-1, 1, length.out = n) y &lt;- 1/(x^2 + 0.05) + rnorm(n, sd = 1) plot(x, y, cex = .5) m1 &lt;- ksmooth(x, y, kernel = &quot;normal&quot;, bandwidth = 0.3, n.points = 100) lines(m1, col = &quot;red&quot;) library(KernSmooth) m2 &lt;- locpoly(x, y, degree = 2, bandwidth = 0.11) lines(m2, col = &quot;blue&quot;) We can see that the Nadaraya-Watson estimator (red line) is biased near the peak, because all nearby samples correspond to smaller values of the mean line. In contrast, the local polynomial estimate (blue line) has much lower bias. Summary Regression with weights can be used to fit models to the data near a given point. A simple application of linear regression can fit polynomials as well as straight lines. The local polynomial regression estimator is a generalization of the Nadaraya-Watson estimator. "],["X07-nearest.html", "Section 7 k-Nearest Neighbour Regression 7.1 Definition of the Estimator 7.2 Properties 7.3 Numerical Experiment 7.4 Variants of the Method", " Section 7 k-Nearest Neighbour Regression In the previous sections we used a fixed bandwidth \\(h\\) to determine the scale on which “closeness” of existing samples to a new input \\(x\\) was measured. While this approach generally works well, problems can appear in regions where samples are sparse (e.g. in example 5.2). This problem can be addressed by choosing \\(h\\) adaptively, using larger bandwidths where samples are sparse and smaller bandwidths in regions where there are many samples. The \\(k\\)-nearest neighbour method is one of several methods which implements this idea. 7.1 Definition of the Estimator Definition 7.1 For \\(k \\in \\{1, \\ldots, n\\}\\), the k-nearest neighbour, or \\(k\\)-NN estimate for the model mean \\(m(x)\\) is given by \\[\\begin{equation} \\hat m_k(x) := \\frac1k \\sum_{i\\in J_k(x)} y_i, \\tag{7.1} \\end{equation}\\] where \\[\\begin{equation*} J_k(x) := \\bigl\\{ i \\bigm| \\mbox{$x_i$ is one of the $k$ nearest observations to $x$} \\bigr\\}. \\end{equation*}\\] The \\(k\\)-NN estimate \\(\\hat m_k(x)\\) is the average of the \\(k\\) responses where the inputs are closest to \\(x\\). We can interpret equation (7.1) as a weighted average \\[\\begin{equation*} \\hat m_k(x) = \\sum_{i=1}^n w_i(x) y_i, \\end{equation*}\\] where the weights are given by \\[\\begin{equation*} w_i(x) = \\begin{cases} \\frac1k, &amp; \\mbox{if $i \\in J_k(x)$, and} \\\\ 0 &amp; \\mbox{otherwise.} \\end{cases} \\end{equation*}\\] If several \\(x_i\\) have the same distance to \\(x\\), some tie-breaking rule must be used to decide which indices to include in the set \\(J_k(x)\\). This case is so unlikely that the choice of rule is not important. One could, for example, pick one of the tied neighbours at random. The method can be used both for the one-dimensional case \\(x\\in\\mathbb{R}\\), and for vector-valued inputs \\(x\\in\\mathbb{R}^p\\). For the one-dimensional case, it is advantageous to sort the data in order of increasing \\(x_i\\). In this case, the position of \\(x\\) in the list of the \\(x_i\\) can be found using a binary search, and the nearest neighbours can be identified by search to the left and right of this position. For \\(p &gt; 1\\) the method becomes computationally very expensive, since the data needs to be sorted afresh for every new input \\(x\\). Advanced data structures like “cover trees” can be used to speed up the process of finding the nearest neighbours. 7.2 Properties The parameter \\(k\\) controls the “smoothness” of the estimate. In the extreme case \\(k = n\\), we have \\(J_n(x) = \\{1, \\ldots, n\\}\\) and \\[\\begin{equation*} \\hat m_k(x) = \\frac1n \\sum_{i=1}^n y_i \\end{equation*}\\] for all \\(x\\), i.e. for this case \\(\\hat m_k\\) is constant. The other extreme is the case of \\(k=1\\), where \\(\\hat m_k(x)\\) always equals the value of the closest \\(x_i\\) and has jumps at the mid-points between the data points. In the next section we will learn how \\(k\\) can be chosen using cross-validation. Independent of the value of \\(k\\), the function \\(\\hat m_k\\) is always a step function, with jumps at points \\(x\\) where two points have equal distance from \\(x\\). 7.3 Numerical Experiment In R, an implementation of the \\(k\\)-NN method can be found in the FNN package. This package implements not only \\(k\\)-NN regression, but also \\(k\\)-NN classification, and it implements sophisticated algorithms to speed up the search for the nearest neighbours in higher-dimensional spaces. The function to perform \\(k\\)-NN regression is knn.reg(). Example 7.1 Here we compute a \\(k\\)-NN estimate for the mean of the faithful dataset, which we have already encountered in examples 5.1 and 5.2. We start by storing the data in the variables x and y: x &lt;- faithful$eruptions y &lt;- faithful$waiting Now we use knn.reg() to compute the \\(k\\)-NN estimate on a grid of values. The help page for knn.reg() explains that we need to convert the input to either a matrix or a data frame; here we use data frames. The grid of input values where we want to estimate the \\(k\\)-NN estimate is passed in via the optional argument test = .... Here we use the arbitrarily chosen value \\(k = 50\\). library(FNN) x.test &lt;- seq(1.5, 5, length.out = 500) m &lt;- knn.reg(data.frame(x=x), y = y, test = data.frame(x=x.test), k = 50) plot(x, y, cex=.5) lines(x.test, m$pred, col = &quot;blue&quot;) The estimated mean curve looks reasonable. 7.4 Variants of the Method For one-dimensional inputs and even \\(k\\), the symmetric k-NN estimate averages the responses corresponding to the \\(k/2\\) nearest neighbours smaller than \\(x\\) and the \\(k/2\\) nearest neighbours larger than \\(x\\). To obtain a continuous estimate, we can define a “local bandwidth” \\(h(x)\\) as \\[\\begin{equation*} h(x) = c \\max\\bigl\\{ |x - x_i| \\bigm| i \\in J_k(x) \\bigr\\} \\end{equation*}\\] for some constant \\(c\\), and then use the Nadaraya-Watson estimator with this bandwidth: \\[\\begin{equation*} \\tilde m(x) = \\frac{\\sum_{i=1}^n K_{h(x)}(x - x_i) y_i}{\\sum_{j=1}^n K_{h(x)}(x - x_j)}, \\end{equation*}\\] where \\(K\\) is a kernel function as before. If we use \\(c = 1\\) together with the uniform kernel \\[\\begin{equation*} K(x) = \\begin{cases} 1/2 &amp; \\mbox{if $-1 \\leq x \\leq 1$} \\\\ 0 &amp; \\mbox{otherwise,} \\end{cases} \\end{equation*}\\] this method coincides with the \\(k\\)-NN estimator. "],["X08-xval.html", "Section 8 Cross-validation 8.1 Regression 8.2 Kernel Density Estimation", " Section 8 Cross-validation In this section we will discuss methods for chosing the bandwidth \\(h\\) for kernel-based methods, and the size \\(k\\) of the neighbourhood used in \\(k\\)-nearest neighbour regression. The methods we will use here are based on cross-validation. The idea of cross-validation is to measure goodness of fit by comparing each sample \\(y_i\\) to a fitted value \\(\\tilde y_i\\), where the fitted values \\(\\tilde y_i\\) are computed from a subset of the data which excludes \\(x_i\\). This way, a method cannot achive a misleadingly good fit by overfitting the data. There are different ways to implement this idea: In k-fold cross-validation, the data is partitioned into \\(k\\) subsamples of approximately equal size. Only \\(k\\) models are fitted, each one leaving out the data from one subsample. Then for every \\(i\\) there is exactly one model which does not use \\((x_i, y_i)\\), and to compute the fitted value for \\((x_i, y_i)\\), we use this model. Since fewer data are used to fit each model, this method gives less accurate results than leave-one-out cross-validation. For this method to work, it is important that the subsamples are independent of each other. In leave-one-out cross-validation, a separate model is fitted for each \\(i \\in \\{1, \\ldots, n\\}\\), leaving out just the sample \\((x_i, y_i)\\) for this model. This is the special case of \\(k\\)-fold cross validation where \\(k = n\\). Since \\(n\\) models need to be fitted to the data, for large \\(n\\) the method can be computationally expensive. 8.1 Regression In linear regression, we find the regression line by minimising the residual sum of squares. One could be tempted to try the same approach here and to find the “best” \\(h\\) by minimising \\[\\begin{equation*} r(h) := \\sum_{i=1}^n \\bigl( y_i - \\hat m_h(x_i) \\bigr)^2. \\end{equation*}\\] Unfortunately, this approach does not work: for \\(h \\downarrow 0\\) we have \\(\\hat m_h(x_i) \\to y_i\\) and thus \\(r(h) \\to 0\\). For this reason, minimising \\(r(h)\\) always finds \\(h=0\\) as the optimal value of \\(h\\). To solve the problem we use leave-one-out cross validation and minimise \\[\\begin{equation*} r_\\mathrm{LOO}(h) := \\sum_{i=1}^n \\bigl( y_i - \\hat m^{(i)}_h(x_i) \\bigr)^2 \\end{equation*}\\] instead, where \\(m^{(i)}\\) is the kernel regression estimate computed without using sample \\(i\\): \\[\\begin{equation*} \\hat m_h^{(i)}(x) = \\frac{\\sum_{j \\;\\mid\\; j\\neq i} K_h(x - x_j)y_j}{\\sum_{j \\;\\mid\\; j\\neq i}K_h(x - x_j)} \\end{equation*}\\] for the Nadaraya-Watson Estimator and similarly for local polynomial regression. A similar approach can be used to find the optimal \\(k\\) for a \\(k\\)-nearest neighbour estimate. 8.2 Kernel Density Estimation When using kernel density estimation in practice, we need to choose the bandwidth \\(h\\). One idea for finding a good \\(h\\) is by using maximum likelihood estimation: We could try to choose \\(h\\) to maximize the likelihood \\[\\begin{equation*} L(h;x_1, \\ldots, x_n) = \\prod_{i=1}^n \\hat f_h(x_i), \\end{equation*}\\] but this gives the solution \\(h=0\\). So, instead we maximize the leave-one-out estimate of the log likelihood, which is given by \\[\\begin{equation*} L_\\mathrm{LOO}(h;x_1, \\ldots, x_n) = \\prod_{i=1}^n \\hat f^{(i)}_h(x_i). \\end{equation*}\\] This technique is known as maximum likelihood cross-validation. When this method is used in practice, it is advantageous to minimise \\[\\begin{align*} \\mathcal{L}_\\mathrm{LOO}(h;x_1, \\ldots, x_n) &amp;:= \\log\\Bigl( L_\\mathrm{LOO}(h;x_1, \\ldots, x_n) \\Bigr) \\\\ &amp;= \\log\\Bigl( \\prod_{i=1}^n \\hat f^{(i)}_h(x_i) \\Bigr) \\\\ &amp;= \\sum_{i=1}^n \\log\\bigl( \\hat f^{(i)}_h(x_i) \\bigr) \\end{align*}\\] instead of \\(L_\\mathrm{LOO}\\), since the product in the definition of the likelihood can be strongly affected by numerical errors. An alternative method to find a good \\(h\\) considers the integrated mean squared error (IMSE) as a measure for the error. This is the same quantity we also used to derive our theoretical results: \\[\\begin{equation*} \\mathrm{IMSE}\\bigl( \\hat f_h \\bigr) = \\int_{-\\infty}^\\infty \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) \\,dx. \\end{equation*}\\] Unfortunately, the \\(h\\) which minimises this expression depends on properties of \\(f\\) and in section 4.3 we were only able to find a heuristic “plug-in” estimate to approximate the best \\(h\\). The following lemma shows how a variant of leave-one-out cross-validation can be used to estimate the optimal \\(h\\) from data. Lemma 8.1 Let \\(\\hat f_h\\) be the kernel density estimate with bandwidth \\(h\\) and let \\[\\begin{equation*} e(h) := \\int \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) dx - 2 \\int \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) f(x) dx. \\end{equation*}\\] Then the following statements hold. We have \\(\\mathrm{IMSE}\\bigl( \\hat f_h \\bigr) = e(h) + \\mathrm{const}\\), where \\(\\mathrm{const}\\) stands for a term which does not depend on \\(h\\). Let \\(f_h^{(i)}\\) be the kernel density estimate computed from the data with sample \\(i\\) omitted. Then \\[\\begin{equation*} \\mathrm{CV}(h) := \\int \\hat f_h(x)^2 dx - \\frac{2}{n}\\sum_{i=1}^n \\hat f_h^{(i)}(x_i) \\end{equation*}\\] is an (approximately) unbiased estimator for \\(e(h)\\). Proof. For the first statement can be shown by expanding the square in the definition of the (I)MSE: \\[\\begin{align*} \\mathrm{IMSE}\\bigl( \\hat f_h \\bigr) &amp;= \\int_{-\\infty}^\\infty \\mathop{\\mathrm{MSE}}\\nolimits\\bigl( \\hat f_h(x) \\bigr) \\,dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\mathbb{E}\\Bigl( \\bigl( \\hat f_h(x) - f(x) \\bigr)^2 \\Bigr) \\,dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\mathbb{E}\\Bigl( \\hat f_h(x)^2 - 2 \\hat f_h(x) f(x) + f(x)^2 \\Bigr) \\,dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\mathbb{E}\\Bigl( \\hat f_h(x)^2 \\Bigr) \\,dx - 2 \\int_{-\\infty}^\\infty \\mathbb{E}\\Bigl( \\hat f_h(x) f(x) \\Bigr) \\,dx \\\\ &amp;\\hskip5cm + \\int_{-\\infty}^\\infty \\mathbb{E}\\Bigl( f(x)^2 \\Bigr) \\,dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\mathbb{E}\\Bigl( \\hat f_h(x)^2 \\Bigr) \\,dx - 2 \\int_{-\\infty}^\\infty \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) f(x) \\,dx \\\\ &amp;\\hskip5cm + \\int_{-\\infty}^\\infty f(x)^2 \\,dx, \\end{align*}\\] where we used the fact that \\(f(x)\\) is not random. Since the last term does not depend on \\(h\\), the first claim is proved. For the second statement we need to consider the kernel density estimates computed from random data \\(X_1, \\ldots, X_n \\sim f\\), i.i.d. We have to show that \\[\\begin{equation*} \\mathbb{E}\\bigl( \\mathrm{CV}(h) \\bigr) = e(h) = \\int \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) dx - 2 \\int \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr) f(x) dx. \\end{equation*}\\] For the first term in the definition of \\(\\mathrm{CV}\\) we get \\[\\begin{equation*} \\mathbb{E}\\Bigl( \\int \\hat f_h(x)^2 dx \\Bigr) = \\int \\mathbb{E}\\bigl( \\hat f_h(x)^2 \\bigr) dx, \\end{equation*}\\] where we used Fubini’s theorem to exchange the expectation with the integral. For the second term we have \\[\\begin{align*} \\mathbb{E}\\Bigl( \\frac{1}{n} \\sum_{i=1}^n \\hat f_h^{(i)}(X_i) \\Bigr) &amp;= \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}\\Bigl( \\hat f_h^{(i)}(X_i) \\Bigr) \\\\ &amp;= \\mathbb{E}\\Bigl( \\hat f_h^{(1)}(X_1) \\Bigr), \\end{align*}\\] since the \\(X_i\\) are independent and identically distributed. Since the estimator \\(f_h^{(1)}\\) is computed from \\(X_2, \\ldots, X_n\\), which are independent of \\(X_1\\), we can evaluate the expecation on the right-hand side by first computing \\(\\mathbb{E}\\Bigl( \\hat f_h^{(1)}(x) \\Bigr)\\) as a function of \\(x\\), then using \\(X_1\\) in place of \\(x\\), and computing the expecation over \\(X_1\\) afterwards. This gives \\[\\begin{align*} \\mathbb{E}\\Bigl( \\frac{1}{n} \\sum_{i=1}^n \\hat f_h^{(i)}(X_i) \\Bigr) &amp;= \\mathbb{E}\\Bigl( \\mathbb{E}\\bigl( \\hat f_h^{(1)}(X_1) \\bigm| X_1 \\bigr) \\Bigr) \\\\ &amp;= \\int \\mathbb{E}\\bigl( \\hat f_h^{(1)}(x) \\bigr) \\, f(x) \\, dx, \\end{align*}\\] since \\(X_1 \\sim f\\). Finally, since \\(\\hat f_h^{(1)}\\) and \\(\\hat f_h\\) only differ in the sample size, by using \\(n-1\\) and \\(n\\) samples respectively, we have \\[\\begin{equation*} \\mathbb{E}\\bigl( \\hat f_h^{(1)}(x) \\bigr) \\approx \\mathbb{E}\\bigl( \\hat f_h(x) \\bigr). \\end{equation*}\\] Combining these equations completes the proof. Using the result from the lemma we see that we can choose \\(h\\) to minimise \\(\\mathrm{CV}(h)\\) in order to get a candidate for the bandwidth \\(h\\). This procedure is known as integrated squared error cross-validation. These two approaches differ slightly in the optimal \\(h\\), but the first one is easier to implement as there is no integration involved. Summary Cross-validation can be used to avoid overfitting the data when we choose \\(h\\). We have seen how to estimate \\(h\\) for kernel-based methods, and \\(k\\) for \\(k\\)-NN regression estimates. "],["X09-examples.html", "Section 9 Examples 9.1 Kernel Density Estimation 9.2 Kernel Regression 9.3 k-Nearest Neighbour Regression", " Section 9 Examples To conclude these notes, we give three examples where we use cross-validation to choose the tuning parameter in kernel density estimation, kernel regression, and k-nearest neighbour regression. 9.1 Kernel Density Estimation Here we show how to find a good bandwidth for Kernel Density Estimation, by using cross-validation. From lemma 8.1 we know that we can choose the \\(h\\) which minimises \\[\\begin{equation} \\mathrm{CV}(h) = \\int \\hat f_h(x)^2 dx - \\frac{2}{n}\\sum_{i=1}^n \\hat f_h^{(i)}(x_i) =: A - B. \\tag{9.1} \\end{equation}\\] We will consider the snow fall dataset again: # data from https://teaching.seehuhn.de/data/buffalo/ buffalo &lt;- read.csv(&quot;data/buffalo.csv&quot;) x &lt;- buffalo$snowfall n &lt;- length(x) In order to speed up the computation of the \\(f_h^{(i)}\\), we implement the kernel density estimate “by hand”. Thus, instead of using the built-in function density, we use the formula \\[\\begin{equation*} \\hat f_h(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i). \\end{equation*}\\] from definition 1.2. We use a number of “tricks” in the R code: For numerically computing the integral of \\(\\hat f_h^2\\) in term \\(A\\) we evaluate \\(\\hat f_h\\) on a grid of \\(x\\)-values, say \\(\\tilde x_1, \\ldots, \\tilde x_m\\). When computing \\(\\hat f_h(\\tilde x_j)\\) we need to compute all pair differences \\(\\tilde x_j - x_i\\). In R, this can efficiently be done using the command outer(x, x.tilde, \"-\"), which returns the pair differences as an \\(n \\times m\\) matrix. Here we use a Gaussian kernel, so that \\(K_h\\) can be evaluated using dnorm(..., sd = h) in R. This function can be applied to the matrix of pair differences; the result is a matrix K where row \\(i\\), column \\(j\\) stores the value \\(K_h(\\tilde x_j - x_i)\\). The kernel density estimate \\(\\hat f_h\\) now corresponds to the column means of the matrix K. In R, these can be efficiently computed using the command colMeans(). Term \\(A\\) in equation (9.1) can now be approximate by the sum of the \\(\\hat f_h(\\tilde x_j)\\), multiplied by the distance between the grid points: \\[\\begin{equation*} A = \\int \\hat f_h(x)^2 \\,dx \\approx \\sum_{j=1}^m \\hat f_h(\\tilde x_j)^2 \\, \\Delta \\tilde x. \\end{equation*}\\] To compute term \\(B\\) in equation (9.1), we can use the formula \\[\\begin{align*} \\sum_{j=1}^n \\hat f_h^{(j)}(x_j) &amp;= \\sum_{j=1}^n \\frac{1}{n-1} \\sum_{i\\neq j} K_h(x_j - x_i) \\\\ &amp;= \\frac{1}{n-1} \\sum_{i,j=1 \\atop i\\neq j}^n K_h(x_j - x_i). \\end{align*}\\] Here we can use outer() again, and then implement the condition \\(i\\neq j\\) by setting the matrix elements corresponding to \\(i=j\\) equal to 0 before taking the sum. Using these ideas, we can implement the function \\(\\mathrm{cv}(h)\\) in R as follows: cv.h &lt;- function(h) { x.min &lt;- min(x) - 3*h x.max &lt;- max(x) + 3*h m &lt;- 1000 dx &lt;- (x.max - x.min) / (m - 1) x.tilde &lt;- seq(x.min, x.max, length.out = m) K &lt;- dnorm(outer(x, x.tilde, &quot;-&quot;), sd = h) f.hat &lt;- colMeans(K) A &lt;- sum(f.hat^2 * dx) K &lt;- dnorm(outer(x, x, &quot;-&quot;), sd = h) diag(K) &lt;- 0 B &lt;- 2 * sum(K) / (n-1) / n return(A - B) } Finally, we evaluate the function cv.h() on a grid of \\(h\\)-values to find a good value of \\(h\\): h &lt;- 10^seq(-1, 3, length.out = 41) cv &lt;- numeric(length(h)) for (i in seq_along(h)) { cv[i] &lt;- cv.h(h[i]) } plot(h, cv, log=&quot;x&quot;, type = &quot;l&quot;) best.h &lt;- h[which.min(cv)] abline(v = best.h) The optimal bandwidth is \\(h = 12.59\\). The kernel density estimate using this \\(h\\) is shown in the following figure. x.min &lt;- min(x) - 3*best.h x.max &lt;- max(x) + 3*best.h m &lt;- 100 x.tilde &lt;- seq(x.min, x.max, length.out = m) K &lt;- dnorm(outer(x, x.tilde, &quot;-&quot;), sd = best.h) f.hat &lt;- colMeans(K) plot(x.tilde, f.hat, type = &quot;l&quot;, xlab = &quot;snowfall [in]&quot;, ylab = &quot;density&quot;) 9.2 Kernel Regression To illustrate cross-validation for the different smoothing methods, we use the faithful dataset again. x &lt;- faithful$eruptions y &lt;- faithful$waiting We compare the methods using the leave-one-out mean squared error \\[\\begin{equation*} r(h) = \\frac1n \\sum_{i=1}^n \\bigl( y_i - \\hat m^{(i)}(x_i) \\bigr)^2. \\end{equation*}\\] We start by considering the Nadaraya-Watson estimator. Here we have to compute \\[\\begin{equation*} \\hat m^{(i)}_h(x_i) = \\frac{\\sum_{j=1, j\\neq i}^n K_h(x_i - x_j) y_j}{\\sum_{j=1, j\\neq i}^n K_h(x_i - x_j)} \\end{equation*}\\] for all \\(i\\in\\{1, \\ldots, n\\}\\). To evaluate this expression in R, we use the same ideas as before: We use outer(x, x, \"-\") to compute all pair differences \\(x_i - x_j\\). We use dnorm(..., sd = h) to compute \\(K_h\\). We can obtain the leave-one-out estimate by setting the diagonal of \\(K\\) to zero. One new idea is needed to compute the products \\(K_h(x_i - x_j) y_j\\) in an efficient way: If we “multiply” a matrix K to a vector y using * (instead of using %*% for the usual matrix vector multiplication), the product is performed element-wise. If y has as many elements as K has rows, then the results is the matrix \\((k_{ij}y_i)_{i,j}\\), i.e. each row of K is multiplied with the corresponding element of y. Combining these ideas, we get the following function to compute the leave-one-out estimate for the mean squared error of the Nadaraya-Watson estimator: r.NW &lt;- function(h) { K &lt;- dnorm(outer(x, x, &quot;-&quot;), sd = h) # compute a leave-one-out estimate diag(K) &lt;- 0 m.hat &lt;- colSums(K*y) / colSums(K) mean((m.hat - y)^2) } We will also consider local linear smoothing, i.e. local polynomial smoothing where the degree \\(p\\) of the polynomials is \\(p=1\\). As we have seen in the section about Polynomial Regression with Weights, the local linear estimator can be computed as \\[\\begin{equation*} \\hat m_h(x) = e_0^\\top (X^\\top W X)^{-1} X^\\top W y, \\end{equation*}\\] where \\(X\\) and \\(W\\) are defined as in equations (6.2) and (6.1). Here we use the “linear” case (\\(p=1\\)) instead of the polynomial case (\\(p\\geq 1\\)). For this case it is easy to check that we have \\[\\begin{equation*} X^\\top W X = \\begin{pmatrix} \\sum_j K_h(x-x_j) &amp; \\sum_j K_h(x-x_j) x_j \\\\ \\sum_j K_h(x-x_j) x_j &amp; \\sum_j K_h(x-x_j) x_j^2 \\end{pmatrix} \\end{equation*}\\] and \\[\\begin{equation*} X^\\top W y = \\begin{pmatrix} \\sum_j K_h(x-x_j) y_j \\\\ \\sum_j K_h(x-x_j) x_j y_j \\end{pmatrix}. \\end{equation*}\\] Using the formula \\[\\begin{equation*} \\begin{pmatrix} a &amp; b \\\\ c &amp; d \\end{pmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d &amp; -b \\\\ -c &amp; a \\end{pmatrix} \\end{equation*}\\] for the inverse of a general \\(2\\times 2\\)-matrix, we find \\[\\begin{equation*} \\hat m_h(x) = \\frac{T_1 T_2 - T_3 T_4}{B_1 B_2 - B_3^2}, \\end{equation*}\\] where \\[\\begin{align*} T_1 &amp;= \\sum_{j=1}^n K_h(x-x_j) y_j , \\\\ T_2 &amp;= \\sum_{j=1}^n K_h(x-x_j) x_j(x_j-x), \\\\ T_3 &amp;= \\sum_{j=1}^n K_h(x-x_j) x_j y_j , \\\\ T_4 &amp;= \\sum_{j=1}^n K_h(x-x_j) (x_j-x) , \\\\ B_1 &amp;= \\sum_{j=1}^n K_h(x-x_j) , \\\\ B_2 &amp;= \\sum_{j=1}^n K_h(x-x_j) x_j^2 , \\\\ B_3 &amp;= \\sum_{j=1}^n K_h(x-x_j) x_j . \\end{align*}\\] As before, for a leave-one-out estimate we need to compute these sums over all \\(j\\neq i\\). Since each of the seven terms listed above contains the term \\(K_h(x-x_j)\\) inside the sum, we can achieve this by setting the corresponing elements of the matrix K to zero. r.LL &lt;- function(h) { dx &lt;- outer(x, x, &quot;-&quot;) K &lt;- dnorm(dx, sd = h) # compute a leave-one-out estimate diag(K) &lt;- 0 T1 &lt;- colSums(y*K) T2 &lt;- colSums(x*dx*K) T3 &lt;- colSums(x*y*K) T4 &lt;- colSums(dx*K) B1 &lt;- colSums(K) B2 &lt;- colSums(x^2*K) B3 &lt;- colSums(x*K) m.hat &lt;- (T1*T2 - T3*T4) / (B1*B2 - B3^2) mean((m.hat - y)^2) } Now we evaluate the function r.NW() and r.LL() on a grid of \\(h\\)-values to find the optimal \\(h\\) for each method. h &lt;- 10^seq(-1.4, 0.1, length.out = 61) mse.nw &lt;- numeric(length(h)) mse.ll &lt;- numeric(length(h)) for (i in seq_along(h)) { mse.nw[i] &lt;- r.NW(h[i]) mse.ll[i] &lt;- r.LL(h[i]) } plot(h, mse.nw, log=&quot;x&quot;, type = &quot;l&quot;, ylim = range(mse.nw, mse.ll), ylab = &quot;leave-one-out MSE&quot;) lines(h, mse.ll, col=&quot;red&quot;) best.h.NW &lt;- h[which.min(mse.nw)] abline(v = best.h.NW) best.h.LL &lt;- h[which.min(mse.ll)] abline(v = best.h.LL, col=&quot;red&quot;) legend(&quot;topleft&quot;, legend = c(&quot;NW&quot;, &quot;LL&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lwd = 2) As expected, the optimal bandwidth for local linear regression is larger than for the Nadaraya Watson estimator. 9.3 k-Nearest Neighbour Regression To conclude this section we use leave-one-out cross-validation to determine the optimal \\(k\\) for \\(k\\)-nearest neighbour regression. Here it seems difficult to make any savings, so we resort to simply fitting \\(n\\) different models in the naive way. For this reason, the code in this section is much slower to run than the code in the previous sections. k &lt;- 10:70 mse.knn &lt;- numeric(length(k)) for (j in seq_along(k)) { y.pred &lt;- numeric(length(x)) for (i in seq_along(x)) { m &lt;- knn.reg(data.frame(x = x[-i]), y = y[-i], test = data.frame(x = x[i]), k = k[j]) y.pred[i] &lt;- m$pred } mse.knn[j] &lt;- mean((y - y.pred)^2) } plot(k, mse.knn, type = &quot;l&quot;, ylab = &quot;leave-one-out MSE&quot;) best.k &lt;- k[which.min(mse.knn)] abline(v = best.k) We note the that leave-one-out mean squared error for kNN is smaller than it is for Nadaraya-Watson or local linear regression, in the case of this dataset. Given the structure of the data, with different regions having very different densities of \\(x\\)-values, it makes sense that a method which choses the bandwidth “adaptively” performs better. To conclude, we show the optimal regression curves for the three smoothing methods together in one plot. x.tilde &lt;- seq(1.5, 5.5, length.out = 501) K &lt;- dnorm(outer(x, x.tilde, &quot;-&quot;), sd = best.h.NW) m.NW &lt;- colSums(K*y) / colSums(K) dx &lt;- outer(x, x.tilde, &quot;-&quot;) K &lt;- dnorm(dx, sd = best.h.LL) T1 &lt;- colSums(y*K) T2 &lt;- colSums(x*dx*K) T3 &lt;- colSums(x*y*K) T4 &lt;- colSums(dx*K) B1 &lt;- colSums(K) B2 &lt;- colSums(x^2*K) B3 &lt;- colSums(x*K) m.LL &lt;- (T1*T2 - T3*T4) / (B1*B2 - B3^2) m &lt;- knn.reg(data.frame(x), y = y[-i], test = data.frame(x=x.tilde), k = best.k) m.kNN &lt;- m$pred colours &lt;- c(&quot;#2C9CDA&quot;, &quot;#811631&quot;, &quot;#E0CA1D&quot;) plot(x, y, xlim = range(x.tilde), cex = .5, xlab = &quot;eruption time [mins]&quot;, ylab = &quot;time to next eruption [mins]&quot;) lines(x.tilde, m.NW, col = colours[1]) lines(x.tilde, m.LL, col = colours[2]) lines(x.tilde, m.kNN, col = colours[3]) legend(&quot;topleft&quot;, legend = c(&quot;NW&quot;, &quot;LL&quot;, &quot;kNN&quot;), col = colours, lwd = 2) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
